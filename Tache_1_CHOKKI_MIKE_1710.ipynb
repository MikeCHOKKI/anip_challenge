{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import functools\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Tuple, Any, Union, Callable, Iterable\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "from urllib.parse import urlparse\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ],
   "id": "969de86a5581c808"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class GlobalConfig:\n",
    "    COUNTRY_CODE: str = \"BJ\"\n",
    "    COUNTRY_NAME: str = \"B√©nin\"\n",
    "    START_YEAR: int = 2015\n",
    "    END_YEAR: int = 2024\n",
    "\n",
    "    WORLD_BANK_API_URL: str = \"https://api.worldbank.org/v2\"\n",
    "    INSTAD_API_URL: str = \"https://instad.bj\"\n",
    "    OVERPASS_API_URL: str = \"https://overpass-api.de/api/interpreter\"\n",
    "    FMI_API_URL: str = \"https://www.imf.org/external/datamapper/api/v1\"\n",
    "    OMS_API_URL: str = \"https://ghoapi.azureedge.net/api\"\n",
    "    UNDP_API_URL: str = (\n",
    "        \"https://hdr.undp.org/sites/default/files/2021-22_HDR/HDR21-22_Composite_indices_complete_time_series.csv\"\n",
    "    )\n",
    "\n",
    "    DEFAULT_PER_PAGE: int = 100\n",
    "    REQUEST_TIMEOUT: int = 30\n",
    "    RETRY_COUNT: int = 3\n",
    "    DELAY_BETWEEN_REQUESTS: float = 0.5\n",
    "\n",
    "    DIRECTORY_STRUCTURE: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"data\": \"data_task_1\",\n",
    "            \"raw\": \"data_task_1/raw\",\n",
    "            \"processed\": \"data_task_1/processed\",\n",
    "            \"final_data\": \"data_task_1/final_data\",\n",
    "            \"logs\": \"logs\",\n",
    "            \"exports\": \"exports\",\n",
    "            \"docs\": \"docs\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    DEFAULT_WB_INDICATORS: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"SP.POP.TOTL\",\n",
    "            \"NY.GDP.MKTP.CD\",\n",
    "            \"NY.GDP.PCAP.CD\",\n",
    "            \"SE.PRM.NENR\",\n",
    "            \"SH.DYN.MORT\",\n",
    "            \"AG.LND.TOTL.K2\",\n",
    "            \"SL.TLF.TOTL.IN\",\n",
    "            \"SP.DYN.TFRT.IN\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    DEFAULT_IMF_INDICATORS: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"NGDP_R\",\n",
    "            \"NGDPD\",\n",
    "            \"PCPIPCH\",\n",
    "            \"LUR\",\n",
    "            \"GGX_NGDP\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    DEFAULT_HEALTH_INDICATORS: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"WHOSIS_000001\",\n",
    "            \"MDG_0000000001\",\n",
    "            \"MDG_0000000003\",\n",
    "            \"WHS4_544\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    OSM_ADMIN_LEVELS: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\"pays\": \"2\", \"d√©partement\": \"4\", \"commune\": \"6\"}\n",
    "    )\n",
    "\n",
    "    EXTERNAL_SCRAPING_URLS: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"rgph\": \"https://www.insae-bj.org/recensement-population.html\",\n",
    "            \"edc\": \"https://www.insae-bj.org/statistiques-economiques.html\",\n",
    "            \"emicov\": \"https://www.insae-bj.org/emicov.html\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    EXTERNAL_CSV_URLS: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"https://data.uis.unesco.org/medias/education/SDG4.csv\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.START_YEAR > self.END_YEAR:\n",
    "            raise ValueError(\n",
    "                f\"START_YEAR ({self.START_YEAR}) must be <= END_YEAR ({self.END_YEAR})\"\n",
    "            )\n",
    "        if not (1900 <= self.START_YEAR <= 2100 and 1900 <= self.END_YEAR <= 2100):\n",
    "            raise ValueError(\"Years must be between 1900 and 2100\")\n",
    "        if self.RETRY_COUNT < 1:\n",
    "            raise ValueError(\"RETRY_COUNT must be >= 1\")\n",
    "        if self.REQUEST_TIMEOUT < 1:\n",
    "            raise ValueError(\"REQUEST_TIMEOUT must be >= 1\")"
   ],
   "id": "d113137e3f5cb81d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class CleaningReport:\n",
    "    source: str\n",
    "    initial_rows: int\n",
    "    final_rows: int\n",
    "    rows_removed: int = 0\n",
    "    duplicates_removed: int = 0\n",
    "    nulls_handled: int = 0\n",
    "    outliers_removed: int = 0\n",
    "    columns_standardized: List[str] = field(default_factory=list)\n",
    "    columns_dropped: List[str] = field(default_factory=list)\n",
    "    data_types_converted: Dict[str, str] = field(default_factory=dict)\n",
    "    issues_detected: List[str] = field(default_factory=list)\n",
    "    cleaning_timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"source\": self.source,\n",
    "            \"initial_rows\": self.initial_rows,\n",
    "            \"final_rows\": self.final_rows,\n",
    "            \"rows_removed\": self.rows_removed,\n",
    "            \"removal_percentage\": (\n",
    "                round((self.rows_removed / self.initial_rows * 100), 2)\n",
    "                if self.initial_rows > 0\n",
    "                else 0\n",
    "            ),\n",
    "            \"duplicates_removed\": self.duplicates_removed,\n",
    "            \"nulls_handled\": self.nulls_handled,\n",
    "            \"outliers_removed\": self.outliers_removed,\n",
    "            \"columns_standardized\": len(self.columns_standardized),\n",
    "            \"columns_dropped\": len(self.columns_dropped),\n",
    "            \"types_converted\": len(self.data_types_converted),\n",
    "            \"issues_count\": len(self.issues_detected),\n",
    "            \"timestamp\": self.cleaning_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }"
   ],
   "id": "ee1673c4fc66e06d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    operation_name: str\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    duration_seconds: float = 0.0\n",
    "    items_processed: int = 0\n",
    "    success: bool = True\n",
    "    error_message: Optional[str] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    def finalize(\n",
    "            self, items: int = 0, success: bool = True, error: Optional[str] = None\n",
    "    ):\n",
    "        self.end_time = datetime.now()\n",
    "        self.duration_seconds = (self.end_time - self.start_time).total_seconds()\n",
    "        self.items_processed = items\n",
    "        self.success = success\n",
    "        self.error_message = error\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"operation\": self.operation_name,\n",
    "            \"start\": self.start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"end\": (\n",
    "                self.end_time.strftime(\"%Y-%m-%d %H:%M:%S\") if self.end_time else None\n",
    "            ),\n",
    "            \"duration\": round(self.duration_seconds, 3),\n",
    "            \"items\": self.items_processed,\n",
    "            \"throughput\": (\n",
    "                round(self.items_processed / self.duration_seconds, 2)\n",
    "                if self.duration_seconds > 0\n",
    "                else 0\n",
    "            ),\n",
    "            \"success\": self.success,\n",
    "            \"error\": self.error_message,\n",
    "            **self.metadata,\n",
    "        }\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        status = \"‚úÖ\" if self.success else \"‚ùå\"\n",
    "        duration_str = f\"{self.duration_seconds:.3f}s\"\n",
    "        if self.items_processed > 0:\n",
    "            throughput = (\n",
    "                self.items_processed / self.duration_seconds\n",
    "                if self.duration_seconds > 0\n",
    "                else 0\n",
    "            )\n",
    "            return f\"{status} {self.operation_name} | Dur√©e: {duration_str} | Items: {self.items_processed} | D√©bit: {throughput:.2f} items/s\"\n",
    "        return f\"{status} {self.operation_name} | Dur√©e: {duration_str}\""
   ],
   "id": "478daa57c11d72f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def is_script() -> bool:\n",
    "    return hasattr(sys.modules[\"__main__\"], \"__file__\")"
   ],
   "id": "e04028bad41dc841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def setup_environment(\n",
    "        log_dir: Optional[Path] = None, log_level: int = logging.INFO\n",
    ") -> None:\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    log_format = \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\"\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format=log_format,\n",
    "        datefmt=date_format,\n",
    "        handlers=[console_handler],\n",
    "    )\n",
    "\n",
    "    if log_dir:\n",
    "        log_dir = Path(log_dir)\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        file_handler = RotatingFileHandler(\n",
    "            log_dir / \"system.log\", maxBytes=2_000_000, backupCount=5, encoding=\"utf-8\"\n",
    "        )\n",
    "        file_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "        logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 100)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "    pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"figure.figsize\": (12, 8),\n",
    "            \"axes.titlesize\": 14,\n",
    "            \"axes.labelsize\": 12,\n",
    "            \"xtick.labelsize\": 10,\n",
    "            \"ytick.labelsize\": 10,\n",
    "            \"legend.fontsize\": 10,\n",
    "        }\n",
    "    )\n",
    "    sns.set_palette(\"Set2\")"
   ],
   "id": "dd1c85b9a2889d8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DirectoryManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_dir: Optional[Path] = None,\n",
    "            custom_structure: Optional[Dict[str, str]] = None,\n",
    "    ):\n",
    "        self.base_dir = base_dir or (\n",
    "            Path(__file__).parent if is_script() else Path(\".\")\n",
    "        )\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._directories: Dict[str, Path] = {}\n",
    "        self.custom_structure = custom_structure\n",
    "\n",
    "    def initialize_structure_directory(\n",
    "            self, structure: Optional[Dict[str, str]] = None\n",
    "    ) -> Dict[str, Path]:\n",
    "        structure_to_use = (\n",
    "                structure or self.custom_structure or GlobalConfig().DIRECTORY_STRUCTURE\n",
    "        )\n",
    "        for name, path in structure_to_use.items():\n",
    "            full_path = self.base_dir / path\n",
    "            full_path.mkdir(parents=True, exist_ok=True)\n",
    "            self._directories[name] = full_path\n",
    "        self.logger.info(f\"{len(self._directories)} dossiers cr√©√©s avec succ√®s.\")\n",
    "        return self._directories\n",
    "\n",
    "    def get_path(self, name: str) -> Optional[Path]:\n",
    "        path = self._directories.get(name)\n",
    "        if path is None:\n",
    "            self.logger.warning(f\"Le dossier '{name}' n'existe pas dans la structure.\")\n",
    "        return path\n",
    "\n",
    "    def list_directories(self) -> Dict[str, Path]:\n",
    "        return self._directories.copy()"
   ],
   "id": "58e8460cc97a9d73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AbstractCollector(ABC):\n",
    "    def __init__(self, config: GlobalConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.session = self._create_session()\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_session() -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        session.headers.update(\n",
    "            {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Educational Research Bot/1.0)\",\n",
    "                \"Accept\": \"application/json, text/html, */*\",\n",
    "                \"Accept-Language\": \"fr,en;q=0.9\",\n",
    "            }\n",
    "        )\n",
    "        return session\n",
    "\n",
    "    def _validate_url(self, url: str) -> bool:\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"Invalid URL: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _make_request_with_retry(\n",
    "            self, url: str, **kwargs\n",
    "    ) -> Tuple[Optional[requests.Response], bool]:\n",
    "        if not self._validate_url(url):\n",
    "            return None, False\n",
    "\n",
    "        method = kwargs.pop(\"method\", \"GET\")\n",
    "\n",
    "        for attempt in range(self.config.RETRY_COUNT):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method=method,\n",
    "                    url=url,\n",
    "                    timeout=self.config.REQUEST_TIMEOUT,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                if self.config.DELAY_BETWEEN_REQUESTS > 0:\n",
    "                    time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "                return response, True\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                status_code = e.response.status_code if e.response else \"N/A\"\n",
    "                self.logger.warning(\n",
    "                    f\"üîÑ Attempt {attempt + 1}/{self.config.RETRY_COUNT} - HTTP {status_code} on {url}\"\n",
    "                )\n",
    "            except requests.exceptions.Timeout:\n",
    "                self.logger.warning(\n",
    "                    f\"üîÑ Attempt {attempt + 1}/{self.config.RETRY_COUNT} - Timeout on {url}\"\n",
    "                )\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                self.logger.warning(\n",
    "                    f\"üîÑ Attempt {attempt + 1}/{self.config.RETRY_COUNT} - Connection error on {url}\"\n",
    "                )\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.warning(\n",
    "                    f\"üîÑ Attempt {attempt + 1}/{self.config.RETRY_COUNT} - Error on {url}: {e}\"\n",
    "                )\n",
    "\n",
    "            if attempt < self.config.RETRY_COUNT - 1:\n",
    "                sleep_time = 2 ** attempt\n",
    "                self.logger.debug(f\"Waiting {sleep_time}s before retry...\")\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "        self.logger.error(f\"‚ùå Failed after {self.config.RETRY_COUNT} attempts: {url}\")\n",
    "        return None, False\n",
    "\n",
    "    @abstractmethod\n",
    "    def collect_data(self) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
    "        pass\n",
    "\n",
    "    def save_data(\n",
    "            self,\n",
    "            data: pd.DataFrame,\n",
    "            file_path: Path,\n",
    "            format_type: str = \"csv\",\n",
    "            add_metadata: bool = True,\n",
    "    ) -> Tuple[bool, Optional[Path]]:\n",
    "        if not isinstance(file_path, Path):\n",
    "            raise TypeError(\"file_path must be a pathlib.Path object\")\n",
    "        if data.empty:\n",
    "            self.logger.warning(\"Empty DataFrame, no data to save.\")\n",
    "            return False, None\n",
    "\n",
    "        try:\n",
    "            meta_data = data.copy()\n",
    "            if add_metadata:\n",
    "                meta_data[\"collection_timestamp\"] = datetime.now()\n",
    "                meta_data[\"collector\"] = self.__class__.__name__\n",
    "\n",
    "            format_type = format_type.lower()\n",
    "            if format_type == \"csv\":\n",
    "                meta_data.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
    "            elif format_type == \"excel\":\n",
    "                meta_data.to_excel(file_path, index=False, engine=\"openpyxl\")\n",
    "            elif format_type == \"json\":\n",
    "                meta_data.to_json(\n",
    "                    file_path,\n",
    "                    orient=\"records\",\n",
    "                    force_ascii=False,\n",
    "                    indent=2,\n",
    "                    date_format=\"iso\",\n",
    "                )\n",
    "            elif format_type == \"parquet\":\n",
    "                meta_data.to_parquet(file_path, index=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {format_type}\")\n",
    "\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            self.logger.info(\n",
    "                f\"‚úÖ Saved: {file_path.name} ({len(data)} rows, {size_mb:.2f} MB)\"\n",
    "            )\n",
    "            return True, file_path\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Save error {file_path}: {e}\")\n",
    "            return False, None"
   ],
   "id": "176be9040d4733ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerformanceTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics: List[PerformanceMetrics] = []\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def add_metric(self, metric: PerformanceMetrics) -> None:\n",
    "        self.metrics.append(metric)\n",
    "        self.logger.debug(f\"{metric.operation_name}: {metric}\")\n",
    "\n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        if not self.metrics:\n",
    "            return {\"total_operations\": 0}\n",
    "        total_duration = sum(m.duration_seconds for m in self.metrics)\n",
    "        successful = sum(1 for m in self.metrics if m.success)\n",
    "        return {\n",
    "            \"total_operations\": len(self.metrics),\n",
    "            \"successful\": successful,\n",
    "            \"failed\": len(self.metrics) - successful,\n",
    "            \"total_duration\": round(total_duration, 3),\n",
    "            \"avg_duration\": round(total_duration / len(self.metrics), 3),\n",
    "            \"total_items\": sum(m.items_processed for m in self.metrics),\n",
    "        }\n",
    "\n",
    "    def print_summary(self):\n",
    "        summary = self.get_summary()\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"R√âSUM√â DES PERFORMANCES\")\n",
    "        print(\"=\" * 70)\n",
    "        for key, value in summary.items():\n",
    "            print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "\n",
    "_global_tracker = PerformanceTracker()"
   ],
   "id": "c0276a71dbbc919e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def timer(\n",
    "        operation_name: Optional[str] = None,\n",
    "        log_result: bool = True,\n",
    "        track_metrics: bool = True,\n",
    "):\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs) -> Any:\n",
    "            op_name = (\n",
    "                    operation_name\n",
    "                    or f\"{getattr(func, '__module__', '<unknown>')}.{func.__name__}\"\n",
    "            )\n",
    "            metric = PerformanceMetrics(\n",
    "                operation_name=op_name, start_time=datetime.now()\n",
    "            )\n",
    "            logger = logging.getLogger(getattr(func, \"__module__\", \"<unknown>\"))\n",
    "\n",
    "            if log_result:\n",
    "                logger.info(f\"‚è±Ô∏è  Start: {op_name}\")\n",
    "\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                metric.finalize(success=True)\n",
    "                if log_result:\n",
    "                    logger.info(f\"‚úÖ Done: {op_name} in {metric.duration_seconds:.3f}s\")\n",
    "                if track_metrics:\n",
    "                    _global_tracker.add_metric(metric)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                metric.finalize(success=False, error=str(e))\n",
    "                if log_result:\n",
    "                    logger.error(\n",
    "                        f\"‚ùå Failed: {op_name} after {metric.duration_seconds:.3f}s - {e}\"\n",
    "                    )\n",
    "                if track_metrics:\n",
    "                    _global_tracker.add_metric(metric)\n",
    "                raise\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ],
   "id": "e029e43fc522d18f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@contextmanager\n",
    "def track_progress(\n",
    "        iterable: Iterable,\n",
    "        desc: str = \"Processing\",\n",
    "        total: Optional[int] = None,\n",
    "        unit: str = \"item\",\n",
    "        leave: bool = True,\n",
    "        **tqdm_kwargs,\n",
    "):\n",
    "    pbar = tqdm(\n",
    "        iterable=iterable,\n",
    "        desc=desc,\n",
    "        total=total,\n",
    "        unit=unit,\n",
    "        leave=leave,\n",
    "        ncols=100,\n",
    "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\",\n",
    "        **tqdm_kwargs,\n",
    "    )\n",
    "    try:\n",
    "        yield pbar\n",
    "    finally:\n",
    "        pbar.close()"
   ],
   "id": "640805f77d846a92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class WorldBankCollector(AbstractCollector):\n",
    "    @timer(operation_name=\"WorldBank.fetch_indicator\", track_metrics=True)\n",
    "    def _fetch_indicator_data(\n",
    "            self,\n",
    "            indicator: str,\n",
    "            start_year: Optional[int] = None,\n",
    "            end_year: Optional[int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        url = f\"{self.config.WORLD_BANK_API_URL}/country/{self.config.COUNTRY_CODE}/indicator/{indicator}\"\n",
    "        params = {\n",
    "            \"date\": f\"{start_year or self.config.START_YEAR}:{end_year or self.config.END_YEAR}\",\n",
    "            \"format\": \"json\",\n",
    "            \"per_page\": self.config.DEFAULT_PER_PAGE,\n",
    "        }\n",
    "        response, success = self._make_request_with_retry(url, params=params)\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            entries = data[1] if isinstance(data, list) and len(data) > 1 else []\n",
    "            records = [\n",
    "                {\n",
    "                    \"indicator_code\": entry[\"indicator\"][\"id\"],\n",
    "                    \"indicator_name\": entry[\"indicator\"][\"value\"],\n",
    "                    \"country_code\": entry[\"country\"][\"id\"],\n",
    "                    \"country_name\": entry[\"country\"][\"value\"],\n",
    "                    \"year\": pd.to_numeric(entry[\"date\"], errors=\"coerce\"),\n",
    "                    \"value\": pd.to_numeric(entry[\"value\"], errors=\"coerce\"),\n",
    "                    \"source\": \"World Bank API\",\n",
    "                }\n",
    "                for entry in entries\n",
    "            ]\n",
    "            return pd.DataFrame(records)\n",
    "        except (ValueError, KeyError, IndexError) as e:\n",
    "            self.logger.error(f\"‚ùå Parse error {indicator}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @timer(operation_name=\"WorldBank.collect_all\", track_metrics=True)\n",
    "    def collect_data(self, indicators: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        indicators = indicators or self.config.DEFAULT_WB_INDICATORS\n",
    "        self.logger.info(\n",
    "            f\"üåç Start World Bank collection ({len(indicators)} indicators)\"\n",
    "        )\n",
    "        all_data = []\n",
    "        with track_progress(indicators, desc=\"World Bank\", unit=\"indicator\") as pbar:\n",
    "            for indicator in pbar:\n",
    "                pbar.set_postfix_str(f\"Indicator: {indicator}\")\n",
    "                df = self._fetch_indicator_data(indicator)\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "                    self.logger.info(f\"‚úÖ {len(df)} records collected\")\n",
    "                time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        result = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "        metric = PerformanceMetrics(\n",
    "            operation_name=\"WorldBank.collect_summary\",\n",
    "            start_time=datetime.now(),\n",
    "            items_processed=len(result),\n",
    "        )\n",
    "        metric.finalize(items=len(result))\n",
    "        _global_tracker.add_metric(metric)\n",
    "        self.logger.info(f\"‚úÖ World Bank done: {len(result)} records\")\n",
    "        return result"
   ],
   "id": "3c9668ac11a89a33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class WebScrapingCollector(AbstractCollector):\n",
    "    @timer(operation_name=\"WebScraping.scrape_tables\", track_metrics=True)\n",
    "    def _scrape_html_tables(\n",
    "        self, url: str, source_name: str, max_tables: int = 10\n",
    "    ) -> pd.DataFrame:\n",
    "        response, success = self._make_request_with_retry(url)\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            tables = soup.find_all(\"table\")\n",
    "            self.logger.info(f\"üìã {len(tables)} tables found on {url}\")\n",
    "            \n",
    "            scraped_data = []\n",
    "            for i, table in enumerate(tables[:max_tables]):\n",
    "                try:\n",
    "                    df = pd.read_html(str(table), header=0)[0]\n",
    "                    \n",
    "                    if df.empty or len(df.columns) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    df = df.dropna(how='all', axis=0)\n",
    "                    df = df.dropna(how='all', axis=1)\n",
    "                    \n",
    "                    if df.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    df.columns = [str(col).strip() for col in df.columns]\n",
    "                    \n",
    "                    df['source_url'] = url\n",
    "                    df['source_name'] = source_name\n",
    "                    df['table_index'] = i\n",
    "                    df['extraction_date'] = datetime.now()\n",
    "                    \n",
    "                    scraped_data.append(df)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"Skipping table {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not scraped_data:\n",
    "                self.logger.warning(f\"No valid tables extracted from {url}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            return pd.concat(scraped_data, ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Scraping error {url}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @timer(operation_name=\"WebScraping.collect_all\", track_metrics=True)\n",
    "    def collect_data(self, max_tables: int = 10) -> pd.DataFrame:\n",
    "        self.logger.info(\"üï∑Ô∏è Start web scraping\")\n",
    "        urls = {\n",
    "            \"instad_trimestres\": \"https://instad.bj/publications/publications-trimestrielles\",\n",
    "            \"instad_mensuelles\": \"https://instad.bj/publications/publications-mensuelles\",\n",
    "        }\n",
    "        all_data = []\n",
    "        with track_progress(\n",
    "            urls.items(), desc=\"Web Scraping\", total=len(urls), unit=\"site\"\n",
    "        ) as pbar:\n",
    "            for source_name, url in pbar:\n",
    "                pbar.set_postfix_str(f\"Source: {source_name}\")\n",
    "                df = self._scrape_html_tables(url, source_name, max_tables)\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "        \n",
    "        result = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "        self.logger.info(f\"‚úÖ Scraping done: {len(result)} records\")\n",
    "        return result"
   ],
   "id": "83c28c72fc76f633"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GeographicCollector(AbstractCollector):\n",
    "    @timer(operation_name=\"Geographic.execute_query\", track_metrics=True)\n",
    "    def _execute_overpass_query(self, query: str, data_type: str) -> pd.DataFrame:\n",
    "        response, success = self._make_request_with_retry(\n",
    "            self.config.OVERPASS_API_URL, data={\"data\": query}, method=\"POST\"\n",
    "        )\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            elements = data.get(\"elements\", [])\n",
    "            records = []\n",
    "            for element in elements:\n",
    "                if \"tags\" not in element:\n",
    "                    continue\n",
    "                record = {\n",
    "                    \"name\": element[\"tags\"].get(\"name\"),\n",
    "                    \"osm_id\": element.get(\"id\"),\n",
    "                    \"latitude\": element.get(\"lat\")\n",
    "                                or element.get(\"center\", {}).get(\"lat\"),\n",
    "                    \"longitude\": element.get(\"lon\")\n",
    "                                 or element.get(\"center\", {}).get(\"lon\"),\n",
    "                    \"data_type\": data_type,\n",
    "                    \"source\": \"OpenStreetMap\",\n",
    "                }\n",
    "                if data_type == \"cities\":\n",
    "                    record.update(\n",
    "                        {\n",
    "                            \"place_type\": element[\"tags\"].get(\"place\"),\n",
    "                            \"population\": pd.to_numeric(\n",
    "                                element[\"tags\"].get(\"population\"), errors=\"coerce\"\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    record.update(\n",
    "                        {\n",
    "                            \"admin_level\": element[\"tags\"].get(\"admin_level\"),\n",
    "                            \"wikidata\": element[\"tags\"].get(\"wikidata\"),\n",
    "                        }\n",
    "                    )\n",
    "                records.append(record)\n",
    "            self.logger.info(f\"üìç {len(records)} {data_type} elements collected\")\n",
    "            return pd.DataFrame(records)\n",
    "        except (ValueError, KeyError) as e:\n",
    "            self.logger.error(f\"‚ùå Parse error Overpass {data_type}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @timer(operation_name=\"Geographic.collect_all\", track_metrics=True)\n",
    "    def collect_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        self.logger.info(\"üó∫Ô∏è Start geographic collection\")\n",
    "        results = {}\n",
    "        cities_query = f\"\"\"\n",
    "        [out:json][timeout:60];\n",
    "        area[\"ISO3166-1\"=\"{self.config.COUNTRY_CODE}\"];\n",
    "        (node(area)[\"place\"~\"city|town|village\"]; way(area)[\"place\"~\"city|town|village\"];);\n",
    "        out center tags;\n",
    "        \"\"\"\n",
    "        cities_df = self._execute_overpass_query(cities_query, \"cities\")\n",
    "        if not cities_df.empty:\n",
    "            results[\"cities\"] = cities_df\n",
    "\n",
    "        admin_levels = list(self.config.OSM_ADMIN_LEVELS.items())\n",
    "        with track_progress(\n",
    "                admin_levels, desc=\"Admin boundaries\", unit=\"level\"\n",
    "        ) as pbar:\n",
    "            for level_name, level_code in pbar:\n",
    "                pbar.set_postfix_str(f\"Level: {level_name}\")\n",
    "                admin_query = f\"\"\"\n",
    "                [out:json][timeout:60];\n",
    "                relation[\"boundary\"=\"administrative\"][\"admin_level\"=\"{level_code}\"][\"name\"~\"{self.config.COUNTRY_NAME}|Benin\"];\n",
    "                out center tags;\n",
    "                \"\"\"\n",
    "                admin_df = self._execute_overpass_query(\n",
    "                    admin_query, f\"admin_{level_name}\"\n",
    "                )\n",
    "                if not admin_df.empty:\n",
    "                    results[f\"admin_{level_name}\"] = admin_df\n",
    "\n",
    "        total = sum(len(df) for df in results.values())\n",
    "        self.logger.info(f\"‚úÖ Geographic done: {total} records\")\n",
    "        return results"
   ],
   "id": "52adf193421c3cfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ExternalCollector(AbstractCollector):\n",
    "    @timer(operation_name=\"External.download_file\", track_metrics=True)\n",
    "    def _download_data(self, url: str) -> pd.DataFrame:\n",
    "        response, success = self._make_request_with_retry(url)\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            from io import BytesIO\n",
    "\n",
    "            content = response.content\n",
    "            try:\n",
    "                df = pd.read_csv(BytesIO(content))\n",
    "                df[\"source\"] = url\n",
    "                return df\n",
    "            except pd.errors.EmptyDataError:\n",
    "                self.logger.warning(f\"‚ö†Ô∏è Empty file: {url}\")\n",
    "                return pd.DataFrame()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                import json\n",
    "\n",
    "                json_data = json.loads(content)\n",
    "                df = pd.json_normalize(json_data)\n",
    "                df[\"source\"] = url\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"‚ùå Unsupported format for {url}: {e}\")\n",
    "                return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Download error {url}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @timer(operation_name=\"External.collect_all\", track_metrics=True)\n",
    "    def collect_data(self, urls: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        urls = urls or self.config.EXTERNAL_CSV_URLS\n",
    "        self.logger.info(f\"üåê External collection ({len(urls)} sources)\")\n",
    "        all_data = []\n",
    "        with track_progress(\n",
    "                enumerate(urls, 1), desc=\"External sources\", total=len(urls), unit=\"source\"\n",
    "        ) as pbar:\n",
    "            for i, url in pbar:\n",
    "                pbar.set_postfix_str(f\"URL {i}/{len(urls)}\")\n",
    "                df = self._download_data(url)\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "                    self.logger.info(f\"‚úÖ {len(df)} records\")\n",
    "        result = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "        self.logger.info(f\"‚úÖ External done: {len(result)} records\")\n",
    "        return result"
   ],
   "id": "b0b928f4be917fac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class IMFCollector(AbstractCollector):\n",
    "    @timer(operation_name=\"IMF.fetch_indicator\", track_metrics=True)\n",
    "    def _fetch_indicator_data(self, indicator: str) -> pd.DataFrame:\n",
    "        url = f\"{self.config.FMI_API_URL}/{indicator}\"\n",
    "        response, success = self._make_request_with_retry(url)\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            records = []\n",
    "            if isinstance(data, dict) and \"values\" in data:\n",
    "                country_data = (\n",
    "                    data[\"values\"].get(indicator, {}).get(self.config.COUNTRY_CODE, {})\n",
    "                )\n",
    "                for year, value in country_data.items():\n",
    "                    records.append(\n",
    "                        {\n",
    "                            \"indicator_code\": indicator,\n",
    "                            \"country_code\": self.config.COUNTRY_CODE,\n",
    "                            \"year\": pd.to_numeric(year, errors=\"coerce\"),\n",
    "                            \"value\": pd.to_numeric(value, errors=\"coerce\"),\n",
    "                            \"source\": \"IMF\",\n",
    "                        }\n",
    "                    )\n",
    "            return pd.DataFrame(records)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Parse error IMF {indicator}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @timer(operation_name=\"IMF.collect_all\", track_metrics=True)\n",
    "    def collect_data(self, indicators: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        indicators = indicators or self.config.DEFAULT_IMF_INDICATORS\n",
    "        self.logger.info(f\"üí∞ Start IMF collection ({len(indicators)} indicators)\")\n",
    "        all_data = []\n",
    "        with track_progress(indicators, desc=\"IMF\", unit=\"indicator\") as pbar:\n",
    "            for indicator in pbar:\n",
    "                pbar.set_postfix_str(f\"Indicator: {indicator}\")\n",
    "                df = self._fetch_indicator_data(indicator)\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "                    self.logger.info(f\"‚úÖ {len(df)} records collected\")\n",
    "                time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        result = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "        metric = PerformanceMetrics(\n",
    "            operation_name=\"IMF.collect_summary\",\n",
    "            start_time=datetime.now(),\n",
    "            items_processed=len(result),\n",
    "        )\n",
    "        metric.finalize(items=len(result))\n",
    "        _global_tracker.add_metric(metric)\n",
    "        self.logger.info(f\"‚úÖ IMF done: {len(result)} records\")\n",
    "        return result"
   ],
   "id": "721b44efe409b697"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class WHOCollector(AbstractCollector):\n",
    "    def _parse_who_data(self, data: Dict, indicator: str) -> List[Dict]:\n",
    "        records = []\n",
    "        for item in data.get(\"value\", []):\n",
    "            records.append(\n",
    "                {\n",
    "                    \"indicator_code\": indicator,\n",
    "                    \"indicator_name\": item.get(\"IndicatorCode\"),\n",
    "                    \"country_code\": item.get(\"SpatialDim\"),\n",
    "                    \"year\": pd.to_numeric(item.get(\"TimeDim\"), errors=\"coerce\"),\n",
    "                    \"value\": pd.to_numeric(item.get(\"NumericValue\"), errors=\"coerce\"),\n",
    "                    \"source\": \"WHO GHO\",\n",
    "                }\n",
    "            )\n",
    "        return records\n",
    "\n",
    "    @timer(operation_name=\"WHO.fetch_indicator\", track_metrics=True)\n",
    "    def _fetch_indicator_data(self, indicator: str) -> pd.DataFrame:\n",
    "        url = f\"{self.config.OMS_API_URL}/{indicator}\"\n",
    "        params = {\"$filter\": f\"SpatialDim eq '{self.config.COUNTRY_CODE}'\"}\n",
    "        response, success = self._make_request_with_retry(url, params=params)\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            records = self._parse_who_data(data, indicator)\n",
    "            return pd.DataFrame(records)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Parse error WHO {indicator}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @timer(operation_name=\"WHO.collect_all\", track_metrics=True)\n",
    "    def collect_data(self, indicators: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        indicators = indicators or self.config.DEFAULT_HEALTH_INDICATORS\n",
    "        self.logger.info(f\"üè• Start WHO collection ({len(indicators)} indicators)\")\n",
    "        all_data = []\n",
    "        with track_progress(indicators, desc=\"WHO\", unit=\"indicator\") as pbar:\n",
    "            for indicator in pbar:\n",
    "                pbar.set_postfix_str(f\"Indicator: {indicator}\")\n",
    "                df = self._fetch_indicator_data(indicator)\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "                    self.logger.info(f\"‚úÖ {len(df)} records collected\")\n",
    "                time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        result = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "        metric = PerformanceMetrics(\n",
    "            operation_name=\"WHO.collect_summary\",\n",
    "            start_time=datetime.now(),\n",
    "            items_processed=len(result),\n",
    "        )\n",
    "        metric.finalize(items=len(result))\n",
    "        _global_tracker.add_metric(metric)\n",
    "        self.logger.info(f\"‚úÖ WHO done: {len(result)} records\")\n",
    "        return result"
   ],
   "id": "9cb3552226e3ca35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class UNDPCollector(AbstractCollector):\n",
    "    @timer(operation_name=\"UNDP.collect_all\", track_metrics=True)\n",
    "    def collect_data(self) -> pd.DataFrame:\n",
    "        self.logger.info(\"üåê Start UNDP collection\")\n",
    "        url = self.config.UNDP_API_URL\n",
    "        response, success = self._make_request_with_retry(url)\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            from io import BytesIO\n",
    "\n",
    "            df = pd.read_csv(BytesIO(response.content))\n",
    "            df_benin = df[df[\"iso3\"] == self.config.COUNTRY_CODE].copy()\n",
    "            df_benin[\"source\"] = \"UNDP HDR\"\n",
    "            metric = PerformanceMetrics(\n",
    "                operation_name=\"UNDP.collect_summary\",\n",
    "                start_time=datetime.now(),\n",
    "                items_processed=len(df_benin),\n",
    "            )\n",
    "            metric.finalize(items=len(df_benin))\n",
    "            _global_tracker.add_metric(metric)\n",
    "            self.logger.info(f\"‚úÖ UNDP: {len(df_benin)} records\")\n",
    "            return df_benin\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå UNDP error: {e}\")\n",
    "            return pd.DataFrame()"
   ],
   "id": "77c3970ed8489b42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class INSAECollector(AbstractCollector):\n",
    "    @timer(operation_name=\"INSAE.download_file\", track_metrics=True)\n",
    "    def _download_excel_or_csv(self, url: str) -> pd.DataFrame:\n",
    "        response, success = self._make_request_with_retry(url)\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            from io import BytesIO\n",
    "\n",
    "            if url.endswith(\".csv\"):\n",
    "                return pd.read_csv(BytesIO(response.content))\n",
    "            else:\n",
    "                return pd.read_excel(BytesIO(response.content), engine=\"openpyxl\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå File read error: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @timer(operation_name=\"INSAE.scrape_source\", track_metrics=True)\n",
    "    def _scrape_insae_data(self, url: str, source: str) -> pd.DataFrame:\n",
    "        response, success = self._make_request_with_retry(url)\n",
    "        if not success or response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            import re\n",
    "            from urllib.parse import urljoin\n",
    "\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            download_links = soup.find_all(\"a\", href=re.compile(r\"\\.(xlsx?|csv)$\"))\n",
    "            data_frames = []\n",
    "            links_to_process = download_links[:5]\n",
    "            with track_progress(\n",
    "                    links_to_process, desc=f\"INSAE {source}\", unit=\"file\"\n",
    "            ) as pbar:\n",
    "                for link in pbar:\n",
    "                    file_url = urljoin(url, link[\"href\"])\n",
    "                    pbar.set_postfix_str(f\"File: {link.get_text(strip=True)[:30]}\")\n",
    "                    self.logger.info(f\"üì• Downloading: {file_url}\")\n",
    "                    file_df = self._download_excel_or_csv(file_url)\n",
    "                    if not file_df.empty:\n",
    "                        file_df[\"source\"] = f\"INSAE - {source}\"\n",
    "                        data_frames.append(file_df)\n",
    "            return (\n",
    "                pd.concat(data_frames, ignore_index=True)\n",
    "                if data_frames\n",
    "                else pd.DataFrame()\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå INSAE scraping error: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    @timer(operation_name=\"INSAE.collect_all\", track_metrics=True)\n",
    "    def collect_data(self) -> pd.DataFrame:\n",
    "        self.logger.info(\"üáßüáØ Start INSAE B√©nin collection\")\n",
    "        insae_urls = self.config.EXTERNAL_SCRAPING_URLS\n",
    "        all_data = []\n",
    "        with track_progress(\n",
    "                insae_urls.items(), desc=\"INSAE sources\", unit=\"source\"\n",
    "        ) as pbar:\n",
    "            for source_name, url in pbar:\n",
    "                if not source_name.startswith((\"rgph\", \"edc\", \"emicov\")):\n",
    "                    continue\n",
    "                pbar.set_postfix_str(f\"Source: {source_name}\")\n",
    "                df = self._scrape_insae_data(url, source_name)\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "\n",
    "        result = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "        metric = PerformanceMetrics(\n",
    "            operation_name=\"INSAE.collect_summary\",\n",
    "            start_time=datetime.now(),\n",
    "            items_processed=len(result),\n",
    "        )\n",
    "        metric.finalize(items=len(result))\n",
    "        _global_tracker.add_metric(metric)\n",
    "        self.logger.info(f\"‚úÖ INSAE done: {len(result)} records\")\n",
    "        return result"
   ],
   "id": "be2809514887ae53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.config = config or self._default_config()\n",
    "        self.reports: Dict[str, CleaningReport] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_config() -> Dict:\n",
    "        return {\n",
    "            \"remove_duplicates\": True,\n",
    "            \"handle_nulls\": True,\n",
    "            \"null_threshold\": 0.7,\n",
    "            \"detect_outliers\": True,\n",
    "            \"outlier_method\": \"iqr\",\n",
    "            \"outlier_threshold\": 3,\n",
    "            \"standardize_text\": True,\n",
    "            \"standardize_dates\": True,\n",
    "            \"convert_types\": True,\n",
    "            \"remove_empty_strings\": True,\n",
    "        }\n",
    "\n",
    "    def _standardize_column_names(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        import re\n",
    "\n",
    "        new_columns = {}\n",
    "        for col in df.columns:\n",
    "            new_col = str(col).lower()\n",
    "            new_col = re.sub(r\"[^\\w\\s]\", \"\", new_col)\n",
    "            new_col = re.sub(r\"\\s+\", \"_\", new_col)\n",
    "            new_col = re.sub(r\"_+\", \"_\", new_col).strip(\"_\")\n",
    "            new_columns[col] = new_col\n",
    "        df = df.rename(columns=new_columns)\n",
    "        self.logger.info(f\"‚úÖ {len(new_columns)} columns standardized\")\n",
    "        return df\n",
    "\n",
    "    def _remove_duplicates(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "        initial_count = len(df)\n",
    "        df = df.drop_duplicates()\n",
    "        duplicates_removed = initial_count - len(df)\n",
    "        if duplicates_removed > 0:\n",
    "            self.logger.info(f\"üóëÔ∏è {duplicates_removed} duplicates removed\")\n",
    "        return df, duplicates_removed\n",
    "\n",
    "    def _clean_text_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "        for col in text_cols:\n",
    "            df[col] = df[col].str.strip()\n",
    "            if self.config.get(\"remove_empty_strings\"):\n",
    "                df[col] = df[col].replace(\"\", np.nan)\n",
    "                df[col] = df[col].replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "        return df\n",
    "\n",
    "    def _convert_data_types(\n",
    "            self, df: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, Dict[str, str]]:\n",
    "        conversions = {}\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == \"object\":\n",
    "                original_type = str(df[col].dtype)\n",
    "                numeric_vals = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                if numeric_vals.notna().sum() / len(df) > 0.8:\n",
    "                    df[col] = numeric_vals\n",
    "                    conversions[col] = f\"{original_type} -> numeric\"\n",
    "                    continue\n",
    "                try:\n",
    "                    date_vals = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "                    if date_vals.notna().sum() / len(df) > 0.8:\n",
    "                        df[col] = date_vals\n",
    "                        conversions[col] = f\"{original_type} -> datetime\"\n",
    "                except:\n",
    "                    pass\n",
    "        if conversions:\n",
    "            self.logger.info(f\"üîÑ {len(conversions)} type conversions\")\n",
    "        return df, conversions\n",
    "\n",
    "    def _remove_empty_columns(\n",
    "            self, df: pd.DataFrame, report: CleaningReport\n",
    "    ) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        threshold = self.config.get(\"null_threshold\", 0.7)\n",
    "        dropped_cols = []\n",
    "        for col in df.columns:\n",
    "            null_ratio = df[col].isnull().sum() / len(df)\n",
    "            if null_ratio > threshold:\n",
    "                dropped_cols.append(col)\n",
    "                report.issues_detected.append(\n",
    "                    f\"Column '{col}' dropped: {null_ratio:.1%} missing values\"\n",
    "                )\n",
    "        if dropped_cols:\n",
    "            df = df.drop(columns=dropped_cols)\n",
    "            self.logger.warning(f\"‚ö†Ô∏è {len(dropped_cols)} columns dropped\")\n",
    "        return df, dropped_cols\n",
    "\n",
    "    def _handle_outliers(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, int]:\n",
    "        if not self.config.get(\"detect_outliers\"):\n",
    "            return df, 0\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        outliers_removed = 0\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 10:\n",
    "                continue\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            outliers_count = outliers_mask.sum()\n",
    "            if outliers_count > 0:\n",
    "                df.loc[outliers_mask, col] = np.nan\n",
    "                outliers_removed += outliers_count\n",
    "        if outliers_removed > 0:\n",
    "            self.logger.info(f\"üîç {outliers_removed} outliers handled\")\n",
    "        return df, outliers_removed\n",
    "\n",
    "    @timer(operation_name=\"DataCleaner.clean_dataset\", track_metrics=True)\n",
    "    def clean_dataset(\n",
    "            self, df: pd.DataFrame, source_name: str\n",
    "    ) -> Tuple[pd.DataFrame, CleaningReport]:\n",
    "        self.logger.info(f\"üßπ Cleaning: {source_name}\")\n",
    "        report = CleaningReport(\n",
    "            source=source_name, initial_rows=len(df), final_rows=len(df)\n",
    "        )\n",
    "        df = self._standardize_column_names(df)\n",
    "        report.columns_standardized = df.columns.tolist()\n",
    "        df, duplicates = self._remove_duplicates(df)\n",
    "        report.duplicates_removed = duplicates\n",
    "        df = self._clean_text_columns(df)\n",
    "        df, dropped_cols = self._remove_empty_columns(df, report)\n",
    "        report.columns_dropped = dropped_cols\n",
    "        df, conversions = self._convert_data_types(df)\n",
    "        report.data_types_converted = conversions\n",
    "        df, outliers = self._handle_outliers(df)\n",
    "        report.outliers_removed = outliers\n",
    "        report.final_rows = len(df)\n",
    "        report.rows_removed = report.initial_rows - report.final_rows\n",
    "        self.reports[source_name] = report\n",
    "        self.logger.info(\n",
    "            f\"‚úÖ Cleaning done: {report.initial_rows} ‚Üí {report.final_rows} rows ({report.rows_removed} removed)\"\n",
    "        )\n",
    "        return df, report\n",
    "\n",
    "    def clean_world_bank_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.logger.info(\"üåç World Bank specific cleaning\")\n",
    "        df = df.dropna(subset=[\"value\"])\n",
    "        df = df[(df[\"year\"] >= 1900) & (df[\"year\"] <= datetime.now().year)]\n",
    "        df = df[df[\"value\"] >= 0]\n",
    "        return df\n",
    "\n",
    "    def clean_geographic_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.logger.info(\"üó∫Ô∏è Geographic specific cleaning\")\n",
    "        df = df.dropna(subset=[\"name\"])\n",
    "        if \"latitude\" in df.columns and \"longitude\" in df.columns:\n",
    "            df = df[\n",
    "                (df[\"latitude\"].between(-90, 90)) & (df[\"longitude\"].between(-180, 180))\n",
    "                ]\n",
    "        return df\n",
    "\n",
    "    def generate_cleaning_summary(self) -> pd.DataFrame:\n",
    "        if not self.reports:\n",
    "            self.logger.warning(\"‚ö†Ô∏è No cleaning reports available\")\n",
    "            return pd.DataFrame()\n",
    "        summary_data = [report.to_dict() for report in self.reports.values()]\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        self.logger.info(\"üìä Cleaning summary generated\")\n",
    "        return summary_df"
   ],
   "id": "d6902d83d4d0fbe4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataCollectorOrchestrator:\n",
    "    def __init__(\n",
    "        self, config: Optional[GlobalConfig] = None, base_dir: Optional[Path] = None\n",
    "    ):\n",
    "        self.config = config or GlobalConfig()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.directory_manager = DirectoryManager(base_dir)\n",
    "        self.directories = self.directory_manager.initialize_structure_directory()\n",
    "        self.collectors = {\n",
    "            \"world_bank\": WorldBankCollector(self.config),\n",
    "            \"imf\": IMFCollector(self.config),\n",
    "            \"who\": WHOCollector(self.config),\n",
    "            \"undp\": UNDPCollector(self.config),\n",
    "            \"insae\": INSAECollector(self.config),\n",
    "            \"web_scraping\": WebScrapingCollector(self.config),\n",
    "            \"geographic\": GeographicCollector(self.config),\n",
    "            \"external\": ExternalCollector(self.config),\n",
    "        }\n",
    "        self.cleaner = DataCleaner()\n",
    "    \n",
    "    def consolidate_final_data(\n",
    "        self, cleaned_data: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        self.logger.info(\"üîó Consolidating final datasets\")\n",
    "        final_datasets = {}\n",
    "        \n",
    "        economic_sources = ['world_bank', 'imf']\n",
    "        economic_data = []\n",
    "        for source in economic_sources:\n",
    "            if source in cleaned_data and not cleaned_data[source].empty:\n",
    "                df = cleaned_data[source].copy()\n",
    "                df['data_source'] = source\n",
    "                economic_data.append(df)\n",
    "        \n",
    "        if economic_data:\n",
    "            economic_consolidated = pd.concat(economic_data, ignore_index=True)\n",
    "            final_datasets['economic_indicators'] = economic_consolidated\n",
    "            filepath = self.directories['final_data'] / 'economic_indicators.csv'\n",
    "            economic_consolidated.to_csv(filepath, index=False, encoding='utf-8')\n",
    "            self.logger.info(f\"üíæ Saved: {filepath.name}\")\n",
    "        \n",
    "        health_sources = ['who']\n",
    "        health_data = []\n",
    "        for source in health_sources:\n",
    "            if source in cleaned_data and not cleaned_data[source].empty:\n",
    "                df = cleaned_data[source].copy()\n",
    "                df['data_source'] = source\n",
    "                health_data.append(df)\n",
    "        \n",
    "        if health_data:\n",
    "            health_consolidated = pd.concat(health_data, ignore_index=True)\n",
    "            final_datasets['health_indicators'] = health_consolidated\n",
    "            filepath = self.directories['final_data'] / 'health_indicators.csv'\n",
    "            health_consolidated.to_csv(filepath, index=False, encoding='utf-8')\n",
    "            self.logger.info(f\"üíæ Saved: {filepath.name}\")\n",
    "        \n",
    "        geo_sources = [k for k in cleaned_data.keys() if 'geographic' in k]\n",
    "        for source in geo_sources:\n",
    "            if source in cleaned_data and not cleaned_data[source].empty:\n",
    "                df = cleaned_data[source].copy()\n",
    "                final_datasets[source] = df\n",
    "                filepath = self.directories['final_data'] / f'{source}.csv'\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "                self.logger.info(f\"üíæ Saved: {filepath.name}\")\n",
    "        \n",
    "        other_sources = [k for k in cleaned_data.keys() \n",
    "                        if k not in economic_sources + health_sources + geo_sources]\n",
    "        for source in other_sources:\n",
    "            if source in cleaned_data and not cleaned_data[source].empty:\n",
    "                df = cleaned_data[source].copy()\n",
    "                final_datasets[source] = df\n",
    "                filepath = self.directories['final_data'] / f'{source}.csv'\n",
    "                df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "                self.logger.info(f\"üíæ Saved: {filepath.name}\")\n",
    "        \n",
    "        self.logger.info(f\"‚úÖ {len(final_datasets)} final datasets created\")\n",
    "        return final_datasets\n",
    "\n",
    "    @timer(operation_name=\"Orchestrator.full_collection\", track_metrics=True)\n",
    "    def run_full_collection(\n",
    "            self, collectors: Optional[List[str]] = None\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        collectors = collectors or list(self.collectors.keys())\n",
    "        results = {}\n",
    "        self.logger.info(f\"üöÄ Starting collection ({len(collectors)} collectors)\")\n",
    "        with track_progress(\n",
    "                collectors, desc=\"Global collection\", unit=\"collector\"\n",
    "        ) as pbar:\n",
    "            for collector_name in pbar:\n",
    "                if collector_name not in self.collectors:\n",
    "                    self.logger.warning(f\"‚ö†Ô∏è Unknown collector: {collector_name}\")\n",
    "                    continue\n",
    "                pbar.set_postfix_str(f\"Collector: {collector_name}\")\n",
    "                self.logger.info(f\"‚ñ∂Ô∏è Starting: {collector_name}\")\n",
    "                try:\n",
    "                    collector = self.collectors[collector_name]\n",
    "                    data = collector.collect_data()\n",
    "                    if collector_name == \"geographic\" and isinstance(data, dict):\n",
    "                        for key, df in data.items():\n",
    "                            if not df.empty:\n",
    "                                filename = f\"{collector_name}_{key}.csv\"\n",
    "                                filepath = self.directories[\"raw\"] / filename\n",
    "                                collector.save_data(df, filepath)\n",
    "                                results[f\"{collector_name}_{key}\"] = df\n",
    "                        combined = pd.concat(\n",
    "                            [df for df in data.values() if not df.empty],\n",
    "                            ignore_index=True,\n",
    "                        )\n",
    "                        if not combined.empty:\n",
    "                            results[collector_name] = combined\n",
    "                    elif isinstance(data, pd.DataFrame) and not data.empty:\n",
    "                        results[collector_name] = data\n",
    "                        filename = f\"{collector_name}_data.csv\"\n",
    "                        filepath = self.directories[\"raw\"] / filename\n",
    "                        collector.save_data(data, filepath)\n",
    "                    record_count = (\n",
    "                        len(data)\n",
    "                        if isinstance(data, pd.DataFrame)\n",
    "                        else sum(\n",
    "                            len(df)\n",
    "                            for df in data.values()\n",
    "                            if isinstance(df, pd.DataFrame)\n",
    "                        )\n",
    "                    )\n",
    "                    self.logger.info(f\"‚úÖ {collector_name}: {record_count} records\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"‚ùå Error {collector_name}: {e}\", exc_info=True)\n",
    "        total_records = sum(\n",
    "            len(df) for df in results.values() if isinstance(df, pd.DataFrame)\n",
    "        )\n",
    "        self.logger.info(f\"üèÅ Collection done: {total_records} records\")\n",
    "        return results\n",
    "\n",
    "    @timer(operation_name=\"Orchestrator.full_cleaning\", track_metrics=True)\n",
    "    def run_full_cleaning(\n",
    "            self, raw_data: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        self.logger.info(f\"üßπ Start cleaning ({len(raw_data)} sources)\")\n",
    "        cleaned_data = {}\n",
    "        with track_progress(raw_data.items(), desc=\"Cleaning\", unit=\"source\") as pbar:\n",
    "            for source_name, df in pbar:\n",
    "                if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "                    continue\n",
    "                pbar.set_postfix_str(f\"Source: {source_name}\")\n",
    "                try:\n",
    "                    cleaned_df, report = self.cleaner.clean_dataset(df, source_name)\n",
    "                    if \"world_bank\" in source_name.lower():\n",
    "                        cleaned_df = self.cleaner.clean_world_bank_data(cleaned_df)\n",
    "                    elif \"geographic\" in source_name.lower():\n",
    "                        cleaned_df = self.cleaner.clean_geographic_data(cleaned_df)\n",
    "                    if not cleaned_df.empty:\n",
    "                        cleaned_data[source_name] = cleaned_df\n",
    "                        filepath = (\n",
    "                                self.directories[\"processed\"] / f\"{source_name}_cleaned.csv\"\n",
    "                        )\n",
    "                        cleaned_df.to_csv(filepath, index=False, encoding=\"utf-8\")\n",
    "                        self.logger.info(f\"üíæ Saved: {filepath.name}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"‚ùå Cleaning error {source_name}: {e}\")\n",
    "        self.logger.info(f\"‚úÖ Cleaning done: {len(cleaned_data)} sources processed\")\n",
    "        return cleaned_data\n",
    "\n",
    "    def generate_collection_summary(\n",
    "            self, data: Dict[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "        summary = []\n",
    "        for source, df in data.items():\n",
    "            if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "                continue\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            date_cols = df.select_dtypes(include=[\"datetime64\"]).columns\n",
    "            summary.append(\n",
    "                {\n",
    "                    \"source\": source,\n",
    "                    \"records\": len(df),\n",
    "                    \"columns\": len(df.columns),\n",
    "                    \"memory_mb\": round(df.memory_usage(deep=True).sum() / (1024 ** 2), 2),\n",
    "                    \"has_nulls\": df.isnull().any().any(),\n",
    "                    \"null_pct\": round(\n",
    "                        df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100, 2\n",
    "                    ),\n",
    "                    \"numeric_cols\": len(numeric_cols),\n",
    "                    \"date_cols\": len(date_cols),\n",
    "                    \"duplicates\": df.duplicated().sum(),\n",
    "                    \"date\": datetime.now().date(),\n",
    "                }\n",
    "            )\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "        if not summary_df.empty:\n",
    "            filepath = self.directories[\"processed\"] / \"collection_summary.csv\"\n",
    "            summary_df.to_csv(filepath, index=False, encoding=\"utf-8\")\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(\"üìä COLLECTION SUMMARY\")\n",
    "            print(\"=\" * 100)\n",
    "            print(summary_df.to_string(index=False))\n",
    "            print(\"=\" * 100 + \"\\n\")\n",
    "        return summary_df\n",
    "\n",
    "    def validate_data_quality(\n",
    "            self, data: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, List[str]]:\n",
    "        self.logger.info(\"üîç Data quality validation\")\n",
    "        issues = {}\n",
    "        for source, df in data.items():\n",
    "            if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "                continue\n",
    "            source_issues = []\n",
    "            dup_count = df.duplicated().sum()\n",
    "            if dup_count > 0:\n",
    "                source_issues.append(f\"Duplicates: {dup_count}\")\n",
    "            high_null_cols = df.columns[df.isnull().sum() / len(df) > 0.5]\n",
    "            if len(high_null_cols) > 0:\n",
    "                source_issues.append(f\"Columns >50% nulls: {list(high_null_cols)}\")\n",
    "            if \"year\" in df.columns:\n",
    "                invalid_years = df[(df[\"year\"] < 1900) | (df[\"year\"] > 2025)]\n",
    "                if len(invalid_years) > 0:\n",
    "                    source_issues.append(f\"Invalid years: {len(invalid_years)}\")\n",
    "            if source_issues:\n",
    "                issues[source] = source_issues\n",
    "        if issues:\n",
    "            self.logger.warning(f\"‚ö†Ô∏è Issues detected in {len(issues)} sources\")\n",
    "            for source, issue_list in issues.items():\n",
    "                for issue in issue_list:\n",
    "                    self.logger.warning(f\"  - {source}: {issue}\")\n",
    "        else:\n",
    "            self.logger.info(\"‚úÖ No quality issues detected\")\n",
    "        return issues\n",
    "\n",
    "    def create_data_dictionary(self, data: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "        self.logger.info(\"üìñ Creating data dictionary\")\n",
    "        dictionary = []\n",
    "        for source, df in data.items():\n",
    "            if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "                continue\n",
    "            for col in df.columns:\n",
    "                entry = {\n",
    "                    \"source\": source,\n",
    "                    \"variable\": col,\n",
    "                    \"type\": str(df[col].dtype),\n",
    "                    \"non_null_count\": df[col].notna().sum(),\n",
    "                    \"null_count\": df[col].isnull().sum(),\n",
    "                    \"null_pct\": round(df[col].isnull().sum() / len(df) * 100, 2),\n",
    "                    \"unique_values\": df[col].nunique(),\n",
    "                }\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    entry.update(\n",
    "                        {\n",
    "                            \"min\": df[col].min(),\n",
    "                            \"max\": df[col].max(),\n",
    "                            \"mean\": round(df[col].mean(), 2),\n",
    "                            \"median\": df[col].median(),\n",
    "                        }\n",
    "                    )\n",
    "                dictionary.append(entry)\n",
    "        dict_df = pd.DataFrame(dictionary)\n",
    "        if not dict_df.empty:\n",
    "            filepath = self.directories[\"docs\"] / \"data_dictionary.csv\"\n",
    "            dict_df.to_csv(filepath, index=False, encoding=\"utf-8\")\n",
    "            self.logger.info(f\"üíæ Dictionary saved: {filepath}\")\n",
    "        return dict_df\n",
    "\n",
    "    @timer(operation_name=\"Orchestrator.complete_pipeline\", track_metrics=True)\n",
    "    def run_complete_pipeline(self) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"üöÄ STARTING COMPLETE ANIP PIPELINE\")\n",
    "        print(\"=\" * 100 + \"\\n\")\n",
    "        print(\"üì° STEP 1/4: DATA COLLECTION\")\n",
    "        print(\"-\" * 100)\n",
    "        raw_data = self.run_full_collection()\n",
    "        print(\"\\nüîç STEP 2/4: INITIAL VALIDATION\")\n",
    "        print(\"-\" * 100)\n",
    "        initial_issues = self.validate_data_quality(raw_data)\n",
    "        print(\"\\nüßπ STEP 3/4: DATA CLEANING\")\n",
    "        print(\"-\" * 100)\n",
    "        cleaned_data = self.run_full_cleaning(raw_data)\n",
    "        print(\"\\nüìä STEP 4/4: GENERATING DELIVERABLES\")\n",
    "        print(\"-\" * 100)\n",
    "        collection_summary = self.generate_collection_summary(cleaned_data)\n",
    "        cleaning_summary = self.cleaner.generate_cleaning_summary()\n",
    "        data_dictionary = self.create_data_dictionary(cleaned_data)\n",
    "        \n",
    "        print(\"\\nüîó STEP 5/5: CONSOLIDATION & EXPORT\")\n",
    "        print(\"-\" * 100)\n",
    "        final_datasets = self.consolidate_final_data(cleaned_data)\n",
    "        \n",
    "        final_issues = self.validate_data_quality(cleaned_data)\n",
    "        if not cleaning_summary.empty:\n",
    "            filepath = self.directories[\"processed\"] / \"cleaning_summary.csv\"\n",
    "            cleaning_summary.to_csv(filepath, index=False, encoding=\"utf-8\")\n",
    "            print(f\"\\nüíæ Cleaning summary: {filepath}\")\n",
    "            print(\"\\n\" + cleaning_summary.to_string(index=False))\n",
    "        _global_tracker.print_summary()\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"‚úÖ PIPELINE COMPLETED\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"üìÇ Raw data: {self.directories['raw']}\")\n",
    "        print(f\"üìÇ Cleaned data: {self.directories['processed']}\")\n",
    "        print(f\"üìÇ Final data: {self.directories['final_data']}\")\n",
    "        print(f\"üìÇ Documentation: {self.directories['docs']}\")\n",
    "        print(\"=\" * 100 + \"\\n\")\n",
    "        return {\n",
    "            \"raw_data\": raw_data,\n",
    "            \"cleaned_data\": cleaned_data,\n",
    "            \"final_datasets\": final_datasets,\n",
    "            \"collection_summary\": collection_summary,\n",
    "            \"cleaning_summary\": cleaning_summary,\n",
    "            \"data_dictionary\": data_dictionary,\n",
    "            \"initial_issues\": initial_issues,\n",
    "            \"final_issues\": final_issues,\n",
    "            \"performance_metrics\": _global_tracker.get_summary(),\n",
    "        }"
   ],
   "id": "cd66812fbbe4c0dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@timer(operation_name=\"Main.execution\", track_metrics=True)\n",
    "def main():\n",
    "    setup_environment(log_dir=Path(\"logs\"))\n",
    "    orchestrator = DataCollectorOrchestrator()\n",
    "    results = orchestrator.run_complete_pipeline()\n",
    "    print(\"\\nüìã FINAL RESULTS:\")\n",
    "    print(f\"  - Sources collected: {len(results['raw_data'])}\")\n",
    "    print(f\"  - Sources cleaned: {len(results['cleaned_data'])}\")\n",
    "    print(f\"  - Final datasets: {len(results['final_datasets'])}\")\n",
    "    print(f\"  - Variables documented: {len(results['data_dictionary'])}\")\n",
    "    if results[\"final_issues\"]:\n",
    "        print(f\"  ‚ö†Ô∏è Remaining issues: {len(results['final_issues'])} sources\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ No quality issues detected\")\n",
    "    print(\"\\n‚è±Ô∏è GLOBAL PERFORMANCE:\")\n",
    "    perf = results[\"performance_metrics\"]\n",
    "    print(f\"  - Total operations: {perf['total_operations']}\")\n",
    "    print(f\"  - Successful: {perf['successful']}\")\n",
    "    print(f\"  - Failed: {perf['failed']}\")\n",
    "    print(f\"  - Total duration: {perf['total_duration']}s\")\n",
    "    print(f\"  - Average duration: {perf['avg_duration']}s\")\n",
    "    print(f\"  - Items processed: {perf['total_items']}\")\n",
    "    performance_df = pd.DataFrame([m.to_dict() for m in _global_tracker.metrics])\n",
    "    perf_filepath = orchestrator.directories[\"logs\"] / \"performance_metrics.csv\"\n",
    "    performance_df.to_csv(perf_filepath, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nüíæ Performance metrics: {perf_filepath}\")\n",
    "    return results"
   ],
   "id": "b13ec4333a4c9d7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "main()",
   "id": "d61e6fc7dbc367d2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
