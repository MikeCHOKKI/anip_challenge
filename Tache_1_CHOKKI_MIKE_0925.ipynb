{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ANIP Challenge ‚Äì T√¢che 1 : Collecte & Pr√©paration des Donn√©es\n",
    "\n",
    "## üéØ Objectif\n",
    "Cette premi√®re t√¢che consiste √† collecter, nettoyer et pr√©parer les donn√©es mises √† disposition\n",
    "(par l‚ÄôANIP ou d‚Äôautres sources ouvertes si n√©cessaire) afin de produire un dataset exploitable,\n",
    "coh√©rent et document√©.\n",
    "Le r√©sultat de cette √©tape doit permettre d‚Äôentamer l‚Äôanalyse et la visualisation (T√¢che 2)\n",
    "sans ambigu√Øt√©s ni incoh√©rences.\n",
    "\n",
    "## Livrables attendus\n",
    "- Un ou plusieurs **datasets finaux** (CSV/Excel) nettoy√©s, harmonis√©s et document√©s.\n",
    "- Un **notebook de pr√©paration** (ce fichier), incluant :\n",
    "- La collecte (scraping / API / importation de fichiers)\n",
    "- Le nettoyage (gestion des valeurs manquantes, doublons, incoh√©rences)\n",
    "- L‚Äôharmonisation (format des dates, unit√©s, typages, renommage des colonnes, etc.)\n",
    "- Un **glossaire/dictionnaire des variables** d√©crivant chaque champ :\n",
    "- Nom de la variable\n",
    "- D√©finition\n",
    "- Unit√© de mesure (si applicable)\n",
    "- Source et p√©riode couverte\n",
    "\n",
    "## Structure du projet\n",
    "- `data/raw/` : donn√©es brutes (telles que collect√©es)\n",
    "- `data/processed/` : donn√©es nettoy√©es et harmonis√©es (r√©sultats de la T√¢che 1)\n",
    "- `docs/glossaire.md` : glossaire/dictionnaire des variables\n",
    "- `notebooks/Tache_1_Nom_Prenom_JJMM.ipynb` : ce notebook"
   ],
   "id": "f898971c98de0159"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration des imports",
   "id": "25c7c71286dbd363"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T21:50:51.303928Z",
     "start_time": "2025-09-23T21:50:51.285233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import warnings\n",
    "import logging\n",
    "from io import BytesIO, StringIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Union, Any\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup"
   ],
   "id": "3f1ba0863aa55543",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration Globale",
   "id": "c7e6bb4361f61de4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T21:50:51.328718Z",
     "start_time": "2025-09-23T21:50:51.312731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class GlobalConfig:\n",
    "    \"\"\"\n",
    "    GlobalConfig serves as a centralized configuration data class that stores essential\n",
    "    constants and settings for the application. It provides default values for parameters related\n",
    "    to country details, API configurations, request handling, and datasets. It aims to reduce\n",
    "    hardcoding of key values and centralize configuration management for better maintainability.\n",
    "\n",
    "    :ivar COUNTRY_CODE: ISO 3166-1 alpha-2 code representing the country.\n",
    "    :type COUNTRY_CODE: str\n",
    "    :ivar COUNTRY_NAME: The official name of the country.\n",
    "    :type COUNTRY_NAME: str\n",
    "    :ivar START_YEAR: The starting year for data processing or analysis.\n",
    "    :type START_YEAR: int\n",
    "    :ivar END_YEAR: The ending year for data processing or analysis.\n",
    "    :type END_YEAR: int\n",
    "    :ivar WORLD_BANK_API_URL: The base URL for the World Bank API.\n",
    "    :type WORLD_BANK_API_URL: str\n",
    "    :ivar INSTAD_BASE_URL: The base URL for the INSTAD application.\n",
    "    :type INSTAD_BASE_URL: str\n",
    "    :ivar OVERPASS_API_URL: The base URL for the Overpass API.\n",
    "    :type OVERPASS_API_URL: str\n",
    "    :ivar DEFAULT_PER_PAGE: The default number of items per page for paginated results.\n",
    "    :type DEFAULT_PER_PAGE: int\n",
    "    :ivar REQUEST_TIMEOUT: The duration (in seconds) before a request times out.\n",
    "    :type REQUEST_TIMEOUT: int\n",
    "    :ivar RETRY_ATTEMPTS: The number of retry attempts allowed after a failed request.\n",
    "    :type RETRY_ATTEMPTS: int\n",
    "    :ivar DELAY_BETWEEN_REQUESTS: The delay (in seconds) between successive requests to APIs,\n",
    "        to avoid throttling or rate-limiting issues.\n",
    "    :type DELAY_BETWEEN_REQUESTS: float\n",
    "    :ivar DEFAULT_WB_INDICATORS: A list of default World Bank indicators used for data\n",
    "        retrieval, represented by their respective codes.\n",
    "    :type DEFAULT_WB_INDICATORS: List[str]\n",
    "    :ivar OSM_ADMIN_LEVELS: A dictionary mapping administrative levels to their corresponding\n",
    "        numeric codes as used in OpenStreetMap (OSM) data.\n",
    "    :type OSM_ADMIN_LEVELS: Dict[str, str]\n",
    "    :ivar EXTERNAL_CSV_URLS: A list of URLs pointing to external CSV resources used for\n",
    "        additional data ingestion.\n",
    "    :type EXTERNAL_CSV_URLS: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    COUNTRY_CODE: str = \"BJ\"\n",
    "    COUNTRY_NAME: str = \"B√©nin\"\n",
    "\n",
    "    START_YEAR: int = 2015\n",
    "    END_YEAR: int = 2025\n",
    "\n",
    "    WORLD_BANK_API_URL: str = \"https://api.worldbank.org/v2\"\n",
    "    INSTAD_BASE_URL: str = \"https://instad.bj\"\n",
    "    OVERPASS_API_URL: str = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "    DEFAULT_PER_PAGE: int = 100\n",
    "    REQUEST_TIMEOUT: int = 30\n",
    "    RETRY_ATTEMPTS: int = 3\n",
    "    DELAY_BETWEEN_REQUESTS: float = 0.5\n",
    "\n",
    "    DEFAULT_WB_INDICATORS: List[str] = field(default_factory=lambda: [\n",
    "        \"SP.POP.TOTL\",  # Population totale\n",
    "        \"NY.GDP.MKTP.CD\",  # PIB (USD courants)\n",
    "        \"NY.GDP.PCAP.CD\",  # PIB par habitant\n",
    "        \"SE.PRM.NENR\",  # Taux net de scolarisation primaire\n",
    "        \"SH.DYN.MORT\",  # Taux de mortalit√© infantile\n",
    "        \"AG.LND.TOTL.K2\",  # Superficie totale (km¬≤)\n",
    "        \"SL.TLF.TOTL.IN\",  # Population active totale\n",
    "        \"SP.DYN.TFRT.IN\",  # Indice synth√©tique de f√©condit√©\n",
    "    ])\n",
    "\n",
    "    OSM_ADMIN_LEVELS: Dict[str, str] = field(default_factory=lambda: {\n",
    "        \"pays\": \"2\",\n",
    "        \"d√©partement\": \"4\",\n",
    "        \"commune\": \"6\"\n",
    "    })\n",
    "\n",
    "    EXTERNAL_CSV_URLS: List[str] = field(default_factory=lambda: [\n",
    "        \"https://data.uis.unesco.org/medias/education/SDG4.csv\",\n",
    "    ])"
   ],
   "id": "225b3b114bee0051",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration du Logging et de pandas",
   "id": "bc8d5b0a7a07507e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T21:50:51.348041Z",
     "start_time": "2025-09-23T21:50:51.335069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configuration_environnement():\n",
    "    \"\"\"\n",
    "    Configures the runtime environment by suppressing specific warnings, setting logging\n",
    "    parameters, configuring pandas display options, and customizing matplotlib and seaborn\n",
    "    styles.\n",
    "\n",
    "    The function is designed to improve the clarity and readability of outputs during\n",
    "    data analysis and visualization tasks.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 100)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "    pd.set_option(\"display.expand_frame_repr\", False)\n",
    "    pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "    plt.rcParams.update({\n",
    "        \"figure.figsize\": (12, 8),\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 10,\n",
    "    })\n",
    "    sns.set_palette(\"Set2\")\n",
    "\n",
    "    logging.info(\"Environnement configur√© avec succ√®s\")\n",
    "    logging.info(f\"D√©but de collecte: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ],
   "id": "8ba00f7ff0661b96",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration du gestionnaire des r√©pertoires",
   "id": "963e88bbcf7505a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T21:50:51.369099Z",
     "start_time": "2025-09-23T21:50:51.355273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GestionnaireRepertoire:\n",
    "    \"\"\"\n",
    "    Manages a directory structure, providing functionality to initialize, create, and retrieve\n",
    "    specific directories. This class encapsulates logic for directory handling, logging\n",
    "    processes, and organizing the structure based on pre-defined specifications.\n",
    "\n",
    "    Intended for use in applications requiring consistent folder structures, such as\n",
    "    data processing, logging, and result exports.\n",
    "\n",
    "    :ivar base_dir: The base directory from which the folder structure is created\n",
    "        and managed. Defaults to the current directory if not specified.\n",
    "    :type base_dir: Path\n",
    "    :ivar logger: Logger instance used for logging activities and debug information\n",
    "        during directory operations.\n",
    "    :type logger: logging.Logger\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dir: Optional[Path] = None):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the class, setting up a base directory and a logger for\n",
    "        the instance. The constructor allows an optional base directory to be specified\n",
    "        or defaults to the current directory.\n",
    "\n",
    "        :param base_dir: Optional base directory for the instance.\n",
    "        :type base_dir: Optional[Path]\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._directory = {}\n",
    "\n",
    "    def _create_directory(self, name: str, path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Creates a directory at the specified path if it doesn't already exist.\n",
    "\n",
    "        This method attempts to create a directory while logging the process. If\n",
    "        the directory already exists, it logs that information as a debug statement.\n",
    "        If the directory is successfully created, it logs this as an info statement.\n",
    "        In case of an error during the creation process, the error is logged as an\n",
    "        error statement, and the exception is re-raised.\n",
    "\n",
    "        :param name: Name of the directory for descriptive logging.\n",
    "        :type name: str\n",
    "        :param path: Path object representing the location where the directory\n",
    "            needs to be created.\n",
    "        :type path: Path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if path.exists():\n",
    "                self.logger.debug(f\"{name} - {path} (existe d√©j√†)\")\n",
    "            else:\n",
    "                path.mkdir(parents=True, exist_ok=True)\n",
    "                self.logger.info(f\"{name} - {path} cr√©er avec succ√®s\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erreur lors de la cr√©ation de {name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_directory_structure(self) -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Initializes and creates the directory structure for the application, ensuring\n",
    "        all required folders exist based on the pre-defined structure. If a folder\n",
    "        does not exist, it will be created. The directory structure includes folders\n",
    "        for storing data, logs, and exported files. This function logs the number\n",
    "        of directories created upon completion.\n",
    "\n",
    "        :return: A dictionary mapping directory names to their corresponding `Path`\n",
    "                 objects after initialization.\n",
    "        :rtype: Dict[str, Path]\n",
    "        \"\"\"\n",
    "        structure = {\n",
    "            \"data\": \"data\",\n",
    "            \"raw\": \"data/raw\",\n",
    "            \"processed\": \"data/processed\",\n",
    "            \"final_data\": \"data/final_data\",\n",
    "            \"logs\": \"logs\",\n",
    "            \"exports\": \"exports\"\n",
    "        }\n",
    "\n",
    "        self._directory = {\n",
    "            name: self._create_directory(name, self.base_dir / path) or self.base_dir / path for name, path in\n",
    "            structure.items()\n",
    "        }\n",
    "\n",
    "        self.logger.info(f\"Structure de {len(self._directory)} dossier cr√©er \")\n",
    "\n",
    "        return self._directory\n",
    "\n",
    "    def get_path(self, name: str) -> Path:\n",
    "        \"\"\"\n",
    "        Retrieve the Path associated with a given name. If the name is not found in the\n",
    "        directory, the base directory is returned as a fallback.\n",
    "\n",
    "        :param name: The name to look up in the directory.\n",
    "        :type name: str\n",
    "        :return: The Path associated with the given name, or the base directory if not found.\n",
    "        :rtype: Path\n",
    "        \"\"\"\n",
    "        return self._directory.get(name, self.base_dir)"
   ],
   "id": "fb6d69bf36410e86",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Collecteurs de donn√©es",
   "id": "774ed4a49d08ced0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classe abstraite centraliser pour tout les types de collecteurs",
   "id": "782db3f9475e8676"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T21:50:51.391977Z",
     "start_time": "2025-09-23T21:50:51.375767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AbstractCollector(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for data collection.\n",
    "\n",
    "    This base class provides a framework for collecting data, making HTTP requests with\n",
    "    retries, and saving collected data in various formats. Subclasses must implement\n",
    "    the abstract `collect_data` method to fetch specific data as required. Additional\n",
    "    utility methods assist in creating HTTP sessions, handling retries, and logging\n",
    "    information.\n",
    "\n",
    "    :ivar config: Configuration object containing settings like retry attempts and\n",
    "                  request timeouts.\n",
    "    :type config: GlobalConfig\n",
    "    :ivar logger: Logger instance for capturing information, warnings, and errors during\n",
    "                  the collector's lifecycle.\n",
    "    :type logger: logging.Logger\n",
    "    :ivar session: Configured HTTP session for making requests with customized headers\n",
    "                   for user agents and language preferences.\n",
    "    :type session: requests.Session\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GlobalConfig):\n",
    "        \"\"\"\n",
    "        Initializes the object with a configuration, sets up a logger,\n",
    "        and creates an HTTP session for subsequent usage.\n",
    "\n",
    "        :param config: The global configuration object to initialize the instance with\n",
    "        :type config: GlobalConfig\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.session = self._create_session()\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_session() -> requests.Session:\n",
    "        \"\"\"\n",
    "        Create and configure a new session with custom headers.\n",
    "\n",
    "        The session is initialized with a user-agent string tailored for\n",
    "        educational research bots, and default headers for accepting JSON,\n",
    "        HTML, and other content types. Default language preference is set\n",
    "        to French with English as a fallback.\n",
    "\n",
    "        :return: A configured `requests.Session` with updated headers.\n",
    "        :rtype: requests.Session\n",
    "        \"\"\"\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Educational Research Bot/1.0)\",\n",
    "            \"Accept\": \"application/json, text/html, */*\",\n",
    "            \"Accept-Language\": \"fr,en;q=0.9\",\n",
    "        })\n",
    "        return session\n",
    "\n",
    "    def _make_request_with_retry(self, url: str, **kwargs) -> Optional[requests.Response]:\n",
    "        \"\"\"\n",
    "        Retries an HTTP request a set number of times in case of failure. Logs each\n",
    "        attempt and, upon persistent failure, logs an error message. Incorporates\n",
    "        exponential backoff in case of retries.\n",
    "\n",
    "        :param url: The URL to which the HTTP request is made\n",
    "        :param kwargs: Additional keyword arguments to customize the request, such as\n",
    "                       headers, data, or query parameters\n",
    "        :return: A requests.Response object if the request is successful; None if all\n",
    "                 retry attempts failed\n",
    "        \"\"\"\n",
    "        for attempt in range(self.config.RETRY_ATTEMPTS):\n",
    "            try:\n",
    "                response = self.session.request(method=kwargs.pop(\"method\", \"get\"),\n",
    "                                                url=url,\n",
    "                                                timeout=self.config.REQUEST_TIMEOUT,\n",
    "                                                **kwargs)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.RequestException as e:\n",
    "                self.logger.warning(f\"üîÑ Tentative {attempt + 1}/{self.config.RETRY_ATTEMPTS} √©chou√©e pour {url}: {e}\")\n",
    "                if attempt < self.config.RETRY_ATTEMPTS - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "\n",
    "        self.logger.error(f\"‚ùå √âchec d√©finitif pour {url}\")\n",
    "        return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def collect_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Defines an abstract method to collect data, which must be implemented by\n",
    "        subclasses. This method is expected to return data in the form of a\n",
    "        DataFrame object from the pandas library.\n",
    "\n",
    "        :raises NotImplementedError: If the subclass does not implement this method.\n",
    "\n",
    "        :returns: A pandas DataFrame containing the collected data.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save_data(self, data: pd.DataFrame, file_path: Path, format_type: str = \"csv\") -> bool:\n",
    "        \"\"\"\n",
    "        Saves a provided dataset to a chosen file format at a specified file path. The supported file\n",
    "        formats are CSV, Excel, JSON, and Parquet. Metadata such as the collection time, date, and\n",
    "        collector class is appended to the dataset prior to saving. Logs will capture the success\n",
    "        or failure of the operation along with the file size and number of rows, if applicable.\n",
    "\n",
    "        :param data: The pandas DataFrame containing the data to be saved.\n",
    "        :param file_path: Path object representing the file to which the data will be saved.\n",
    "        :param format_type: Desired file format for saving the data. Supported values are 'csv',\n",
    "            'excel', 'json', and 'parquet'. Default is 'csv'.\n",
    "        :return: A boolean indicating whether the data was successfully saved.\n",
    "        \"\"\"\n",
    "        if data.empty:\n",
    "            self.logger.warning(\"Aucune donn√©es √† sauvegarder\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            meta_data = data.copy()\n",
    "            meta_data['collection_time'] = datetime.now().time()\n",
    "            meta_data['collection_date'] = datetime.now().date()\n",
    "            meta_data['collector_class'] = self.__class__.__name__\n",
    "\n",
    "            if format_type.lower() == \"csv\":\n",
    "                data.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
    "            elif format_type.lower() == \"excel\":\n",
    "                data.to_excel(file_path, index=False, engine=\"xlsxwriter\")\n",
    "            elif format_type.lower() == \"json\":\n",
    "                data.to_json(file_path, orient=\"records\", force_ascii=False, indent=4, date_format=\"iso\")\n",
    "            elif format_type.lower() == \"parquet\":\n",
    "                data.to_parquet(file_path, index=False, engine=\"pyarrow\")\n",
    "            else:\n",
    "                raise ValueError(\"Format non valide. Vous ne pouvez choisir que csv, excel, json ou parquet.\")\n",
    "\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            self.logger.info(f\"Sauvegard√©: {file_path.name} ({len(data)} lignes, {size_mb:.2f} MB)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erreur lors de la sauvegarde {file_path}: {e}\")\n",
    "            return False"
   ],
   "id": "3a5cef037487f98f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collecteur de donn√©es pour World Bank",
   "id": "7254e2d7302ec811"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T21:50:51.412136Z",
     "start_time": "2025-09-23T21:50:51.398745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WorldBankDataCollector(AbstractCollector):\n",
    "    \"\"\"\n",
    "    Handles data collection from the World Bank API.\n",
    "\n",
    "    This class provides methods for fetching data from the World Bank for specific indicators\n",
    "    over a range of years. The data is formatted into pandas DataFrames for further analysis\n",
    "    and processing, supporting features like pagination and retrying on request failures.\n",
    "\n",
    "    :ivar config: Configuration object containing API settings, such as base URL, country\n",
    "        code, default indicators, start and end year, and delay between requests.\n",
    "    :type config: object\n",
    "    :ivar logger: Logger instance used for logging information, warnings, and errors during\n",
    "        the data collection process.\n",
    "    :type logger: logging.Logger\n",
    "    \"\"\"\n",
    "\n",
    "    def _fetch_indicator_data(self, indicator: str, start_year: int = None, end_year: int = None,\n",
    "                              per_page: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetches indicator data from the World Bank API for a specific country within a specified time range\n",
    "        and formats the data into a pandas DataFrame. The function allows filtering by start and end years\n",
    "        and configuring the number of entries per page.\n",
    "\n",
    "        :param indicator: The unique identifier of the indicator data to fetch.\n",
    "        :type indicator: str\n",
    "        :param start_year: The starting year for the data range (default is derived from configuration).\n",
    "        :type start_year: int, optional\n",
    "        :param end_year: The ending year for the data range (default is derived from configuration).\n",
    "        :type end_year: int, optional\n",
    "        :param per_page: The number of results per page to fetch (default is derived from configuration).\n",
    "        :type per_page: int, optional\n",
    "\n",
    "        :return: A pandas DataFrame containing the requested indicator data with columns such as:\n",
    "            `indicator_code`, `indicator_name`, `country_code`, `country_name`, `year`, `value`,\n",
    "            and `source`.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        url = f\"{self.config.WORLD_BANK_API_URL}/country/{self.config.COUNTRY_CODE}/indicator/{indicator}\"\n",
    "\n",
    "        params = {\n",
    "            \"data\": f\"{start_year or self.config.START_YEAR}:{end_year or self.config.END_YEAR}\",\n",
    "            \"format\": \"json\",\n",
    "            \"per_page\": per_page or self.config.DEFAULT_PER_PAGE,\n",
    "        }\n",
    "\n",
    "        response = self._make_request_with_retry(url, params=params)\n",
    "\n",
    "        if response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            entries = data[1] if isinstance(data, list) and len(data) > 1 and data[1] else []\n",
    "\n",
    "            record = []\n",
    "\n",
    "            for entry in entries:\n",
    "                record.append({\n",
    "                    \"indicator_code\": entry[\"indicator\"][\"id\"],\n",
    "                    \"indicator_name\": entry[\"indicator\"][\"value\"],\n",
    "                    \"country_code\": entry[\"country\"][\"id\"],\n",
    "                    \"country_name\": entry[\"country\"][\"value\"],\n",
    "                    \"year\": pd.to_numeric(entry[\"date\"], errors=\"coerce\"),\n",
    "                    \"value\": pd.to_numeric(entry[\"value\"], errors=\"coerce\"),\n",
    "                    \"source\": \"World Bank API\",\n",
    "                })\n",
    "\n",
    "            return pd.DataFrame(record)\n",
    "        except (ValueError, KeyError) as e:\n",
    "            self.logger.error(f\"Erreur parsing JSON pour {indicator}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def collect_data(self, indicators: Optional[List[str]] = None, start_year: int = None, end_year: int = None,\n",
    "                     per_page: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collects data from the World Bank based on the provided indicators and time frame.\n",
    "\n",
    "        This method facilitates fetching and aggregating data for a given list of\n",
    "        indicators over a specified range of years. The data is retrieved from the\n",
    "        World Bank API, consolidated into a dataframe, and returned for further\n",
    "        processing.\n",
    "\n",
    "        :param indicators: Optional list of indicators to collect data for. If not\n",
    "            provided, default indicators defined in the configuration will be used.\n",
    "        :type indicators: List[str], optional\n",
    "        :param start_year: The start year of the data range to collect. If None,\n",
    "            the default value or full range supported by the API may be used.\n",
    "        :type start_year: int, optional\n",
    "        :param end_year: The end year of the data range to collect. If None, the\n",
    "            default value or full range supported by the API may be used.\n",
    "        :type end_year: int, optional\n",
    "        :param per_page: The number of results to retrieve per page. If None, a\n",
    "            default limit will be used based on API specifications.\n",
    "        :type per_page: int, optional\n",
    "        :return: A DataFrame containing the collected data for the specified\n",
    "            indicators and years. If no data is collected, an empty DataFrame is\n",
    "            returned.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        indicators = indicators or self.config.DEFAULT_WB_INDICATORS\n",
    "\n",
    "        donnees = []\n",
    "        self.logger.info(f\"D√©but collecte World Bank ({len(indicators)} indicateurs)\")\n",
    "\n",
    "        for i, indicator in enumerate(indicators, start=1):\n",
    "            self.logger.info(f\"[{i}/{len(indicators)}] Collecte: {indicator}\")\n",
    "\n",
    "            fetch_data = self._fetch_indicator_data(indicator, start_year, end_year, per_page)\n",
    "            if not fetch_data.empty:\n",
    "                donnees.append(fetch_data)\n",
    "                self.logger.info(f\"{len(fetch_data)} enregistrements pour {indicator}\")\n",
    "\n",
    "            time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        datasets = pd.concat(donnees, ignore_index=True) if donnees else pd.DataFrame()\n",
    "        self.logger.info(f\"Fin collecte World Bank ({len(datasets)} enregistrements)\")\n",
    "        return datasets"
   ],
   "id": "ea33a14249a9c32",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collecte de donn√©es par Web Scraping",
   "id": "d9dc9753fcd1d305"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T21:50:51.432930Z",
     "start_time": "2025-09-23T21:50:51.419172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WebScrapingDataCollector(AbstractCollector):\n",
    "    \"\"\"\n",
    "    Collects and aggregates data by scraping tables from predefined HTML web pages.\n",
    "\n",
    "    This class is used for web scraping tasks where data is extracted from HTML tables\n",
    "    and aggregated into a unified pandas DataFrame. It supports scraping from multiple\n",
    "    URLs and handles metadata tagging for each scraped table, including identifying\n",
    "    the source, the URL, and the table's index. The collected data can be further\n",
    "    processed or analyzed.\n",
    "\n",
    "    :ivar logger: Logger used for logging the scraping process info, warnings, and errors.\n",
    "    :type logger: logging.Logger\n",
    "    \"\"\"\n",
    "\n",
    "    def _scrape_html_tables(self, url: str, source_name: str, max_tables: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrapes tables from an HTML page at the specified URL and returns them as a pandas DataFrame.\n",
    "        Each table is assigned metadata, including its source URL, source name, and an index indicating its\n",
    "        position among the other scraped tables.\n",
    "\n",
    "        :param url: The URL of the webpage containing the HTML tables.\n",
    "        :type url: str\n",
    "        :param source_name: The name of the data source for added metadata within the resulting DataFrame.\n",
    "        :type source_name: str\n",
    "        :param max_tables: The maximum number of tables to scrape from the webpage. Defaults to 5.\n",
    "        :type max_tables: int\n",
    "        :return: A pandas DataFrame containing the combined data from the extracted HTML tables.\n",
    "                 If no tables are found or scraping fails, an empty DataFrame is returned.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        response = self._make_request_with_retry(url)\n",
    "\n",
    "        if not response:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            tables = soup.find_all(\"table\")\n",
    "\n",
    "            self.logger.info(f\"{len(tables)} tableaux trouv√©s sur {url}\")\n",
    "\n",
    "            scraped_tables = []\n",
    "            for i, table in enumerate(tables[:max_tables]):\n",
    "                try:\n",
    "                    dataframe = pd.read_html(StringIO(str(table)))[0]\n",
    "                    dataframe[\"source_url\"] = url\n",
    "                    dataframe[\"source_name\"] = source_name\n",
    "                    dataframe['table_index'] = i\n",
    "                    scraped_tables.append(dataframe)\n",
    "                    self.logger.debug(f\"Tableau {i + 1}: {dataframe.shape}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Erreur tableau {i + 1} sur {url}: {e}\")\n",
    "\n",
    "            dataset = pd.concat(scraped_tables, ignore_index=True) if scraped_tables else pd.DataFrame()\n",
    "            return dataset\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erreur scraping {url}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def collect_data(self, max_tables: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrapes data from predefined URLs and aggregates the results into a single DataFrame.\n",
    "\n",
    "        The method retrieves tables from multiple sources, processes them, and concatenates\n",
    "        the scraped data into one unified DataFrame. If no data is collected, an empty\n",
    "        DataFrame is returned.\n",
    "\n",
    "        :param max_tables: Maximum number of tables to scrape from each URL. Defaults to 5.\n",
    "        :type max_tables: int\n",
    "        :return: Aggregated dataset containing the scraped data from all specified URLs.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        self.logger.info(\"D√©but du web scraping\")\n",
    "\n",
    "        urls_for_scraping = {\n",
    "            \"instad_trimestres\": \"https://instad.bj/publications/publications-trimestrielles\",\n",
    "            \"instad_mois\": \"https://instad.bj/publications/publications-mensuelles\",\n",
    "            \"demographic_external\": [\n",
    "                \"https://hub.worldpop.org/project/categories?id=3\",\n",
    "                \"https://dhsprogram.com/data/available-datasets.cfm\",\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        scraped_data = []\n",
    "        urls = []\n",
    "\n",
    "        for source_name, url_for_scraping in urls_for_scraping.items():\n",
    "            if isinstance(urls_for_scraping, str):\n",
    "                urls = [url_for_scraping]\n",
    "\n",
    "            for url in urls:\n",
    "                donnees = self._scrape_html_tables(url, source_name, max_tables)\n",
    "                if not donnees.empty:\n",
    "                    scraped_data.append(donnees)\n",
    "\n",
    "        datasets = pd.concat(scraped_data, ignore_index=True) if scraped_data else pd.DataFrame()\n",
    "        self.logger.info(f\"Scraping termin√©: {len(datasets)} enregistrements\")\n",
    "        return datasets"
   ],
   "id": "68dc8a0ec4e52433",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Collecte de donn√©es g√©ographique",
   "id": "f2ff19ff068f0272"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T21:50:51.452794Z",
     "start_time": "2025-09-23T21:50:51.440053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GeographicDataCollector(AbstractCollector):\n",
    "    def _execute_overpass_query(self, query: str, data_type: str) -> pd.DataFrame:\n",
    "        response = self._make_request_with_retry(self.config.OVERPASS_API_URL, params={\"data\": query}, method=\"post\")\n",
    "\n",
    "        if response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            elements = data.get(\"elements\", [])\n",
    "\n",
    "            records = []\n",
    "            if not elements.empty:\n",
    "                for element in elements:\n",
    "                    if \"tags\" in element:\n",
    "                        record = {\n",
    "                            \"name\": element[\"tags\"].get(\"name\"),\n",
    "                            \"osm_id\": element.get(\"id\"),\n",
    "                            \"latitude\": element.get(\"lat\") or element.get(\"center\", {}).get(\"lat\"),\n",
    "                            \"longitude\": element.get(\"lon\") or element.get(\"center\", {}).get(\"lon\"),\n",
    "                            \"data_type\": data_type,\n",
    "                            \"source\": \"OpenStreetMap\",\n",
    "                        }\n",
    "\n",
    "                        if data_type == \"cities\":\n",
    "                            record.update({\n",
    "                                \"place_type\": element[\"tags\"].get(\"place\"),\n",
    "                                \"population\": pd.to_numeric(element[\"tags\"].get(\"population\"), errors=\"coerce\")\n",
    "                            })\n",
    "                        else:\n",
    "                            record.update({\n",
    "                                \"admin_level\": element[\"tags\"].get(\"admin_level\"),\n",
    "                                \"wikidata\": element[\"tags\"].get(\"wikidata\")\n",
    "                            })\n",
    "\n",
    "                        records.append(record)\n",
    "\n",
    "                self.logger.info(f\"üìç {len(records)} √©l√©ments {data_type} collect√©s\")\n",
    "                return pd.DataFrame(records)\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except (ValueError, KeyError) as e:\n",
    "            self.logger.error(f\"Erreur parsing Overpass pour {data_type}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _collecte_cities(self) -> pd.DataFrame:\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:60];\n",
    "        area[\"ISO3166-1\"=\"{self.config.COUNTRY_CODE}\"][admin_level=\"2\"];\n",
    "        (\n",
    "          node(area)[\"place\"~\"city|town|village\"];\n",
    "          way(area)[\"place\"~\"city|town|village\"];\n",
    "          relation(area)[\"place\"~\"city|town|village\"];\n",
    "        );\n",
    "        out center tags;\n",
    "        \"\"\"\n",
    "\n",
    "        return self._execute_overpass_query(query, \"cities\")\n",
    "\n",
    "    def _collecte_administratives_boundaries(self, admin_level: str, level_name: str) -> pd.DataFrame:\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:60];\n",
    "        relation[\"boundary\"=\"administrative\"][\"admin_level\"=\"{admin_level}\"][\"name\"~\"{self.config.COUNTRY_NAME}|Benin\"];\n",
    "        out center tags;\n",
    "        \"\"\"\n",
    "\n",
    "        return self._execute_overpass_query(query, f\"admin_boundaries_{level_name}\")\n",
    "\n",
    "    def collect_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        pass"
   ],
   "id": "a58625df1dcab3fc",
   "outputs": [],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
