{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7fefb6201855c4",
   "metadata": {},
   "source": [
    "# ANIP Challenge ‚Äì T√¢che 1 : Collecte & Pr√©paration des Donn√©es\n",
    "#\n",
    "## üéØ Objectif\n",
    "Cette premi√®re t√¢che consiste √† collecter, nettoyer et pr√©parer les donn√©es mises √† disposition\n",
    "(par l‚ÄôANIP ou d‚Äôautres sources ouvertes si n√©cessaire) afin de produire un dataset exploitable,\n",
    "coh√©rent et document√©.\n",
    "Le r√©sultat de cette √©tape doit permettre d‚Äôentamer l‚Äôanalyse et la visualisation (T√¢che 2)\n",
    "sans ambigu√Øt√©s ni incoh√©rences.\n",
    "#\n",
    "## Livrables attendus\n",
    "- Un ou plusieurs **datasets finaux** (CSV/Excel) nettoy√©s, harmonis√©s et document√©s.\n",
    "- Un **notebook de pr√©paration** (ce fichier), incluant :\n",
    "- La collecte (scraping / API / importation de fichiers)\n",
    "- Le nettoyage (gestion des valeurs manquantes, doublons, incoh√©rences)\n",
    "- L‚Äôharmonisation (format des dates, unit√©s, typages, renommage des colonnes, etc.)\n",
    "- Un **glossaire/dictionnaire des variables** d√©crivant chaque champ :\n",
    "- Nom de la variable\n",
    "- D√©finition\n",
    "- Unit√© de mesure (si applicable)\n",
    "- Source et p√©riode couverte\n",
    "#\n",
    "## Structure du projet\n",
    "- `data/raw/` : donn√©es brutes (telles que collect√©es)\n",
    "- `data/processed/` : donn√©es nettoy√©es et harmonis√©es (r√©sultats de la T√¢che 1)\n",
    "- `docs/glossaire.md` : glossaire/dictionnaire des variables\n",
    "- `notebooks/Tache_1_Nom_Prenom_JJMM.ipynb` : ce notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699fec304554ce",
   "metadata": {},
   "source": [
    "## Configuration des imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847885e233ad1cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.053342Z",
     "start_time": "2025-09-25T19:54:45.046959Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import warnings\n",
    "import logging\n",
    "from io import BytesIO, StringIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Union, Any\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a6af7fa049df",
   "metadata": {},
   "source": [
    "## Configuration Globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78fd6a14fdfcbcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.089811Z",
     "start_time": "2025-09-25T19:54:45.081911Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GlobalConfig:\n",
    "    \"\"\"\n",
    "    GlobalConfig serves as a centralized configuration data class that stores essential\n",
    "    constants and settings for the application. It provides default values for parameters related\n",
    "    to country details, API configurations, request handling, and datasets. It aims to reduce\n",
    "    hardcoding of key values and centralize configuration management for better maintainability.\n",
    "\n",
    "    :ivar COUNTRY_CODE: ISO 3166-1 alpha-2 code representing the country.\n",
    "    :type COUNTRY_CODE: str\n",
    "    :ivar COUNTRY_NAME: The official name of the country.\n",
    "    :type COUNTRY_NAME: str\n",
    "    :ivar START_YEAR: The starting year for data processing or analysis.\n",
    "    :type START_YEAR: int\n",
    "    :ivar END_YEAR: The ending year for data processing or analysis.\n",
    "    :type END_YEAR: int\n",
    "    :ivar WORLD_BANK_API_URL: The base URL for the World Bank API.\n",
    "    :type WORLD_BANK_API_URL: str\n",
    "    :ivar INSTAD_BASE_URL: The base URL for the INSTAD application.\n",
    "    :type INSTAD_BASE_URL: str\n",
    "    :ivar OVERPASS_API_URL: The base URL for the Overpass API.\n",
    "    :type OVERPASS_API_URL: str\n",
    "    :ivar DEFAULT_PER_PAGE: The default number of items per page for paginated results.\n",
    "    :type DEFAULT_PER_PAGE: int\n",
    "    :ivar REQUEST_TIMEOUT: The duration (in seconds) before a request times out.\n",
    "    :type REQUEST_TIMEOUT: int\n",
    "    :ivar RETRY_ATTEMPTS: The number of retry attempts allowed after a failed request.\n",
    "    :type RETRY_ATTEMPTS: int\n",
    "    :ivar DELAY_BETWEEN_REQUESTS: The delay (in seconds) between successive requests to APIs,\n",
    "        to avoid throttling or rate-limiting issues.\n",
    "    :type DELAY_BETWEEN_REQUESTS: float\n",
    "    :ivar DEFAULT_WB_INDICATORS: A list of default World Bank indicators used for data\n",
    "        retrieval, represented by their respective codes.\n",
    "    :type DEFAULT_WB_INDICATORS: List[str]\n",
    "    :ivar OSM_ADMIN_LEVELS: A dictionary mapping administrative levels to their corresponding\n",
    "        numeric codes as used in OpenStreetMap (OSM) data.\n",
    "    :type OSM_ADMIN_LEVELS: Dict[str, str]\n",
    "    :ivar EXTERNAL_CSV_URLS: A list of URLs pointing to external CSV resources used for\n",
    "        additional data ingestion.\n",
    "    :type EXTERNAL_CSV_URLS: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    COUNTRY_CODE: str = \"BJ\"\n",
    "    COUNTRY_NAME: str = \"B√©nin\"\n",
    "\n",
    "    START_YEAR: int = 2015\n",
    "    END_YEAR: int = 2025\n",
    "\n",
    "    WORLD_BANK_API_URL: str = \"https://api.worldbank.org/v2\"\n",
    "    INSTAD_BASE_URL: str = \"https://instad.bj\"\n",
    "    OVERPASS_API_URL: str = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "    DEFAULT_PER_PAGE: int = 100\n",
    "    REQUEST_TIMEOUT: int = 30\n",
    "    RETRY_ATTEMPTS: int = 3\n",
    "    DELAY_BETWEEN_REQUESTS: float = 0.5\n",
    "\n",
    "    DEFAULT_WB_INDICATORS: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"SP.POP.TOTL\",  # Population totale\n",
    "            \"NY.GDP.MKTP.CD\",  # PIB (USD courants)\n",
    "            \"NY.GDP.PCAP.CD\",  # PIB par habitant\n",
    "            \"SE.PRM.NENR\",  # Taux net de scolarisation primaire\n",
    "            \"SH.DYN.MORT\",  # Taux de mortalit√© infantile\n",
    "            \"AG.LND.TOTL.K2\",  # Superficie totale (km¬≤)\n",
    "            \"SL.TLF.TOTL.IN\",  # Population active totale\n",
    "            \"SP.DYN.TFRT.IN\",  # Indice synth√©tique de f√©condit√©\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    OSM_ADMIN_LEVELS: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\"pays\": \"2\",\n",
    "                                 \"d√©partement\": \"4\", \"commune\": \"6\"}\n",
    "    )\n",
    "\n",
    "    EXTERNAL_CSV_URLS: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"https://data.uis.unesco.org/medias/education/SDG4.csv\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc571983cec50983",
   "metadata": {},
   "source": [
    "## Configuration du Logging et de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88abe59289efed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.110279Z",
     "start_time": "2025-09-25T19:54:45.096812Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Configures the runtime environment by suppressing specific warnings, setting logging\n",
    "    parameters, configuring pandas display options, and customizing matplotlib and seaborn\n",
    "    styles.\n",
    "\n",
    "    The function is designed to improve the clarity and readability of outputs during\n",
    "    data analysis and visualization tasks.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 100)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "    pd.set_option(\"display.expand_frame_repr\", False)\n",
    "    pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"figure.figsize\": (12, 8),\n",
    "            \"axes.titlesize\": 14,\n",
    "            \"axes.labelsize\": 12,\n",
    "            \"xtick.labelsize\": 10,\n",
    "            \"ytick.labelsize\": 10,\n",
    "            \"legend.fontsize\": 10,\n",
    "        }\n",
    "    )\n",
    "    sns.set_palette(\"Set2\")\n",
    "\n",
    "    logging.info(\"Environnement configur√© avec succ√®s\")\n",
    "    logging.info(\n",
    "        f\"D√©but de collecte: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872b1bc41d8a020",
   "metadata": {},
   "source": [
    "## Configuration du gestionnaire des r√©pertoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34fcb0a71ec689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.145033Z",
     "start_time": "2025-09-25T19:54:45.118065Z"
    }
   },
   "outputs": [],
   "source": [
    "class DirectoryManager:\n",
    "    \"\"\"\n",
    "    Manages a directory structure, providing functionality to initialize, create, and retrieve\n",
    "    specific directories. This class encapsulates logic for directory handling, logging\n",
    "    processes, and organizing the structure based on pre-defined specifications.\n",
    "\n",
    "    Intended for use in applications requiring consistent folder structures, such as\n",
    "    data processing, logging, and result exports.\n",
    "\n",
    "    :ivar base_dir: The base directory from which the folder structure is created\n",
    "        and managed. Defaults to the current directory if not specified.\n",
    "    :type base_dir: Path\n",
    "    :ivar logger: Logger instance used for logging activities and debug information\n",
    "        during directory operations.\n",
    "    :type logger: logging.Logger\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dir: Optional[Path] = None):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the class, setting up a base directory and a logger for\n",
    "        the instance. The constructor allows an optional base directory to be specified\n",
    "        or defaults to the current directory.\n",
    "\n",
    "        :param base_dir: Optional base directory for the instance.\n",
    "        :type base_dir: Optional[Path]\n",
    "        \"\"\"\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._directory = {}\n",
    "\n",
    "    def _create_directory(self, name: str, path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Creates a directory at the specified path if it doesn't already exist.\n",
    "\n",
    "        This method attempts to create a directory while logging the process. If\n",
    "        the directory already exists, it logs that information as a debug statement.\n",
    "        If the directory is successfully created, it logs this as an info statement.\n",
    "        In case of an error during the creation process, the error is logged as an\n",
    "        error statement, and the exception is re-raised.\n",
    "\n",
    "        :param name: Name of the directory for descriptive logging.\n",
    "        :type name: str\n",
    "        :param path: Path object representing the location where the directory\n",
    "            needs to be created.\n",
    "        :type path: Path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if path.exists():\n",
    "                self.logger.debug(f\"{name} - {path} (existe d√©j√†)\")\n",
    "            else:\n",
    "                path.mkdir(parents=True, exist_ok=True)\n",
    "                self.logger.info(f\"{name} - {path} cr√©er avec succ√®s\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erreur lors de la cr√©ation de {name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_directory_structure(self) -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Initializes and creates the directory structure for the application, ensuring\n",
    "        all required folders exist based on the pre-defined structure. If a folder\n",
    "        does not exist, it will be created. The directory structure includes folders\n",
    "        for storing data, logs, and exported files. This function logs the number\n",
    "        of directories created upon completion.\n",
    "\n",
    "        :return: A dictionary mapping directory names to their corresponding `Path`\n",
    "                 objects after initialization.\n",
    "        :rtype: Dict[str, Path]\n",
    "        \"\"\"\n",
    "        structure = {\n",
    "            \"data\": \"data\",\n",
    "            \"raw\": \"data/raw\",\n",
    "            \"processed\": \"data/processed\",\n",
    "            \"final_data\": \"data/final_data\",\n",
    "            \"logs\": \"logs\",\n",
    "            \"exports\": \"exports\",\n",
    "        }\n",
    "\n",
    "        self._directory = {\n",
    "            name: self._create_directory(name, self.base_dir / path)\n",
    "            or self.base_dir / path\n",
    "            for name, path in structure.items()\n",
    "        }\n",
    "\n",
    "        self.logger.info(f\"Structure de {len(self._directory)} dossier cr√©er \")\n",
    "\n",
    "        return self._directory\n",
    "\n",
    "    def get_path(self, name: str) -> Path:\n",
    "        \"\"\"\n",
    "        Retrieve the Path associated with a given name. If the name is not found in the\n",
    "        directory, the base directory is returned as a fallback.\n",
    "\n",
    "        :param name: The name to look up in the directory.\n",
    "        :type name: str\n",
    "        :return: The Path associated with the given name, or the base directory if not found.\n",
    "        :rtype: Path\n",
    "        \"\"\"\n",
    "        return self._directory.get(name, self.base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fafb50618b44bf",
   "metadata": {},
   "source": [
    "## Collecteurs de donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74015703ec0c0cc4",
   "metadata": {},
   "source": [
    "### Classe abstraite centraliser pour tout les types de collecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5db97129a60633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.173546Z",
     "start_time": "2025-09-25T19:54:45.153966Z"
    }
   },
   "outputs": [],
   "source": [
    "class AbstractCollector(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for data collection.\n",
    "\n",
    "    This base class provides a framework for collecting data, making HTTP requests with\n",
    "    retries, and saving collected data in various formats. Subclasses must implement\n",
    "    the abstract `collect_data` method to fetch specific data as required. Additional\n",
    "    utility methods assist in creating HTTP sessions, handling retries, and logging\n",
    "    information.\n",
    "\n",
    "    :ivar config: Configuration object containing settings like retry attempts and\n",
    "                  request timeouts.\n",
    "    :type config: GlobalConfig\n",
    "    :ivar logger: Logger instance for capturing information, warnings, and errors during\n",
    "                  the collector's lifecycle.\n",
    "    :type logger: logging.Logger\n",
    "    :ivar session: Configured HTTP session for making requests with customized headers\n",
    "                   for user agents and language preferences.\n",
    "    :type session: requests.Session\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GlobalConfig):\n",
    "        \"\"\"\n",
    "        Initializes the object with a configuration, sets up a logger,\n",
    "        and creates an HTTP session for subsequent usage.\n",
    "\n",
    "        :param config: The global configuration object to initialize the instance with\n",
    "        :type config: GlobalConfig\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.session = self._create_session()\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_session() -> requests.Session:\n",
    "        \"\"\"\n",
    "        Create and configure a new session with custom headers.\n",
    "\n",
    "        The session is initialized with a user-agent string tailored for\n",
    "        educational research bots, and default headers for accepting JSON,\n",
    "        HTML, and other content types. Default language preference is set\n",
    "        to French with English as a fallback.\n",
    "\n",
    "        :return: A configured `requests.Session` with updated headers.\n",
    "        :rtype: requests.Session\n",
    "        \"\"\"\n",
    "        session = requests.Session()\n",
    "        session.headers.update(\n",
    "            {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Educational Research Bot/1.0)\",\n",
    "                \"Accept\": \"application/json, text/html, */*\",\n",
    "                \"Accept-Language\": \"fr,en;q=0.9\",\n",
    "            }\n",
    "        )\n",
    "        return session\n",
    "\n",
    "    def _make_request_with_retry(\n",
    "            self, url: str, **kwargs\n",
    "    ) -> Optional[requests.Response]:\n",
    "        \"\"\"\n",
    "        Retries an HTTP request a set number of times in case of failure. Logs each\n",
    "        attempt and, upon persistent failure, logs an error message. Incorporates\n",
    "        exponential backoff in case of retries.\n",
    "\n",
    "        :param url: The URL to which the HTTP request is made\n",
    "        :param kwargs: Additional keyword arguments to customize the request, such as\n",
    "                       headers, data, or query parameters\n",
    "        :return: A requests.Response object if the request is successful; None if all\n",
    "                 retry attempts failed\n",
    "        \"\"\"\n",
    "        for attempt in range(self.config.RETRY_ATTEMPTS):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method=kwargs.pop(\"method\", \"get\"),\n",
    "                    url=url,\n",
    "                    timeout=self.config.REQUEST_TIMEOUT,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.RequestException as e:\n",
    "                self.logger.warning(\n",
    "                    f\"üîÑ Tentative {attempt + 1}/{self.config.RETRY_ATTEMPTS} √©chou√©e pour {url}: {e}\"\n",
    "                )\n",
    "                if attempt < self.config.RETRY_ATTEMPTS - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "\n",
    "        self.logger.error(f\"‚ùå √âchec d√©finitif pour {url}\")\n",
    "        return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def collect_data(self) -> pd.DataFrame | Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Defines an abstract method to collect data, which must be implemented by\n",
    "        subclasses. This method is expected to return data in the form of a\n",
    "        DataFrame object from the pandas library.\n",
    "\n",
    "        :raises NotImplementedError: If the subclass does not implement this method.\n",
    "\n",
    "        :returns: A pandas DataFrame containing the collected data.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save_data(\n",
    "            self, data: pd.DataFrame, file_path: Path, format_type: str = \"csv\"\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Saves a provided dataset to a chosen file format at a specified file path. The supported file\n",
    "        formats are CSV, Excel, JSON, and Parquet. Metadata such as the collection time, date, and\n",
    "        collector class is appended to the dataset prior to saving. Logs will capture the success\n",
    "        or failure of the operation along with the file size and number of rows, if applicable.\n",
    "\n",
    "        :param data: The pandas DataFrame containing the data to be saved.\n",
    "        :param file_path: Path object representing the file to which the data will be saved.\n",
    "        :param format_type: Desired file format for saving the data. Supported values are 'csv',\n",
    "            'excel', 'json', and 'parquet'. Default is 'csv'.\n",
    "        :return: A boolean indicating whether the data was successfully saved.\n",
    "        \"\"\"\n",
    "        if data.empty:\n",
    "            self.logger.warning(\"Aucune donn√©es √† sauvegarder\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            meta_data = data.copy()\n",
    "            meta_data[\"collection_time\"] = datetime.now().time()\n",
    "            meta_data[\"collection_date\"] = datetime.now().date()\n",
    "            meta_data[\"collector_class\"] = self.__class__.__name__\n",
    "\n",
    "            if format_type.lower() == \"csv\":\n",
    "                meta_data.to_csv(file_path, index=False, encoding=\"utf-8\")\n",
    "            elif format_type.lower() == \"excel\":\n",
    "                meta_data.to_excel(file_path, index=False, engine=\"xlsxwriter\")\n",
    "            elif format_type.lower() == \"json\":\n",
    "                meta_data.to_json(\n",
    "                    file_path,\n",
    "                    orient=\"records\",\n",
    "                    force_ascii=False,\n",
    "                    indent=4,\n",
    "                    date_format=\"iso\",\n",
    "                )\n",
    "            elif format_type.lower() == \"parquet\":\n",
    "                meta_data.to_parquet(file_path, index=False, engine=\"pyarrow\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Format non valide. Vous ne pouvez choisir que csv, excel, json ou parquet.\"\n",
    "                )\n",
    "\n",
    "            size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "            self.logger.info(\n",
    "                f\"Sauvegard√©: {file_path.name} ({len(data)} lignes, {size_mb:.2f} MB)\"\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erreur lors de la sauvegarde {file_path}: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917f4907b913a5b",
   "metadata": {},
   "source": [
    "### Collecteur de donn√©es pour World Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c279c38be9d0c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.203391Z",
     "start_time": "2025-09-25T19:54:45.181714Z"
    }
   },
   "outputs": [],
   "source": [
    "class WorldBankCollector(AbstractCollector):\n",
    "    \"\"\"\n",
    "    Handles data collection from the World Bank API.\n",
    "\n",
    "    This class provides methods for fetching data from the World Bank for specific indicators\n",
    "    over a range of years. The data is formatted into pandas DataFrames for further analysis\n",
    "    and processing, supporting features like pagination and retrying on request failures.\n",
    "\n",
    "    :ivar config: Configuration object containing API settings, such as base URL, country\n",
    "        code, default indicators, start and end year, and delay between requests.\n",
    "    :type config: object\n",
    "    :ivar logger: Logger instance used for logging information, warnings, and errors during\n",
    "        the data collection process.\n",
    "    :type logger: logging.Logger\n",
    "    \"\"\"\n",
    "\n",
    "    def _fetch_indicator_data(\n",
    "            self,\n",
    "            indicator: str,\n",
    "            start_year: Optional[int] = None,\n",
    "            end_year: Optional[int] = None,\n",
    "            per_page: Optional[int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetches indicator data from the World Bank API for a specific country within a specified time range\n",
    "        and formats the data into a pandas DataFrame. The function allows filtering by start and end years\n",
    "        and configuring the number of entries per page.\n",
    "\n",
    "        :param indicator: The unique identifier of the indicator data to fetch.\n",
    "        :type indicator: str\n",
    "        :param start_year: The starting year for the data range (default is derived from configuration).\n",
    "        :type start_year: int, optional\n",
    "        :param end_year: The ending year for the data range (default is derived from configuration).\n",
    "        :type end_year: int, optional\n",
    "        :param per_page: The number of results per page to fetch (default is derived from configuration).\n",
    "        :type per_page: int, optional\n",
    "\n",
    "        :return: A pandas DataFrame containing the requested indicator data with columns such as:\n",
    "            `indicator_code`, `indicator_name`, `country_code`, `country_name`, `year`, `value`,\n",
    "            and `source`.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        url = f\"{self.config.WORLD_BANK_API_URL}/country/{self.config.COUNTRY_CODE}/indicator/{indicator}\"\n",
    "\n",
    "        params = {\n",
    "            \"data\": f\"{start_year or self.config.START_YEAR}:{end_year or self.config.END_YEAR}\",\n",
    "            \"format\": \"json\",\n",
    "            \"per_page\": per_page or self.config.DEFAULT_PER_PAGE,\n",
    "        }\n",
    "\n",
    "        response = self._make_request_with_retry(url, params=params)\n",
    "\n",
    "        if response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            entries = (\n",
    "                data[1] if isinstance(data, list) and len(\n",
    "                    data) > 1 and data[1] else []\n",
    "            )\n",
    "\n",
    "            record = []\n",
    "\n",
    "            for entry in entries:\n",
    "                record.append(\n",
    "                    {\n",
    "                        \"indicator_code\": entry[\"indicator\"][\"id\"],\n",
    "                        \"indicator_name\": entry[\"indicator\"][\"value\"],\n",
    "                        \"country_code\": entry[\"country\"][\"id\"],\n",
    "                        \"country_name\": entry[\"country\"][\"value\"],\n",
    "                        \"year\": pd.to_numeric(entry[\"date\"], errors=\"coerce\"),\n",
    "                        \"value\": pd.to_numeric(entry[\"value\"], errors=\"coerce\"),\n",
    "                        \"source\": \"World Bank API\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            return pd.DataFrame(record)\n",
    "        except (ValueError, KeyError) as e:\n",
    "            self.logger.error(f\"Erreur parsing JSON pour {indicator}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def collect_data(\n",
    "            self,\n",
    "            indicators: Optional[List[str]] = None,\n",
    "            start_year: Optional[int] = None,\n",
    "            end_year: Optional[int] = None,\n",
    "            per_page: Optional[int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collects data from the World Bank based on the provided indicators and time frame.\n",
    "\n",
    "        This method facilitates fetching and aggregating data for a given list of\n",
    "        indicators over a specified range of years. The data is retrieved from the\n",
    "        World Bank API, consolidated into a dataframe, and returned for further\n",
    "        processing.\n",
    "\n",
    "        :param indicators: Optional list of indicators to collect data for. If not\n",
    "            provided, default indicators defined in the configuration will be used.\n",
    "        :type indicators: List[str], optional\n",
    "        :param start_year: The start year of the data range to collect. If None,\n",
    "            the default value or full range supported by the API may be used.\n",
    "        :type start_year: int, optional\n",
    "        :param end_year: The end year of the data range to collect. If None, the\n",
    "            default value or full range supported by the API may be used.\n",
    "        :type end_year: int, optional\n",
    "        :param per_page: The number of results to retrieve per page. If None, a\n",
    "            default limit will be used based on API specifications.\n",
    "        :type per_page: int, optional\n",
    "        :return: A DataFrame containing the collected data for the specified\n",
    "            indicators and years. If no data is collected, an empty DataFrame is\n",
    "            returned.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        indicators = indicators or self.config.DEFAULT_WB_INDICATORS\n",
    "\n",
    "        donnees = []\n",
    "        self.logger.info(\n",
    "            f\"D√©but collecte World Bank ({len(indicators)} indicateurs)\")\n",
    "\n",
    "        for i, indicator in enumerate(indicators, start=1):\n",
    "            self.logger.info(f\"[{i}/{len(indicators)}] Collecte: {indicator}\")\n",
    "\n",
    "            fetch_data = self._fetch_indicator_data(\n",
    "                indicator, start_year, end_year, per_page\n",
    "            )\n",
    "            if not fetch_data.empty:\n",
    "                donnees.append(fetch_data)\n",
    "                self.logger.info(\n",
    "                    f\"{len(fetch_data)} enregistrements pour {indicator}\")\n",
    "\n",
    "            time.sleep(self.config.DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "        datasets = pd.concat(\n",
    "            donnees, ignore_index=True) if donnees else pd.DataFrame()\n",
    "        self.logger.info(\n",
    "            f\"Fin collecte World Bank ({len(datasets)} enregistrements)\")\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da387b17fba843",
   "metadata": {},
   "source": [
    "### Collecte de donn√©es par Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992bb1bfd51a130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.225503Z",
     "start_time": "2025-09-25T19:54:45.211624Z"
    }
   },
   "outputs": [],
   "source": [
    "class WebScrapingCollector(AbstractCollector):\n",
    "    \"\"\"\n",
    "    Collects and aggregates data by scraping tables from predefined HTML web pages.\n",
    "\n",
    "    This class is used for web scraping tasks where data is extracted from HTML tables\n",
    "    and aggregated into a unified pandas DataFrame. It supports scraping from multiple\n",
    "    URLs and handles metadata tagging for each scraped table, including identifying\n",
    "    the source, the URL, and the table's index. The collected data can be further\n",
    "    processed or analyzed.\n",
    "\n",
    "    :ivar logger: Logger used for logging the scraping process info, warnings, and errors.\n",
    "    :type logger: logging.Logger\n",
    "    \"\"\"\n",
    "\n",
    "    def _scrape_html_tables(\n",
    "            self, url: str, source_name: str, max_tables: int = 5\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrapes tables from an HTML page at the specified URL and returns them as a pandas DataFrame.\n",
    "        Each table is assigned metadata, including its source URL, source name, and an index indicating its\n",
    "        position among the other scraped tables.\n",
    "\n",
    "        :param url: The URL of the webpage containing the HTML tables.\n",
    "        :type url: str\n",
    "        :param source_name: The name of the data source for added metadata within the resulting DataFrame.\n",
    "        :type source_name: str\n",
    "        :param max_tables: The maximum number of tables to scrape from the webpage. Defaults to 5.\n",
    "        :type max_tables: int\n",
    "        :return: A pandas DataFrame containing the combined data from the extracted HTML tables.\n",
    "                 If no tables are found or scraping fails, an empty DataFrame is returned.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        response = self._make_request_with_retry(url)\n",
    "\n",
    "        if not response:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            tables = soup.find_all(\"table\")\n",
    "\n",
    "            self.logger.info(f\"{len(tables)} tableaux trouv√©s sur {url}\")\n",
    "\n",
    "            scraped_tables = []\n",
    "            for i, table in enumerate(tables[:max_tables]):\n",
    "                try:\n",
    "                    dataframe = pd.read_html(StringIO(str(table)))[0]\n",
    "                    dataframe[\"source_url\"] = url\n",
    "                    dataframe[\"source_name\"] = source_name\n",
    "                    dataframe[\"table_index\"] = i\n",
    "                    scraped_tables.append(dataframe)\n",
    "                    self.logger.debug(f\"Tableau {i + 1}: {dataframe.shape}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Erreur tableau {i + 1} sur {url}: {e}\")\n",
    "\n",
    "            dataset = (\n",
    "                pd.concat(scraped_tables, ignore_index=True)\n",
    "                if scraped_tables\n",
    "                else pd.DataFrame()\n",
    "            )\n",
    "            return dataset\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erreur scraping {url}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def collect_data(self, max_tables: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scrapes data from predefined URLs and aggregates the results into a single DataFrame.\n",
    "\n",
    "        The method retrieves tables from multiple sources, processes them, and concatenates\n",
    "        the scraped data into one unified DataFrame. If no data is collected, an empty\n",
    "        DataFrame is returned.\n",
    "\n",
    "        :param max_tables: Maximum number of tables to scrape from each URL. Defaults to 5.\n",
    "        :type max_tables: int\n",
    "        :return: Aggregated dataset containing the scraped data from all specified URLs.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        self.logger.info(\"D√©but du web scraping\")\n",
    "\n",
    "        urls_for_scraping = {\n",
    "            \"instad_trimestres\": \"https://instad.bj/publications/publications-trimestrielles\",\n",
    "            \"instad_mois\": \"https://instad.bj/publications/publications-mensuelles\",\n",
    "            \"demographic_external\": [\n",
    "                \"https://hub.worldpop.org/project/categories?id=3\",\n",
    "                \"https://dhsprogram.com/data/available-datasets.cfm\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        scraped_data = []\n",
    "        urls = []\n",
    "\n",
    "        for source_name, source_urls in urls_for_scraping.items():\n",
    "            if isinstance(source_urls, str):\n",
    "                urls_to_process = [source_urls]\n",
    "            else:\n",
    "                urls_to_process = source_urls\n",
    "\n",
    "            for url in urls_to_process:\n",
    "                donnees = self._scrape_html_tables(\n",
    "                    url, source_name, max_tables)\n",
    "                if not donnees.empty:\n",
    "                    scraped_data.append(donnees)\n",
    "\n",
    "        datasets = (\n",
    "            pd.concat(scraped_data, ignore_index=True)\n",
    "            if scraped_data\n",
    "            else pd.DataFrame()\n",
    "        )\n",
    "        self.logger.info(f\"Scraping termin√©: {len(datasets)} enregistrements\")\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19be9562184984",
   "metadata": {},
   "source": [
    "### Collecte de donn√©es g√©ographique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76d71d378517b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.247678Z",
     "start_time": "2025-09-25T19:54:45.234677Z"
    }
   },
   "outputs": [],
   "source": [
    "class GeographicCollector(AbstractCollector):\n",
    "    \"\"\"\n",
    "    Handles the collection of geographical data through the Overpass API, including city-level data\n",
    "    and administrative boundaries. The primary purpose of the class is to query OpenStreetMap data,\n",
    "    process the results, and return them in pandas DataFrame structures for further analytics\n",
    "    or storage.\n",
    "\n",
    "    The class provides functionalities to interact with the Overpass API, retry in case of failure,\n",
    "    and process elements such as cities, towns, villages, and administrative regions by their levels.\n",
    "\n",
    "    :ivar config: Configuration settings for Overpass API and collection parameters.\n",
    "    :type config: Configuration\n",
    "    :ivar logger: Logger instance used for logging information, warnings, and errors during collection.\n",
    "    :type logger: Logger\n",
    "    \"\"\"\n",
    "\n",
    "    def _execute_overpass_query(self, query: str, data_type: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes an Overpass API query and processes the resulting data into a pandas DataFrame.\n",
    "\n",
    "        :param query: The Overpass QL query string.\n",
    "        :type query: str\n",
    "        :param data_type: The type of data being queried (e.g., \"cities\", \"regions\").\n",
    "        :type data_type: str\n",
    "        :return: A pandas DataFrame containing the processed records based on the queried data.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        response = self._make_request_with_retry(\n",
    "            self.config.OVERPASS_API_URL, data={\"data\": query}, method=\"post\"\n",
    "        )\n",
    "\n",
    "        if response is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            elements = data.get(\"elements\", [])\n",
    "\n",
    "            records = []\n",
    "            if elements:\n",
    "                for element in elements:\n",
    "                    if \"tags\" in element:\n",
    "                        record = {\n",
    "                            \"name\": element[\"tags\"].get(\"name\"),\n",
    "                            \"osm_id\": element.get(\"id\"),\n",
    "                            \"latitude\": element.get(\"lat\")\n",
    "                            or element.get(\"center\", {}).get(\"lat\"),\n",
    "                            \"longitude\": element.get(\"lon\")\n",
    "                            or element.get(\"center\", {}).get(\"lon\"),\n",
    "                            \"data_type\": data_type,\n",
    "                            \"source\": \"OpenStreetMap\",\n",
    "                        }\n",
    "\n",
    "                        if data_type == \"cities\":\n",
    "                            record.update(\n",
    "                                {\n",
    "                                    \"place_type\": element[\"tags\"].get(\"place\"),\n",
    "                                    \"population\": pd.to_numeric(\n",
    "                                        element[\"tags\"].get(\"population\"),\n",
    "                                        errors=\"coerce\",\n",
    "                                    ),\n",
    "                                }\n",
    "                            )\n",
    "                        else:\n",
    "                            record.update(\n",
    "                                {\n",
    "                                    \"admin_level\": element[\"tags\"].get(\"admin_level\"),\n",
    "                                    \"wikidata\": element[\"tags\"].get(\"wikidata\"),\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        records.append(record)\n",
    "\n",
    "                self.logger.info(\n",
    "                    f\"üìç {len(records)} √©l√©ments {data_type} collect√©s\")\n",
    "                return pd.DataFrame(records)\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "\n",
    "        except (ValueError, KeyError) as e:\n",
    "            self.logger.error(f\"Erreur parsing Overpass pour {data_type}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _collecte_cities(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Executes an Overpass query to collect city, town, and village data within a specific country\n",
    "        defined by its ISO3166-1 country code and admin level. This method queries all place types\n",
    "        marked as city, town, or village under the given area's boundary.\n",
    "\n",
    "        :return: A pandas DataFrame containing the retrieved place data including\n",
    "                 center coordinates and tags.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:60];\n",
    "        area[\"ISO3166-1\"=\"{self.config.COUNTRY_CODE}\"][admin_level=\"2\"];\n",
    "        (\n",
    "          node(area)[\"place\"~\"city|town|village\"];\n",
    "          way(area)[\"place\"~\"city|town|village\"];\n",
    "          relation(area)[\"place\"~\"city|town|village\"];\n",
    "        );\n",
    "        out center tags;\n",
    "        \"\"\"\n",
    "\n",
    "        return self._execute_overpass_query(query, \"cities\")\n",
    "\n",
    "    def _collecte_administratives_boundaries(\n",
    "            self, admin_level: str, level_name: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collects administrative boundaries based on the specified administrative level and name.\n",
    "\n",
    "        This function executes an Overpass API query to retrieve administrative boundary data based on\n",
    "        the specified administrative level and level name.\n",
    "\n",
    "        :param admin_level: The administrative level to be queried (e.g., \"4\", \"8\").\n",
    "        :type admin_level: str\n",
    "        :param level_name: A descriptive name for the level, used for naming the resulting dataset.\n",
    "        :type level_name: str\n",
    "        :return: A pandas DataFrame containing the administrative boundaries' data retrieved from the Overpass API.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:60];\n",
    "        relation[\"boundary\"=\"administrative\"][\"admin_level\"=\"{admin_level}\"][\"name\"~\"{self.config.COUNTRY_NAME}|Benin\"];\n",
    "        out center tags;\n",
    "        \"\"\"\n",
    "\n",
    "        return self._execute_overpass_query(query, f\"admin_boundaries_{level_name}\")\n",
    "\n",
    "    def collect_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Collect geographical data, including cities data and administrative boundaries,\n",
    "        and return it as a dictionary of dataframes.\n",
    "\n",
    "        This method performs the following actions:\n",
    "        1. Collects cities-level geographical data.\n",
    "        2. Iterates through administrative levels as specified in the configuration,\n",
    "           collecting data for each level and appending it to the results if not empty.\n",
    "        3. Logs the total number of records collected upon completion.\n",
    "\n",
    "        :param self: Reference to the instance of the class.\n",
    "        :return: A dictionary where keys are string labels describing the type of data\n",
    "                 (e.g., 'cities', 'admin_boundaries_{level_name}') and values are\n",
    "                 pandas DataFrame objects containing the respective geographical data.\n",
    "        :rtype: Dict[str, pd.DataFrame]\n",
    "        \"\"\"\n",
    "        self.logger.info(\"D√©but collecte donn√©es g√©ographiques\")\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        cities_data = self._collecte_cities()\n",
    "        if not cities_data.empty:\n",
    "            results[\"cities\"] = cities_data\n",
    "\n",
    "        for level_name, level_code in self.config.OSM_ADMIN_LEVELS.items():\n",
    "            administrative_boundaries_data = self._collecte_administratives_boundaries(\n",
    "                level_code, level_name\n",
    "            )\n",
    "            if not administrative_boundaries_data.empty:\n",
    "                results[f\"admin_boundaries_{level_name}\"] = (\n",
    "                    administrative_boundaries_data\n",
    "                )\n",
    "\n",
    "        totals = sum(len(dataframe) for dataframe in results.values())\n",
    "        self.logger.info(f\"G√©ographie termin√©e: {totals} enregistrements\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeffef7a66786bcb",
   "metadata": {},
   "source": [
    "### Collecteur de donn√©es a travers des fichiers externe (csv, json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ed5eeb22deffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.267884Z",
     "start_time": "2025-09-25T19:54:45.255248Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExternalCollector(AbstractCollector):\n",
    "    \"\"\"\n",
    "    Handles external data collection by downloading and processing data from specified URLs in CSV or\n",
    "    JSON formats. The collected data is returned as a unified pandas DataFrame.\n",
    "\n",
    "    The class provides methods to fetch data from URLs, handle different file formats, and combine\n",
    "    datasets into a single cohesive DataFrame. It is designed to handle retries, data consistency,\n",
    "    and logging during the download and processing stages.\n",
    "\n",
    "    :ivar config: Configuration object providing external URLs and other settings.\n",
    "    :type config: Any\n",
    "    :ivar logger: Logger instance for logging information, warnings, and errors during\n",
    "        the data collection process.\n",
    "    :type logger: logging.Logger\n",
    "    \"\"\"\n",
    "\n",
    "    def _download_csv_data(self, url: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Downloads CSV data from the provided URL and returns it as a pandas DataFrame. The method\n",
    "        supports both CSV and JSON formats and adds a 'source' column to indicate the origin of the data.\n",
    "\n",
    "        :param url: The URL to download the data from.\n",
    "        :type url: str\n",
    "        :return: A pandas DataFrame containing the downloaded data. If the response is empty, invalid,\n",
    "            or cannot be parsed in supported formats (CSV/JSON), an empty DataFrame is returned.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        response = self._make_request_with_retry(url)\n",
    "\n",
    "        if not response:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            content = response.content\n",
    "\n",
    "            try:\n",
    "                dataframe = pd.read_csv(BytesIO(content))\n",
    "                dataframe[\"source\"] = url\n",
    "                return dataframe\n",
    "            except pd.errors.EmptyDataError:\n",
    "                self.logger.warning(f\"Fichier vide: {url}\")\n",
    "                return pd.DataFrame()\n",
    "            except Exception as csv_e:\n",
    "                try:\n",
    "                    json_data = json.loads(content)\n",
    "                    dataframe = pd.json_normalize(json_data)\n",
    "                    dataframe[\"source\"] = url\n",
    "                    return dataframe\n",
    "                except Exception as json_e:\n",
    "                    self.logger.error(\n",
    "                        f\"Format non support√© pour {url}:\\nCsv error: {csv_e}\\nJson error: {json_e}\"\n",
    "                    )\n",
    "                    return pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erreur t√©l√©chargement {url}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def collect_data(self, urls: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collect data from a list of external CSV URLs and combine them into a single DataFrame.\n",
    "\n",
    "        This method downloads and processes data from the specified URLs (or default URLs if none\n",
    "        are provided). If the data is successfully downloaded and is not empty, it is appended\n",
    "        to a list of datasets. Finally, all datasets are concatenated into a single pandas\n",
    "        DataFrame.\n",
    "\n",
    "        :param urls: Optional list of CSV URLs to download data from. If None, the method\n",
    "            will use the default URLs specified in the configuration.\n",
    "        :return: A pandas DataFrame containing the combined data from all successfully\n",
    "            downloaded datasets.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        urls = urls or self.config.EXTERNAL_CSV_URLS\n",
    "\n",
    "        self.logger.info(f\"Collecte donn√©es externes ({len(urls)} sources)\")\n",
    "\n",
    "        donnees = []\n",
    "\n",
    "        for i, url in enumerate(urls, 1):\n",
    "            self.logger.info(f\"[{i}/{len(urls)}] T√©l√©chargement: {url}\")\n",
    "\n",
    "            data = self._download_csv_data(url)\n",
    "            if not data.empty:\n",
    "                donnees.append(data)\n",
    "                self.logger.info(f\"{len(data)} enregistrements de {url}\")\n",
    "\n",
    "        datasets = pd.concat(\n",
    "            donnees, ignore_index=True) if donnees else pd.DataFrame()\n",
    "        self.logger.info(f\"Externe termin√©: {len(datasets)} enregistrements\")\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a3902d1a6e30b",
   "metadata": {},
   "source": [
    "### Orchestrateur principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5c618f921328d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.289603Z",
     "start_time": "2025-09-25T19:54:45.275263Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataCollectorOrchestrator:\n",
    "    \"\"\"\n",
    "    Orchestrates the data collection process from various sources.\n",
    "\n",
    "    This class is designed to manage the overall data collection workflow, including managing\n",
    "    directories, initializing data collectors for multiple sources, executing the data collection\n",
    "    process, validating the collected data, and generating summaries. It encapsulates all necessary\n",
    "    functionalities to perform structured data collection tasks and ensures data quality and\n",
    "    organization.\n",
    "\n",
    "    :ivar config: Configuration object used across the application.\n",
    "    :type config: GlobalConfig\n",
    "    :ivar logger: Logger instance for logging collection processes, errors, and other system-related events.\n",
    "    :type logger: logging.Logger\n",
    "    :ivar directory_manager: Instance responsible for managing directory structure and paths.\n",
    "    :type directory_manager: DirectoryManager\n",
    "    :ivar directories: A mapping of directory names to their respective file paths, initialized by the directory manager.\n",
    "    :type directories: dict\n",
    "    :ivar collectors: Dictionary mapping collector names to their corresponding collector instances for handling specific data sources.\n",
    "    :type collectors: dict\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, config: Optional[GlobalConfig] = None, base_dir: Optional[Path] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a class instance with configuration, directory management, and data collectors.\n",
    "\n",
    "        This constructor sets up the configuration, logging, a directory manager for managing\n",
    "        required directories, and initializes data collectors for various data sources.\n",
    "\n",
    "        :param config: Optional application-wide configuration, defaults to None. If None,\n",
    "            a new instance of GlobalConfig is created.\n",
    "        :type config: Optional[GlobalConfig]\n",
    "        :param base_dir: Optional base directory used by the directory manager, defaults to None.\n",
    "        :type base_dir: Optional[Path]\n",
    "        \"\"\"\n",
    "        self.config = config or GlobalConfig()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        self.directory_manager = DirectoryManager(base_dir)\n",
    "        self.directories = self.directory_manager.initialize_directory_structure()\n",
    "\n",
    "        self.collectors = {\n",
    "            \"world_bank_collector\": WorldBankCollector(self.config),\n",
    "            \"web_scraping_collector\": WebScrapingCollector(self.config),\n",
    "            \"geographic_collector\": GeographicCollector(self.config),\n",
    "            \"external_collector\": ExternalCollector(self.config),\n",
    "        }\n",
    "\n",
    "    def run_full_data_collector(\n",
    "            self, data_collector: Optional[List[str]] = None\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Executes a full data collection process for the specified collectors or all available\n",
    "        collectors if none are specified. The method iterates over the provided or default\n",
    "        collector names, invokes their data collection logic, validates the results, and saves\n",
    "        them to predefined directories. The underlying logic handles special cases for certain\n",
    "        collectors like geographic-related data to ensure proper merging and saving of multiple\n",
    "        dataframes. Comprehensive logging is performed to track the state and progression of\n",
    "        the collection process, including success, warnings, and errors.\n",
    "\n",
    "        :param data_collector: Optional list of collector names to execute. If not provided,\n",
    "            all available collectors will be used.\n",
    "        :type data_collector: Optional[List[str]]\n",
    "        :return: A dictionary where the keys are collector names (or specific keys for\n",
    "            specialized collectors), and the values are the corresponding collected dataframes.\n",
    "        :rtype: Dict[str, pd.DataFrame]\n",
    "        \"\"\"\n",
    "        collector = data_collector or list(self.collectors.keys())\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        results = {}\n",
    "        self.logger.info(\n",
    "            f\"Lancement collecte compl√®te ({len(collector)} collecteurs)\")\n",
    "\n",
    "        for collector_name in collector:\n",
    "            if collector_name not in self.collectors:\n",
    "                self.logger.warning(f\"‚ö†Ô∏è Collecteur inconnu: {collector_name}\")\n",
    "                continue\n",
    "\n",
    "            self.logger.info(f\"‚ñ∂Ô∏è D√©marrage collecteur: {collector_name}\")\n",
    "            collector_start = datetime.now()\n",
    "\n",
    "            try:\n",
    "                collector_instance = self.collectors[collector_name]\n",
    "                data = collector_instance.collect_data()\n",
    "\n",
    "                if collector_name == \"geographic_collector\" and isinstance(data, dict):\n",
    "                    for key, dataframe in data.items():\n",
    "                        if not dataframe.empty:\n",
    "                            filename = f\"{collector_name}_{key}.csv\"\n",
    "                            filepath = self.directories[\"raw\"] / filename\n",
    "                            collector_instance.save_data(dataframe, filepath)\n",
    "                            results[f\"{collector_name}_{key}\"] = dataframe\n",
    "\n",
    "                    combined_geo = pd.concat(\n",
    "                        [dataframe for dataframe in data.values()\n",
    "                         if not dataframe.empty],\n",
    "                        ignore_index=True\n",
    "                    ) if data else pd.DataFrame()\n",
    "\n",
    "                    if not combined_geo.empty:\n",
    "                        results[collector_name] = combined_geo\n",
    "\n",
    "                elif not data.empty if isinstance(data, pd.DataFrame) else False:\n",
    "                    results[collector_name] = data\n",
    "                    filename = f\"{collector_name}_data.csv\"\n",
    "                    filepath = self.directories[\"raw\"] / filename\n",
    "                    collector_instance.save_data(data, filepath)\n",
    "\n",
    "                duration = datetime.now() - collector_start\n",
    "                record_count = len(data) if isinstance(data, pd.DataFrame) else sum(\n",
    "                    len(df) for df in data.values() if isinstance(df, pd.DataFrame)\n",
    "                )\n",
    "                self.logger.info(\n",
    "                    f\"{collector_name} termin√©: {record_count} enregistrements en {duration}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    f\"‚ùå Erreur {collector_name}: {e}\", exc_info=True)\n",
    "\n",
    "        total_duration = datetime.now() - start_time\n",
    "        total_records = sum(len(df) for df in results.values()\n",
    "                            if isinstance(df, pd.DataFrame))\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Collecte termin√©e: {total_records} enregistrements en {total_duration}\"\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def generate_collection_summary(\n",
    "            self, results: Dict[str, pd.DataFrame]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates a summary DataFrame from a collection of DataFrames provided in the `results` dictionary.\n",
    "        The summary includes metadata for each DataFrame such as the number of records, columns, memory usage,\n",
    "        missing values percentage, numerical and date-related column counts, date range (if applicable),\n",
    "        and the number of duplicate rows. If the summary DataFrame is not empty, it saves the results to a CSV\n",
    "        file and logs the output.\n",
    "\n",
    "        :param results: Dictionary where each key is a source name (str), and each value is a DataFrame\n",
    "            containing the data to be summarized.\n",
    "        :type results: Dict[str, pd.DataFrame]\n",
    "        :return: A summary DataFrame containing metadata for each source and its corresponding data.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        summary = []\n",
    "\n",
    "        for source, data in results.items():\n",
    "            if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "                numeric_cols = data.select_dtypes(\n",
    "                    include=[np.number]).columns.tolist()\n",
    "                date_cols = data.select_dtypes(\n",
    "                    include=['datetime64']).columns.tolist()\n",
    "\n",
    "                summary.append(\n",
    "                    {\n",
    "                        \"source\": source,\n",
    "                        \"records_count\": len(data),\n",
    "                        \"columns_count\": len(data.columns),\n",
    "                        \"memory_usage_mb\": round(\n",
    "                            data.memory_usage(\n",
    "                                deep=True).sum() / (1024 * 1024), 2\n",
    "                        ),\n",
    "                        \"collection_date\": datetime.now().date(),\n",
    "                        \"has_nulls\": data.isnull().any().any(),\n",
    "                        \"null_percentage\": round(\n",
    "                            (data.isnull().sum().sum() /\n",
    "                             (len(data) * len(data.columns))) * 100, 2\n",
    "                        ),\n",
    "                        \"numeric_columns\": len(numeric_cols),\n",
    "                        \"date_columns\": len(date_cols),\n",
    "                        \"date_range\": (\n",
    "                            f\"{data['year'].min()}-{data['year'].max()}\"\n",
    "                            if \"year\" in data.columns\n",
    "                            else \"N/A\"\n",
    "                        ),\n",
    "                        \"duplicate_rows\": data.duplicated().sum(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        summary_dataframe = pd.DataFrame(summary)\n",
    "\n",
    "        if not summary_dataframe.empty:\n",
    "            summary_path = self.directories[\"processed\"] / \\\n",
    "                \"collection_summary.csv\"\n",
    "            summary_dataframe.to_csv(\n",
    "                summary_path, index=False, encoding='utf-8')\n",
    "            self.logger.info(f\"R√©sum√© sauvegard√©: {summary_path}\")\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(\"R√âSUM√â DE LA COLLECTE DE DONN√âES\")\n",
    "            print(\"=\" * 100)\n",
    "            print(summary_dataframe.to_string(index=False))\n",
    "            print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "        return summary_dataframe\n",
    "\n",
    "    def validate_collected_data(self, results: Dict[str, pd.DataFrame]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Validates the collected data from various sources, checks for specific data quality\n",
    "        issues, and generates a validation report.\n",
    "\n",
    "        The data quality checks include:\n",
    "        - Checking for duplicate rows in the data.\n",
    "        - Identifying columns with more than 50% missing values.\n",
    "        - Validating 'year' column values to ensure they fall within a sensible range\n",
    "          (between 1900 and the current year).\n",
    "\n",
    "        :param results: Dictionary containing the data to validate, with keys as source\n",
    "            names and values as pandas DataFrames representing the data collected.\n",
    "        :type results: Dict[str, pd.DataFrame]\n",
    "        :return: A dictionary detailing the quality issues detected for each data source,\n",
    "            if any exist. Each key represents the data source, and each value is a list of\n",
    "            issues found.\n",
    "        :rtype: Dict[str, List[str]]\n",
    "        \"\"\"\n",
    "        validation_report = {}\n",
    "\n",
    "        for source, data in results.items():\n",
    "            if not isinstance(data, pd.DataFrame) or data.empty:\n",
    "                continue\n",
    "\n",
    "            issues = []\n",
    "\n",
    "            if data.duplicated().any():\n",
    "                issues.append(f\"Doublons d√©tect√©s: {data.duplicated().sum()}\")\n",
    "\n",
    "            critical_nulls = data.isnull().sum(\n",
    "            )[data.isnull().sum() > len(data) * 0.5]\n",
    "            if not critical_nulls.empty:\n",
    "                issues.append(\n",
    "                    f\"Colonnes avec >50% de valeurs manquantes: {critical_nulls.index.tolist()}\")\n",
    "\n",
    "            if 'year' in data.columns:\n",
    "                invalid_years = data[\n",
    "                    (data['year'] < 1900) | (\n",
    "                        data['year'] > datetime.now().year)\n",
    "                ]\n",
    "                if not invalid_years.empty:\n",
    "                    issues.append(\n",
    "                        f\"Ann√©es invalides d√©tect√©es: {len(invalid_years)}\")\n",
    "\n",
    "            if issues:\n",
    "                validation_report[source] = issues\n",
    "\n",
    "        if validation_report:\n",
    "            self.logger.warning(\n",
    "                f\"Probl√®mes de qualit√© d√©tect√©s:\\n{json.dumps(validation_report, indent=2)}\")\n",
    "        else:\n",
    "            self.logger.info(\"Toutes les donn√©es ont pass√© la validation\")\n",
    "\n",
    "        return validation_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d095fba461371b9c",
   "metadata": {},
   "source": [
    "### Fonction main pour tester l'orchestrateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca6e8c7a66c251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:54:45.311632Z",
     "start_time": "2025-09-25T19:54:45.305277Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Entry point for the application, orchestrating the execution of the data collection\n",
    "    process, summarization of results, and summary display.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    setup_environment()\n",
    "\n",
    "    main_orchestrator = DataCollectorOrchestrator()\n",
    "\n",
    "    main_results = main_orchestrator.run_full_data_collector()\n",
    "\n",
    "    _ = main_orchestrator.generate_collection_summary(main_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1cd5a28e4f67c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T19:55:14.670683Z",
     "start_time": "2025-09-25T19:54:45.320489Z"
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2a970",
   "metadata": {},
   "source": [
    "## N√©ttoyeur des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db5b10",
   "metadata": {},
   "source": [
    "### Configuration du rapport du nettoyeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c338c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CleaningReport:\n",
    "    source: str\n",
    "    initial_rows: int\n",
    "    final_rows: int\n",
    "    rows_removed: int = 0\n",
    "    duplicates_removed: int = 0\n",
    "    nulls_handled: int = 0\n",
    "    outliers_removed: int = 0\n",
    "    columns_standardized: List[str] = field(default_factory=list)\n",
    "    columns_dropped: List[str] = field(default_factory=list)\n",
    "    data_types_converted: Dict[str, str] = field(default_factory=dict)\n",
    "    issues_detected: List[str] = field(default_factory=list)\n",
    "    cleaning_timestamp: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'source': self.source,\n",
    "            'initial_rows': self.initial_rows,\n",
    "            'final_rows': self.final_rows,\n",
    "            'rows_removed': self.rows_removed,\n",
    "            'removal_percentage': round((self.rows_removed / self.initial_rows * 100), 2) if self.initial_rows > 0 else 0,\n",
    "            'duplicates_removed': self.duplicates_removed,\n",
    "            'nulls_handled': self.nulls_handled,\n",
    "            'outliers_removed': self.outliers_removed,\n",
    "            'columns_standardized': len(self.columns_standardized),\n",
    "            'columns_dropped': len(self.columns_dropped),\n",
    "            'issues_count': len(self.issues_detected),\n",
    "            'cleaning_timestamp': self.cleaning_timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
