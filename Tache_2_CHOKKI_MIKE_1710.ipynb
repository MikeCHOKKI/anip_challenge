{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import functools\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Tuple, Any, Union\n",
    "from dataclasses import dataclass, field\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ],
   "id": "cd7e9bfa83aff5b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    \"\"\"Configuration pour l'analyse exploratoire\"\"\"\n",
    "\n",
    "    COUNTRY_CODE: str = \"BJ\"\n",
    "    COUNTRY_NAME: str = \"Bénin\"\n",
    "\n",
    "    # Seuils pour la détection d'anomalies\n",
    "    ZSCORE_THRESHOLD: float = 3.0\n",
    "    IQR_MULTIPLIER: float = 1.5\n",
    "    CORRELATION_THRESHOLD: float = 0.7\n",
    "\n",
    "    # Configuration des indicateurs\n",
    "    POPULATION_AGE_YOUNG: int = 15\n",
    "    POPULATION_AGE_OLD: int = 65\n",
    "    GDP_BASE_YEAR: int = 2015\n",
    "\n",
    "    # Répertoires\n",
    "    DIRECTORY_STRUCTURE: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"input\": \"data_task_1/final_data\",\n",
    "            \"output\": \"data_task_2\",\n",
    "            \"processed\": \"data_task_2/processed\",\n",
    "            \"enriched\": \"data_task_2/enriched\",\n",
    "            \"analysis\": \"data_task_2/analysis\",\n",
    "            \"anomalies\": \"data_task_2/anomalies\",\n",
    "            \"visualizations\": \"data_task_2/visualizations\",\n",
    "            \"logs\": \"logs_task_2\",\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "b15f3c48b3af863a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def setup_analysis_environment(log_dir: Optional[Path] = None) -> None:\n",
    "    \"\"\"Configure l'environnement d'analyse\"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    log_format = \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\"\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=log_format,\n",
    "        datefmt=date_format,\n",
    "        handlers=[console_handler],\n",
    "    )\n",
    "\n",
    "    if log_dir:\n",
    "        log_dir = Path(log_dir)\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        file_handler = RotatingFileHandler(\n",
    "            log_dir / \"analysis.log\",\n",
    "            maxBytes=5_000_000,\n",
    "            backupCount=3,\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        file_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "        logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 100)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"Set2\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"figure.figsize\": (14, 8),\n",
    "            \"axes.titlesize\": 14,\n",
    "            \"axes.labelsize\": 12,\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "727559e67c14f3c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DirectoryManager:\n",
    "    \"\"\"Gestionnaire de répertoires pour l'analyse\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_dir: Optional[Path] = None, config: Optional[AnalysisConfig] = None\n",
    "    ):\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._directories: Dict[str, Path] = {}\n",
    "\n",
    "    def initialize_structure(self) -> Dict[str, Path]:\n",
    "        \"\"\"Crée la structure de répertoires\"\"\"\n",
    "        for name, path in self.config.DIRECTORY_STRUCTURE.items():\n",
    "            full_path = self.base_dir / path\n",
    "            full_path.mkdir(parents=True, exist_ok=True)\n",
    "            self._directories[name] = full_path\n",
    "\n",
    "        self.logger.info(f\"✅ {len(self._directories)} répertoires créés\")\n",
    "        return self._directories\n",
    "\n",
    "    def get_path(self, name: str) -> Optional[Path]:\n",
    "        \"\"\"Récupère un chemin de répertoire\"\"\"\n",
    "        return self._directories.get(name)\n",
    "\n"
   ],
   "id": "5a51cd7b34fc4262"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class AnomalyReport:\n",
    "    \"\"\"Rapport de détection d'anomalies\"\"\"\n",
    "\n",
    "    dataset_name: str\n",
    "    variable: str\n",
    "    anomaly_type: str\n",
    "    anomaly_count: int\n",
    "    anomaly_percentage: float\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"dataset\": self.dataset_name,\n",
    "            \"variable\": self.variable,\n",
    "            \"anomaly_type\": self.anomaly_type,\n",
    "            \"count\": self.anomaly_count,\n",
    "            \"percentage\": round(self.anomaly_percentage, 2),\n",
    "            \"details\": str(self.details),\n",
    "            \"timestamp\": self.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n"
   ],
   "id": "3cd8b74e3e0b04ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Chargeur de données consolidées\"\"\"\n",
    "\n",
    "    def __init__(self, input_dir: Path):\n",
    "        self.input_dir = input_dir\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def load_all_datasets(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Charge tous les datasets disponibles\"\"\"\n",
    "        datasets = {}\n",
    "\n",
    "        if not self.input_dir.exists():\n",
    "            self.logger.error(f\"❌ Répertoire introuvable: {self.input_dir}\")\n",
    "            return datasets\n",
    "\n",
    "        csv_files = list(self.input_dir.glob(\"*.csv\"))\n",
    "\n",
    "        if not csv_files:\n",
    "            self.logger.warning(f\"⚠️ Aucun fichier CSV trouvé dans {self.input_dir}\")\n",
    "            return datasets\n",
    "\n",
    "        self.logger.info(f\"📂 Chargement de {len(csv_files)} fichiers...\")\n",
    "\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                dataset_name = file_path.stem\n",
    "                df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    datasets[dataset_name] = df\n",
    "                    self.logger.info(\n",
    "                        f\"✅ {dataset_name}: {len(df)} lignes, {len(df.columns)} colonnes\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.logger.warning(f\"⚠️ {dataset_name}: fichier vide\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    f\"❌ Erreur lors du chargement de {file_path.name}: {e}\"\n",
    "                )\n",
    "\n",
    "        self.logger.info(f\"📊 Total: {len(datasets)} datasets chargés\")\n",
    "        return datasets\n",
    "\n"
   ],
   "id": "c67280c0e2db2d86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DescriptiveAnalyzer:\n",
    "    \"\"\"Analyseur descriptif des données\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_dataset(self, df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyse descriptive complète d'un dataset\"\"\"\n",
    "        self.logger.info(f\"📊 Analyse descriptive: {dataset_name}\")\n",
    "\n",
    "        analysis = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"shape\": df.shape,\n",
    "            \"memory_usage_mb\": round(df.memory_usage(deep=True).sum() / (1024**2), 2),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"dtypes\": df.dtypes.value_counts().to_dict(),\n",
    "        }\n",
    "\n",
    "        # Statistiques de complétude\n",
    "        analysis[\"completeness\"] = {\n",
    "            \"total_cells\": df.size,\n",
    "            \"non_null_cells\": df.notna().sum().sum(),\n",
    "            \"null_cells\": df.isna().sum().sum(),\n",
    "            \"completeness_rate\": round(df.notna().sum().sum() / df.size * 100, 2),\n",
    "        }\n",
    "\n",
    "        # Statistiques par colonne\n",
    "        column_stats = []\n",
    "        for col in df.columns:\n",
    "            stat = {\n",
    "                \"column\": col,\n",
    "                \"dtype\": str(df[col].dtype),\n",
    "                \"non_null\": int(df[col].notna().sum()),\n",
    "                \"null\": int(df[col].isna().sum()),\n",
    "                \"null_pct\": round(df[col].isna().sum() / len(df) * 100, 2),\n",
    "                \"unique\": int(df[col].nunique()),\n",
    "            }\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                stat.update(\n",
    "                    {\n",
    "                        \"min\": df[col].min(),\n",
    "                        \"max\": df[col].max(),\n",
    "                        \"mean\": round(df[col].mean(), 3),\n",
    "                        \"median\": df[col].median(),\n",
    "                        \"std\": round(df[col].std(), 3),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            column_stats.append(stat)\n",
    "\n",
    "        analysis[\"column_statistics\"] = pd.DataFrame(column_stats)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def identify_temporal_variables(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Identifie les variables temporelles\"\"\"\n",
    "        temporal_vars = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if any(\n",
    "                keyword in col.lower() for keyword in [\"year\", \"année\", \"date\", \"time\"]\n",
    "            ):\n",
    "                temporal_vars.append(col)\n",
    "            elif df[col].dtype == \"datetime64[ns]\":\n",
    "                temporal_vars.append(col)\n",
    "\n",
    "        return temporal_vars\n",
    "\n",
    "    def identify_spatial_variables(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Identifie les variables spatiales\"\"\"\n",
    "        spatial_vars = []\n",
    "\n",
    "        spatial_keywords = [\n",
    "            \"région\",\n",
    "            \"region\",\n",
    "            \"département\",\n",
    "            \"department\",\n",
    "            \"commune\",\n",
    "            \"ville\",\n",
    "            \"city\",\n",
    "            \"localité\",\n",
    "            \"locality\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"admin\",\n",
    "        ]\n",
    "\n",
    "        for col in df.columns:\n",
    "            if any(keyword in col.lower() for keyword in spatial_keywords):\n",
    "                spatial_vars.append(col)\n",
    "\n",
    "        return spatial_vars\n",
    "\n",
    "    def generate_summary_report(\n",
    "        self, analyses: Dict[str, Dict[str, Any]]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Génère un rapport de synthèse\"\"\"\n",
    "        summary_data = []\n",
    "\n",
    "        for dataset_name, analysis in analyses.items():\n",
    "            summary_data.append(\n",
    "                {\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"rows\": analysis[\"shape\"][0],\n",
    "                    \"columns\": analysis[\"shape\"][1],\n",
    "                    \"memory_mb\": analysis[\"memory_usage_mb\"],\n",
    "                    \"completeness_rate\": analysis[\"completeness\"][\"completeness_rate\"],\n",
    "                    \"null_cells\": analysis[\"completeness\"][\"null_cells\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n"
   ],
   "id": "556568494c50ca7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TrendAnalyzer:\n",
    "    \"\"\"Analyseur de tendances temporelles\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_temporal_trends(\n",
    "        self, df: pd.DataFrame, time_col: str, value_cols: List[str]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Analyse les tendances temporelles\"\"\"\n",
    "        self.logger.info(f\"📈 Analyse des tendances temporelles\")\n",
    "\n",
    "        trends = {}\n",
    "\n",
    "        for value_col in value_cols:\n",
    "            if value_col not in df.columns or not pd.api.types.is_numeric_dtype(\n",
    "                df[value_col]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Agrégation par période temporelle\n",
    "                trend_df = (\n",
    "                    df.groupby(time_col)[value_col]\n",
    "                    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                # Calcul des variations\n",
    "                trend_df[\"pct_change\"] = trend_df[\"mean\"].pct_change() * 100\n",
    "                trend_df[\"cumulative_change\"] = (\n",
    "                    (trend_df[\"mean\"] / trend_df[\"mean\"].iloc[0]) - 1\n",
    "                ) * 100\n",
    "\n",
    "                trends[value_col] = trend_df\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"⚠️ Erreur tendance {value_col}: {e}\")\n",
    "\n",
    "        return trends\n",
    "\n",
    "    def calculate_growth_rates(\n",
    "        self, df: pd.DataFrame, time_col: str, value_col: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Calcule les taux de croissance\"\"\"\n",
    "        result = df.copy()\n",
    "        result = result.sort_values(time_col)\n",
    "\n",
    "        # Taux de croissance annuel\n",
    "        result[\"growth_rate\"] = result[value_col].pct_change() * 100\n",
    "\n",
    "        # Taux de croissance annuel moyen (CAGR)\n",
    "        if len(result) > 1:\n",
    "            first_value = result[value_col].iloc[0]\n",
    "            last_value = result[value_col].iloc[-1]\n",
    "            n_periods = len(result) - 1\n",
    "\n",
    "            if first_value > 0 and last_value > 0:\n",
    "                cagr = (((last_value / first_value) ** (1 / n_periods)) - 1) * 100\n",
    "                result[\"cagr\"] = cagr\n",
    "\n",
    "        return result\n",
    "\n"
   ],
   "id": "b69e1f2eed384dd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SpatialAnalyzer:\n",
    "    \"\"\"Analyseur de dynamiques spatiales\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_spatial_distribution(\n",
    "        self, df: pd.DataFrame, spatial_col: str, value_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Analyse la distribution spatiale\"\"\"\n",
    "        self.logger.info(f\"🗺️ Analyse spatiale sur {spatial_col}\")\n",
    "\n",
    "        spatial_stats = []\n",
    "\n",
    "        for value_col in value_cols:\n",
    "            if value_col not in df.columns or not pd.api.types.is_numeric_dtype(\n",
    "                df[value_col]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stats_by_location = (\n",
    "                    df.groupby(spatial_col)[value_col]\n",
    "                    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\", \"sum\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                stats_by_location[\"variable\"] = value_col\n",
    "                stats_by_location[\"cv\"] = (\n",
    "                    stats_by_location[\"std\"] / stats_by_location[\"mean\"]\n",
    "                ) * 100\n",
    "\n",
    "                spatial_stats.append(stats_by_location)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"⚠️ Erreur spatiale {value_col}: {e}\")\n",
    "\n",
    "        if spatial_stats:\n",
    "            return pd.concat(spatial_stats, ignore_index=True)\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def calculate_regional_disparities(\n",
    "        self, df: pd.DataFrame, spatial_col: str, value_col: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calcule les disparités régionales\"\"\"\n",
    "        regional_means = df.groupby(spatial_col)[value_col].mean()\n",
    "\n",
    "        disparities = {\n",
    "            \"gini_coefficient\": self._calculate_gini(regional_means.values),\n",
    "            \"coefficient_variation\": (regional_means.std() / regional_means.mean())\n",
    "            * 100,\n",
    "            \"range_ratio\": (\n",
    "                regional_means.max() / regional_means.min()\n",
    "                if regional_means.min() > 0\n",
    "                else np.nan\n",
    "            ),\n",
    "            \"max_value\": regional_means.max(),\n",
    "            \"min_value\": regional_means.min(),\n",
    "            \"mean_value\": regional_means.mean(),\n",
    "        }\n",
    "\n",
    "        return disparities\n",
    "\n",
    "    def _calculate_gini(self, values: np.ndarray) -> float:\n",
    "        \"\"\"Calcule le coefficient de Gini\"\"\"\n",
    "        values = np.array(values)\n",
    "        values = values[~np.isnan(values)]\n",
    "\n",
    "        if len(values) == 0:\n",
    "            return np.nan\n",
    "\n",
    "        values = np.sort(values)\n",
    "        n = len(values)\n",
    "        index = np.arange(1, n + 1)\n",
    "\n",
    "        return (2 * np.sum(index * values)) / (n * np.sum(values)) - (n + 1) / n\n",
    "\n"
   ],
   "id": "68eb14222d66865e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CorrelationAnalyzer:\n",
    "    \"\"\"Analyseur de corrélations\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def calculate_correlations(\n",
    "        self, df: pd.DataFrame, method: str = \"pearson\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Calcule la matrice de corrélation\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        if len(numeric_cols) < 2:\n",
    "            self.logger.warning(\n",
    "                \"⚠️ Pas assez de colonnes numériques pour la corrélation\"\n",
    "            )\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if method == \"pearson\":\n",
    "            corr_matrix = df[numeric_cols].corr(method=\"pearson\")\n",
    "        elif method == \"spearman\":\n",
    "            corr_matrix = df[numeric_cols].corr(method=\"spearman\")\n",
    "        else:\n",
    "            corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "        return corr_matrix\n",
    "\n",
    "    def find_strong_correlations(\n",
    "        self, corr_matrix: pd.DataFrame, threshold: Optional[float] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Identifie les corrélations fortes\"\"\"\n",
    "        threshold = threshold or self.config.CORRELATION_THRESHOLD\n",
    "\n",
    "        strong_corr = []\n",
    "\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                var1 = corr_matrix.columns[i]\n",
    "                var2 = corr_matrix.columns[j]\n",
    "                corr_value = corr_matrix.iloc[i, j]\n",
    "\n",
    "                if abs(corr_value) >= threshold:\n",
    "                    strong_corr.append(\n",
    "                        {\n",
    "                            \"variable_1\": var1,\n",
    "                            \"variable_2\": var2,\n",
    "                            \"correlation\": round(corr_value, 3),\n",
    "                            \"strength\": (\n",
    "                                \"forte\" if abs(corr_value) >= 0.8 else \"modérée\"\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if not strong_corr:\n",
    "            return pd.DataFrame(\n",
    "                columns=[\"variable_1\", \"variable_2\", \"correlation\", \"strength\"]\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(strong_corr).sort_values(\n",
    "            \"correlation\", key=abs, ascending=False\n",
    "        )\n",
    "\n",
    "    def cross_dataset_correlation(\n",
    "        self, df1: pd.DataFrame, df2: pd.DataFrame, merge_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Corrélations croisées entre datasets\"\"\"\n",
    "        try:\n",
    "            merged = pd.merge(\n",
    "                df1, df2, on=merge_cols, how=\"inner\", suffixes=(\"_1\", \"_2\")\n",
    "            )\n",
    "\n",
    "            numeric_cols = merged.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "            if len(numeric_cols) >= 2:\n",
    "                return self.calculate_correlations(merged[numeric_cols])\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Erreur corrélation croisée: {e}\")\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n"
   ],
   "id": "a7ac0578378691dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AnomalyDetector:\n",
    "    \"\"\"Détecteur d'anomalies\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.anomaly_reports: List[AnomalyReport] = []\n",
    "\n",
    "    def detect_zscore_anomalies(\n",
    "        self, df: pd.DataFrame, dataset_name: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Détecte les anomalies par Z-score\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        anomalies_df = pd.DataFrame()\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 3:\n",
    "                continue\n",
    "\n",
    "            z_scores = np.abs(zscore(df[col].dropna()))\n",
    "            anomaly_mask = z_scores > self.config.ZSCORE_THRESHOLD\n",
    "\n",
    "            if anomaly_mask.any():\n",
    "                anomaly_count = anomaly_mask.sum()\n",
    "                anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "\n",
    "                report = AnomalyReport(\n",
    "                    dataset_name=dataset_name,\n",
    "                    variable=col,\n",
    "                    anomaly_type=\"zscore\",\n",
    "                    anomaly_count=anomaly_count,\n",
    "                    anomaly_percentage=anomaly_pct,\n",
    "                    details={\"threshold\": self.config.ZSCORE_THRESHOLD},\n",
    "                )\n",
    "                self.anomaly_reports.append(report)\n",
    "\n",
    "        return anomalies_df\n",
    "\n",
    "    def detect_iqr_anomalies(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Détecte les anomalies par IQR\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        anomalies_list = []\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 4:\n",
    "                continue\n",
    "\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            lower_bound = Q1 - self.config.IQR_MULTIPLIER * IQR\n",
    "            upper_bound = Q3 + self.config.IQR_MULTIPLIER * IQR\n",
    "\n",
    "            anomaly_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "\n",
    "            if anomaly_mask.any():\n",
    "                anomaly_count = anomaly_mask.sum()\n",
    "                anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "\n",
    "                report = AnomalyReport(\n",
    "                    dataset_name=dataset_name,\n",
    "                    variable=col,\n",
    "                    anomaly_type=\"iqr\",\n",
    "                    anomaly_count=anomaly_count,\n",
    "                    anomaly_percentage=anomaly_pct,\n",
    "                    details={\n",
    "                        \"lower_bound\": lower_bound,\n",
    "                        \"upper_bound\": upper_bound,\n",
    "                        \"Q1\": Q1,\n",
    "                        \"Q3\": Q3,\n",
    "                        \"IQR\": IQR,\n",
    "                    },\n",
    "                )\n",
    "                self.anomaly_reports.append(report)\n",
    "\n",
    "                anomalies_list.append(\n",
    "                    {\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"variable\": col,\n",
    "                        \"anomaly_indices\": df[anomaly_mask].index.tolist(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return pd.DataFrame(anomalies_list) if anomalies_list else pd.DataFrame()\n",
    "\n",
    "    def detect_inconsistencies(\n",
    "        self, df: pd.DataFrame, dataset_name: str\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Détecte les incohérences\"\"\"\n",
    "        inconsistencies = []\n",
    "\n",
    "        # Vérification des valeurs négatives inappropriées\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            if any(\n",
    "                keyword in col.lower()\n",
    "                for keyword in [\"population\", \"count\", \"nombre\", \"effectif\"]\n",
    "            ):\n",
    "                negative_count = (df[col] < 0).sum()\n",
    "                if negative_count > 0:\n",
    "                    inconsistencies.append(\n",
    "                        {\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variable\": col,\n",
    "                            \"issue\": \"valeurs_négatives\",\n",
    "                            \"count\": negative_count,\n",
    "                            \"description\": f\"{negative_count} valeurs négatives pour une variable de comptage\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Vérification des années invalides\n",
    "        for col in df.columns:\n",
    "            if \"year\" in col.lower() or \"année\" in col.lower():\n",
    "                invalid_years = df[(df[col] < 1900) | (df[col] > 2025)]\n",
    "                if len(invalid_years) > 0:\n",
    "                    inconsistencies.append(\n",
    "                        {\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variable\": col,\n",
    "                            \"issue\": \"années_invalides\",\n",
    "                            \"count\": len(invalid_years),\n",
    "                            \"description\": f\"{len(invalid_years)} années hors de la plage [1900-2025]\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return inconsistencies\n",
    "\n",
    "    def generate_anomaly_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Génère le rapport d'anomalies\"\"\"\n",
    "        if not self.anomaly_reports:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return pd.DataFrame([report.to_dict() for report in self.anomaly_reports])\n",
    "\n"
   ],
   "id": "5d44a1e3489e05ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class IndicatorBuilder:\n",
    "    \"\"\"Constructeur d'indicateurs dérivés\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def build_demographic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Construit les indicateurs démographiques\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        # Taux de croissance de la population\n",
    "        if \"population\" in df.columns or \"total_population\" in df.columns:\n",
    "            pop_col = \"population\" if \"population\" in df.columns else \"total_population\"\n",
    "\n",
    "            if \"year\" in df.columns or \"année\" in df.columns:\n",
    "                time_col = \"year\" if \"year\" in df.columns else \"année\"\n",
    "                result = result.sort_values(time_col)\n",
    "                result[\"population_growth_rate\"] = result[pop_col].pct_change() * 100\n",
    "\n",
    "        # Ratio de population jeune\n",
    "        if \"population_0_14\" in df.columns and \"total_population\" in df.columns:\n",
    "            result[\"youth_population_ratio\"] = (\n",
    "                result[\"population_0_14\"] / result[\"total_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        # Ratio de population âgée\n",
    "        if \"population_65_plus\" in df.columns and \"total_population\" in df.columns:\n",
    "            result[\"elderly_population_ratio\"] = (\n",
    "                result[\"population_65_plus\"] / result[\"total_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        # Densité de population (si superficie disponible)\n",
    "        if all(col in df.columns for col in [\"total_population\", \"surface_area_km2\"]):\n",
    "            result[\"population_density\"] = (\n",
    "                result[\"total_population\"] / result[\"surface_area_km2\"]\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"✅ Indicateurs démographiques créés\")\n",
    "        return result\n",
    "\n",
    "    def build_economic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Construit les indicateurs économiques\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        # PIB par habitant\n",
    "        if all(col in df.columns for col in [\"gdp\", \"total_population\"]):\n",
    "            result[\"gdp_per_capita\"] = result[\"gdp\"] / result[\"total_population\"]\n",
    "\n",
    "        # Taux de croissance du PIB\n",
    "        if \"gdp\" in df.columns:\n",
    "            if \"year\" in df.columns or \"année\" in df.columns:\n",
    "                time_col = \"year\" if \"year\" in df.columns else \"année\"\n",
    "                result = result.sort_values(time_col)\n",
    "                result[\"gdp_growth_rate\"] = result[\"gdp\"].pct_change() * 100\n",
    "\n",
    "        # Indice de PIB (base 100)\n",
    "        if \"gdp\" in df.columns and \"year\" in df.columns:\n",
    "            base_year = self.config.GDP_BASE_YEAR\n",
    "            base_gdp = (\n",
    "                result[result[\"year\"] == base_year][\"gdp\"].iloc[0]\n",
    "                if base_year in result[\"year\"].values\n",
    "                else result[\"gdp\"].iloc[0]\n",
    "            )\n",
    "            result[\"gdp_index\"] = (result[\"gdp\"] / base_gdp) * 100\n",
    "\n",
    "        self.logger.info(\"✅ Indicateurs économiques créés\")\n",
    "        return result\n",
    "\n",
    "    def build_education_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Construit les indicateurs d'éducation\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        # Taux de scolarisation net\n",
    "        if all(\n",
    "            col in df.columns for col in [\"enrolled_students\", \"school_age_population\"]\n",
    "        ):\n",
    "            result[\"net_enrollment_rate\"] = (\n",
    "                result[\"enrolled_students\"] / result[\"school_age_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        # Ratio élèves-enseignants\n",
    "        if all(col in df.columns for col in [\"total_students\", \"total_teachers\"]):\n",
    "            result[\"student_teacher_ratio\"] = (\n",
    "                result[\"total_students\"] / result[\"total_teachers\"]\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"✅ Indicateurs d'éducation créés\")\n",
    "        return result\n",
    "\n",
    "    def build_composite_index(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        indicators: List[str],\n",
    "        weights: Optional[List[float]] = None,\n",
    "        index_name: str = \"composite_index\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Construit un indice composite\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        available_indicators = [ind for ind in indicators if ind in df.columns]\n",
    "\n",
    "        if not available_indicators:\n",
    "            self.logger.warning(f\"⚠️ Aucun indicateur disponible pour {index_name}\")\n",
    "            return result\n",
    "\n",
    "        # Normalisation des indicateurs (min-max)\n",
    "        scaler = StandardScaler()\n",
    "        normalized_data = scaler.fit_transform(df[available_indicators].fillna(0))\n",
    "        normalized_df = pd.DataFrame(\n",
    "            normalized_data, columns=available_indicators, index=df.index\n",
    "        )\n",
    "\n",
    "        # Application des poids\n",
    "        if weights is None:\n",
    "            weights = [1 / len(available_indicators)] * len(available_indicators)\n",
    "\n",
    "        # Calcul de l'indice composite\n",
    "        result[index_name] = sum(\n",
    "            normalized_df[ind] * weight\n",
    "            for ind, weight in zip(available_indicators, weights)\n",
    "        )\n",
    "\n",
    "        # Normalisation finale (0-100)\n",
    "        result[index_name] = (\n",
    "            (result[index_name] - result[index_name].min())\n",
    "            / (result[index_name].max() - result[index_name].min())\n",
    "        ) * 100\n",
    "\n",
    "        self.logger.info(f\"✅ Indice composite '{index_name}' créé\")\n",
    "        return result\n",
    "\n",
    "    def build_regional_development_index(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Construit un indice de développement régional\"\"\"\n",
    "        indicators = []\n",
    "\n",
    "        # Sélection automatique des indicateurs disponibles\n",
    "        if \"gdp_per_capita\" in df.columns:\n",
    "            indicators.append(\"gdp_per_capita\")\n",
    "        if \"net_enrollment_rate\" in df.columns:\n",
    "            indicators.append(\"net_enrollment_rate\")\n",
    "        if \"life_expectancy\" in df.columns:\n",
    "            indicators.append(\"life_expectancy\")\n",
    "        if \"access_electricity\" in df.columns:\n",
    "            indicators.append(\"access_electricity\")\n",
    "\n",
    "        if indicators:\n",
    "            return self.build_composite_index(\n",
    "                df, indicators, index_name=\"regional_development_index\"\n",
    "            )\n",
    "\n",
    "        self.logger.warning(\"⚠️ Pas assez d'indicateurs pour l'indice de développement\")\n",
    "        return df\n",
    "\n"
   ],
   "id": "ade1dba5c1b5f6f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AggregationEngine:\n",
    "    \"\"\"Moteur d'agrégation temporelle et spatiale\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def temporal_aggregation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        time_col: str,\n",
    "        value_cols: List[str],\n",
    "        agg_functions: Optional[Dict[str, str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if agg_functions is None:\n",
    "            agg_functions = {col: \"mean\" for col in value_cols}\n",
    "\n",
    "        valid_agg = {\n",
    "            col: func\n",
    "            for col, func in agg_functions.items()\n",
    "            if col in df.columns and col != time_col\n",
    "        }\n",
    "\n",
    "        if not valid_agg:\n",
    "            self.logger.warning(\"⚠️ Aucune colonne valide pour l'agrégation\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        aggregated = df.groupby(time_col).agg(valid_agg).reset_index()\n",
    "\n",
    "        aggregated.columns = [\n",
    "            f\"{col}_{func}\" if col != time_col else col\n",
    "            for col, func in zip(\n",
    "                aggregated.columns, [time_col] + list(valid_agg.values())\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.logger.info(f\"✅ Agrégation temporelle: {len(aggregated)} périodes\")\n",
    "        return aggregated\n",
    "\n",
    "    def spatial_aggregation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        spatial_col: str,\n",
    "        value_cols: List[str],\n",
    "        agg_functions: Optional[Dict[str, str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Agrégation spatiale\"\"\"\n",
    "        if agg_functions is None:\n",
    "            agg_functions = {col: \"sum\" for col in value_cols}\n",
    "\n",
    "        valid_agg = {\n",
    "            col: func\n",
    "            for col, func in agg_functions.items()\n",
    "            if col in df.columns and col != spatial_col\n",
    "        }\n",
    "\n",
    "        if not valid_agg:\n",
    "            self.logger.warning(\"⚠️ Aucune colonne valide pour l'agrégation spatiale\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        aggregated = df.groupby(spatial_col).agg(valid_agg).reset_index()\n",
    "\n",
    "        self.logger.info(f\"✅ Agrégation spatiale: {len(aggregated)} zones\")\n",
    "        return aggregated\n",
    "\n",
    "    def normalize_by_population(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        value_cols: List[str],\n",
    "        population_col: str = \"total_population\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Normalise les valeurs par habitant\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        if population_col not in df.columns:\n",
    "            self.logger.warning(f\"⚠️ Colonne {population_col} introuvable\")\n",
    "            return result\n",
    "\n",
    "        for col in value_cols:\n",
    "            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                new_col_name = f\"{col}_per_capita\"\n",
    "                result[new_col_name] = result[col] / result[population_col]\n",
    "                self.logger.info(f\"✅ Créé: {new_col_name}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def multi_level_aggregation(\n",
    "        self, df: pd.DataFrame, group_cols: List[str], value_cols: List[str]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Agrégation multi-niveaux\"\"\"\n",
    "        aggregations = {}\n",
    "\n",
    "        for i in range(1, len(group_cols) + 1):\n",
    "            level_cols = group_cols[:i]\n",
    "            level_name = \"_\".join(level_cols)\n",
    "\n",
    "            agg_dict = {\n",
    "                col: [\"sum\", \"mean\", \"count\"] for col in value_cols if col in df.columns\n",
    "            }\n",
    "\n",
    "            if agg_dict:\n",
    "                agg_result = df.groupby(level_cols).agg(agg_dict).reset_index()\n",
    "                agg_result.columns = [\n",
    "                    \"_\".join(col).strip(\"_\") for col in agg_result.columns\n",
    "                ]\n",
    "                aggregations[level_name] = agg_result\n",
    "                self.logger.info(f\"✅ Agrégation niveau {i}: {len(agg_result)} groupes\")\n",
    "\n",
    "        return aggregations\n",
    "\n"
   ],
   "id": "80fbe76e080a2f69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class VisualizationEngine:\n",
    "    \"\"\"Moteur de visualisation\"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def plot_temporal_trends(\n",
    "        self, trends: Dict[str, pd.DataFrame], time_col: str, save: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"Visualise les tendances temporelles\"\"\"\n",
    "        n_plots = len(trends)\n",
    "        if n_plots == 0:\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(min(n_plots, 3), 1, figsize=(14, 4 * min(n_plots, 3)))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for idx, (var_name, trend_df) in enumerate(list(trends.items())[:3]):\n",
    "            ax = axes[idx]\n",
    "            ax.plot(\n",
    "                trend_df[time_col],\n",
    "                trend_df[\"mean\"],\n",
    "                marker=\"o\",\n",
    "                linewidth=2,\n",
    "                label=\"Moyenne\",\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                trend_df[time_col],\n",
    "                trend_df[\"mean\"] - trend_df[\"std\"],\n",
    "                trend_df[\"mean\"] + trend_df[\"std\"],\n",
    "                alpha=0.3,\n",
    "                label=\"±1 écart-type\",\n",
    "            )\n",
    "            ax.set_title(\n",
    "                f\"Évolution temporelle: {var_name}\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Année\")\n",
    "            ax.set_ylabel(\"Valeur\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"temporal_trends.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"💾 Graphique sauvegardé: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_correlation_heatmap(\n",
    "        self, corr_matrix: pd.DataFrame, save: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"Visualise la matrice de corrélation\"\"\"\n",
    "        if corr_matrix.empty:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "        sns.heatmap(\n",
    "            corr_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        ax.set_title(\"Matrice de corrélation\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"correlation_heatmap.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"💾 Heatmap sauvegardée: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_spatial_distribution(\n",
    "        self,\n",
    "        spatial_stats: pd.DataFrame,\n",
    "        spatial_col: str,\n",
    "        value_col: str = \"mean\",\n",
    "        save: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Visualise la distribution spatiale\"\"\"\n",
    "        if spatial_stats.empty:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        spatial_stats_sorted = spatial_stats.sort_values(\n",
    "            value_col, ascending=False\n",
    "        ).head(20)\n",
    "\n",
    "        bars = ax.barh(\n",
    "            spatial_stats_sorted[spatial_col], spatial_stats_sorted[value_col]\n",
    "        )\n",
    "\n",
    "        # Gradient de couleurs\n",
    "        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(bars)))\n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "\n",
    "        ax.set_xlabel(\"Valeur moyenne\")\n",
    "        ax.set_title(f\"Distribution spatiale (Top 20)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"spatial_distribution.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"💾 Distribution sauvegardée: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n"
   ],
   "id": "e7c8abd9018f3977"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ExplorationOrchestrator:\n",
    "    \"\"\"Orchestrateur de l'exploration et analyse\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config: Optional[AnalysisConfig] = None, base_dir: Optional[Path] = None\n",
    "    ):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialisation des composants\n",
    "        self.dir_manager = DirectoryManager(self.base_dir, self.config)\n",
    "        self.directories = self.dir_manager.initialize_structure()\n",
    "\n",
    "        self.loader = DataLoader(self.directories[\"input\"])\n",
    "        self.descriptive_analyzer = DescriptiveAnalyzer(self.config)\n",
    "        self.trend_analyzer = TrendAnalyzer(self.config)\n",
    "        self.spatial_analyzer = SpatialAnalyzer(self.config)\n",
    "        self.correlation_analyzer = CorrelationAnalyzer(self.config)\n",
    "        self.anomaly_detector = AnomalyDetector(self.config)\n",
    "        self.indicator_builder = IndicatorBuilder(self.config)\n",
    "        self.aggregation_engine = AggregationEngine(self.config)\n",
    "        self.viz_engine = VisualizationEngine(self.directories[\"visualizations\"])\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "    def run_descriptive_analysis(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 1: Analyse descriptive\"\"\"\n",
    "        self.logger.info(\"📊 PHASE 1: ANALYSE DESCRIPTIVE\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"📊 PHASE 1: ANALYSE DESCRIPTIVE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        analyses = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            self.logger.info(f\"▶️ Analyse: {name}\")\n",
    "            analysis = self.descriptive_analyzer.analyze_dataset(df, name)\n",
    "            analyses[name] = analysis\n",
    "\n",
    "            # Sauvegarde des statistiques\n",
    "            if \"column_statistics\" in analysis:\n",
    "                stats_path = self.directories[\"analysis\"] / f\"{name}_column_stats.csv\"\n",
    "                analysis[\"column_statistics\"].to_csv(stats_path, index=False)\n",
    "\n",
    "        # Rapport de synthèse\n",
    "        summary_report = self.descriptive_analyzer.generate_summary_report(analyses)\n",
    "        summary_path = self.directories[\"analysis\"] / \"descriptive_summary.csv\"\n",
    "        summary_report.to_csv(summary_path, index=False)\n",
    "\n",
    "        print(\"\\n✅ Analyse descriptive terminée\")\n",
    "        print(f\"   Datasets analysés: {len(analyses)}\")\n",
    "        print(f\"   Rapport: {summary_path.name}\")\n",
    "\n",
    "        return analyses\n",
    "\n",
    "    def run_trend_analysis(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 2: Analyse des tendances\"\"\"\n",
    "        self.logger.info(\"📈 PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"📈 PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        all_trends = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            temporal_vars = self.descriptive_analyzer.identify_temporal_variables(df)\n",
    "\n",
    "            if not temporal_vars:\n",
    "                continue\n",
    "\n",
    "            time_col = temporal_vars[0]\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            value_cols = [col for col in numeric_cols if col != time_col][:5]\n",
    "\n",
    "            if value_cols:\n",
    "                trends = self.trend_analyzer.analyze_temporal_trends(\n",
    "                    df, time_col, value_cols\n",
    "                )\n",
    "\n",
    "                if trends:\n",
    "                    all_trends[name] = trends\n",
    "\n",
    "                    # Sauvegarde\n",
    "                    for var, trend_df in trends.items():\n",
    "                        trend_path = (\n",
    "                            self.directories[\"analysis\"] / f\"{name}_{var}_trend.csv\"\n",
    "                        )\n",
    "                        trend_df.to_csv(trend_path, index=False)\n",
    "\n",
    "                    # Visualisation\n",
    "                    self.viz_engine.plot_temporal_trends(trends, time_col)\n",
    "\n",
    "        print(f\"\\n✅ Analyse des tendances terminée\")\n",
    "        print(f\"   Datasets avec tendances: {len(all_trends)}\")\n",
    "\n",
    "        return all_trends\n",
    "\n",
    "    def run_spatial_analysis(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 3: Analyse spatiale\"\"\"\n",
    "        self.logger.info(\"🗺️ PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🗺️ PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        spatial_results = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            spatial_vars = self.descriptive_analyzer.identify_spatial_variables(df)\n",
    "\n",
    "            if not spatial_vars:\n",
    "                continue\n",
    "\n",
    "            spatial_col = spatial_vars[0]\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            value_cols = [col for col in numeric_cols if col != spatial_col][:5]\n",
    "\n",
    "            if value_cols:\n",
    "                spatial_dist = self.spatial_analyzer.analyze_spatial_distribution(\n",
    "                    df, spatial_col, value_cols\n",
    "                )\n",
    "\n",
    "                if not spatial_dist.empty:\n",
    "                    spatial_results[name] = spatial_dist\n",
    "\n",
    "                    # Sauvegarde\n",
    "                    spatial_path = (\n",
    "                        self.directories[\"analysis\"] / f\"{name}_spatial_analysis.csv\"\n",
    "                    )\n",
    "                    spatial_dist.to_csv(spatial_path, index=False)\n",
    "\n",
    "                    # Visualisation\n",
    "                    self.viz_engine.plot_spatial_distribution(spatial_dist, spatial_col)\n",
    "\n",
    "        print(f\"\\n✅ Analyse spatiale terminée\")\n",
    "        print(f\"   Datasets analysés spatialement: {len(spatial_results)}\")\n",
    "\n",
    "        return spatial_results\n",
    "\n",
    "    def run_correlation_analysis(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 4: Analyse des corrélations\"\"\"\n",
    "        self.logger.info(\"🔗 PHASE 4: ANALYSE DES CORRÉLATIONS\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔗 PHASE 4: ANALYSE DES CORRÉLATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        correlations = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            corr_matrix = self.correlation_analyzer.calculate_correlations(df)\n",
    "\n",
    "            if not corr_matrix.empty:\n",
    "                correlations[name] = {\n",
    "                    \"matrix\": corr_matrix,\n",
    "                    \"strong_correlations\": self.correlation_analyzer.find_strong_correlations(\n",
    "                        corr_matrix\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                # Sauvegarde\n",
    "                corr_path = self.directories[\"analysis\"] / f\"{name}_correlations.csv\"\n",
    "                corr_matrix.to_csv(corr_path)\n",
    "\n",
    "                strong_corr_path = (\n",
    "                    self.directories[\"analysis\"] / f\"{name}_strong_correlations.csv\"\n",
    "                )\n",
    "                if not correlations[name][\"strong_correlations\"].empty:\n",
    "                    correlations[name][\"strong_correlations\"].to_csv(\n",
    "                        strong_corr_path, index=False\n",
    "                    )\n",
    "\n",
    "                # Visualisation\n",
    "                self.viz_engine.plot_correlation_heatmap(corr_matrix)\n",
    "\n",
    "        print(f\"\\n✅ Analyse des corrélations terminée\")\n",
    "        print(f\"   Datasets analysés: {len(correlations)}\")\n",
    "\n",
    "        return correlations\n",
    "\n",
    "    def run_anomaly_detection(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 5: Détection des anomalies\"\"\"\n",
    "        self.logger.info(\"🔍 PHASE 5: DÉTECTION DES ANOMALIES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔍 PHASE 5: DÉTECTION DES ANOMALIES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        all_anomalies = {}\n",
    "        all_inconsistencies = []\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            # Détection Z-score\n",
    "            self.anomaly_detector.detect_zscore_anomalies(df, name)\n",
    "\n",
    "            # Détection IQR\n",
    "            iqr_anomalies = self.anomaly_detector.detect_iqr_anomalies(df, name)\n",
    "\n",
    "            # Détection d'incohérences\n",
    "            inconsistencies = self.anomaly_detector.detect_inconsistencies(df, name)\n",
    "\n",
    "            if not iqr_anomalies.empty:\n",
    "                all_anomalies[name] = iqr_anomalies\n",
    "\n",
    "            if inconsistencies:\n",
    "                all_inconsistencies.extend(inconsistencies)\n",
    "\n",
    "        # Rapport global\n",
    "        anomaly_report = self.anomaly_detector.generate_anomaly_report()\n",
    "        if not anomaly_report.empty:\n",
    "            anomaly_path = self.directories[\"anomalies\"] / \"anomaly_report.csv\"\n",
    "            anomaly_report.to_csv(anomaly_path, index=False)\n",
    "            print(f\"\\n📄 Rapport d'anomalies: {anomaly_path.name}\")\n",
    "            print(f\"   Total anomalies détectées: {len(anomaly_report)}\")\n",
    "\n",
    "        # Rapport d'incohérences\n",
    "        if all_inconsistencies:\n",
    "            incon_df = pd.DataFrame(all_inconsistencies)\n",
    "            incon_path = self.directories[\"anomalies\"] / \"inconsistencies_report.csv\"\n",
    "            incon_df.to_csv(incon_path, index=False)\n",
    "            print(f\"📄 Rapport d'incohérences: {incon_path.name}\")\n",
    "            print(f\"   Total incohérences: {len(all_inconsistencies)}\")\n",
    "\n",
    "        print(f\"\\n✅ Détection des anomalies terminée\")\n",
    "\n",
    "        return {\n",
    "            \"anomalies\": all_anomalies,\n",
    "            \"anomaly_report\": anomaly_report,\n",
    "            \"inconsistencies\": all_inconsistencies,\n",
    "        }\n",
    "\n",
    "    def run_indicator_creation(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Phase 6: Création d'indicateurs\"\"\"\n",
    "        self.logger.info(\"🔧 PHASE 6: CRÉATION D'INDICATEURS DÉRIVÉS\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔧 PHASE 6: CRÉATION D'INDICATEURS DÉRIVÉS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        enriched_datasets = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            enriched = df.copy()\n",
    "\n",
    "            # Indicateurs démographiques\n",
    "            enriched = self.indicator_builder.build_demographic_indicators(enriched)\n",
    "\n",
    "            # Indicateurs économiques\n",
    "            enriched = self.indicator_builder.build_economic_indicators(enriched)\n",
    "\n",
    "            # Indicateurs d'éducation\n",
    "            enriched = self.indicator_builder.build_education_indicators(enriched)\n",
    "\n",
    "            # Indice de développement régional\n",
    "            enriched = self.indicator_builder.build_regional_development_index(enriched)\n",
    "\n",
    "            # Comptage des nouvelles colonnes\n",
    "            new_cols = set(enriched.columns) - set(df.columns)\n",
    "\n",
    "            if new_cols:\n",
    "                enriched_datasets[name] = enriched\n",
    "\n",
    "                # Sauvegarde\n",
    "                enriched_path = self.directories[\"enriched\"] / f\"{name}_enriched.csv\"\n",
    "                enriched.to_csv(enriched_path, index=False)\n",
    "\n",
    "                print(f\"\\n✅ {name}: {len(new_cols)} nouveaux indicateurs\")\n",
    "                for col in sorted(new_cols):\n",
    "                    print(f\"   - {col}\")\n",
    "\n",
    "        print(f\"\\n✅ Création d'indicateurs terminée\")\n",
    "        print(f\"   Datasets enrichis: {len(enriched_datasets)}\")\n",
    "\n",
    "        return enriched_datasets\n",
    "\n",
    "    def run_aggregations(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 7: Agrégations\"\"\"\n",
    "        self.logger.info(\"📊 PHASE 7: AGRÉGATIONS TEMPORELLES ET SPATIALES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"📊 PHASE 7: AGRÉGATIONS TEMPORELLES ET SPATIALES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        aggregations = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            dataset_aggs = {}\n",
    "\n",
    "            # Agrégation temporelle\n",
    "            temporal_vars = self.descriptive_analyzer.identify_temporal_variables(df)\n",
    "            if temporal_vars:\n",
    "                time_col = temporal_vars[0]\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols if col != time_col][:10]\n",
    "\n",
    "                if value_cols:\n",
    "                    temp_agg = self.aggregation_engine.temporal_aggregation(\n",
    "                        df, time_col, value_cols\n",
    "                    )\n",
    "                    if not temp_agg.empty:\n",
    "                        dataset_aggs[\"temporal\"] = temp_agg\n",
    "                        temp_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_temporal_agg.csv\"\n",
    "                        )\n",
    "                        temp_agg.to_csv(temp_path, index=False)\n",
    "\n",
    "            # Agrégation spatiale\n",
    "            spatial_vars = self.descriptive_analyzer.identify_spatial_variables(df)\n",
    "            if spatial_vars:\n",
    "                spatial_col = spatial_vars[0]\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols][:10]\n",
    "\n",
    "                if value_cols:\n",
    "                    spatial_agg = self.aggregation_engine.spatial_aggregation(\n",
    "                        df, spatial_col, value_cols\n",
    "                    )\n",
    "                    if not spatial_agg.empty:\n",
    "                        dataset_aggs[\"spatial\"] = spatial_agg\n",
    "                        spatial_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_spatial_agg.csv\"\n",
    "                        )\n",
    "                        spatial_agg.to_csv(spatial_path, index=False)\n",
    "\n",
    "            # Normalisation par population\n",
    "            if \"total_population\" in df.columns or \"population\" in df.columns:\n",
    "                pop_col = (\n",
    "                    \"total_population\"\n",
    "                    if \"total_population\" in df.columns\n",
    "                    else \"population\"\n",
    "                )\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols if col != pop_col][:5]\n",
    "\n",
    "                if value_cols:\n",
    "                    normalized = self.aggregation_engine.normalize_by_population(\n",
    "                        df, value_cols, pop_col\n",
    "                    )\n",
    "                    new_cols = [\n",
    "                        col for col in normalized.columns if \"_per_capita\" in col\n",
    "                    ]\n",
    "\n",
    "                    if new_cols:\n",
    "                        dataset_aggs[\"per_capita\"] = normalized[new_cols + [pop_col]]\n",
    "                        norm_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_per_capita.csv\"\n",
    "                        )\n",
    "                        normalized.to_csv(norm_path, index=False)\n",
    "\n",
    "            if dataset_aggs:\n",
    "                aggregations[name] = dataset_aggs\n",
    "                print(f\"\\n✅ {name}: {len(dataset_aggs)} types d'agrégation\")\n",
    "\n",
    "        print(f\"\\n✅ Agrégations terminées\")\n",
    "        print(f\"   Datasets agrégés: {len(aggregations)}\")\n",
    "\n",
    "        return aggregations\n",
    "\n",
    "    def generate_methodology_document(self) -> pd.DataFrame:\n",
    "        \"\"\"Génère la documentation méthodologique\"\"\"\n",
    "        self.logger.info(\"📝 Génération de la documentation méthodologique\")\n",
    "\n",
    "        methodology = []\n",
    "\n",
    "        # Anomalies\n",
    "        anomaly_report = self.anomaly_detector.generate_anomaly_report()\n",
    "        if not anomaly_report.empty:\n",
    "            for _, row in anomaly_report.iterrows():\n",
    "                methodology.append(\n",
    "                    {\n",
    "                        \"category\": \"Anomalie\",\n",
    "                        \"dataset\": row[\"dataset\"],\n",
    "                        \"variable\": row[\"variable\"],\n",
    "                        \"type\": row[\"anomaly_type\"],\n",
    "                        \"action\": f\"{row['count']} valeurs détectées ({row['percentage']}%)\",\n",
    "                        \"justification\": f\"Seuil: {self.config.ZSCORE_THRESHOLD if row['anomaly_type'] == 'zscore' else self.config.IQR_MULTIPLIER}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Indicateurs créés\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"population_growth_rate\",\n",
    "                \"type\": \"Dérivé\",\n",
    "                \"action\": \"Taux de croissance calculé\",\n",
    "                \"justification\": \"Variation annuelle en pourcentage\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"gdp_per_capita\",\n",
    "                \"type\": \"Ratio\",\n",
    "                \"action\": \"PIB / Population\",\n",
    "                \"justification\": \"Normalisation économique\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"regional_development_index\",\n",
    "                \"type\": \"Composite\",\n",
    "                \"action\": \"Indice multi-dimensionnel\",\n",
    "                \"justification\": \"Combinaison normalisée de plusieurs indicateurs\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Agrégations\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Agrégation\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"temporal_aggregation\",\n",
    "                \"type\": \"Temporelle\",\n",
    "                \"action\": \"Agrégation par année\",\n",
    "                \"justification\": \"Analyse des tendances historiques\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Agrégation\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"spatial_aggregation\",\n",
    "                \"type\": \"Spatiale\",\n",
    "                \"action\": \"Agrégation par région\",\n",
    "                \"justification\": \"Comparaisons géographiques\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology_df = pd.DataFrame(methodology)\n",
    "\n",
    "        # Sauvegarde\n",
    "        method_path = self.directories[\"analysis\"] / \"methodology_documentation.csv\"\n",
    "        methodology_df.to_csv(method_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"✅ Documentation: {method_path.name}\")\n",
    "\n",
    "        return methodology_df\n",
    "\n",
    "    def run_complete_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"Exécute le pipeline complet d'analyse\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🚀 DÉMARRAGE PIPELINE ANALYSE COMPLÈTE - TÂCHE 2\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Chargement des données\n",
    "        datasets = self.loader.load_all_datasets()\n",
    "\n",
    "        if not datasets:\n",
    "            self.logger.error(\n",
    "                \"❌ Aucune donnée chargée. Vérifiez le répertoire d'entrée.\"\n",
    "            )\n",
    "            return {}\n",
    "\n",
    "        # Phase 1: Analyse descriptive\n",
    "        descriptive_results = self.run_descriptive_analysis(datasets)\n",
    "\n",
    "        # Phase 2: Tendances temporelles\n",
    "        trend_results = self.run_trend_analysis(datasets)\n",
    "\n",
    "        # Phase 3: Dynamiques spatiales\n",
    "        spatial_results = self.run_spatial_analysis(datasets)\n",
    "\n",
    "        # Phase 4: Corrélations\n",
    "        correlation_results = self.run_correlation_analysis(datasets)\n",
    "\n",
    "        # Phase 5: Anomalies\n",
    "        anomaly_results = self.run_anomaly_detection(datasets)\n",
    "\n",
    "        # Phase 6: Indicateurs dérivés\n",
    "        enriched_datasets = self.run_indicator_creation(datasets)\n",
    "\n",
    "        # Phase 7: Agrégations\n",
    "        aggregation_results = self.run_aggregations(\n",
    "            enriched_datasets if enriched_datasets else datasets\n",
    "        )\n",
    "\n",
    "        # Documentation méthodologique\n",
    "        methodology_doc = self.generate_methodology_document()\n",
    "\n",
    "        # Rapport final\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"✅ PIPELINE TERMINÉ AVEC SUCCÈS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n📊 RÉSULTATS:\")\n",
    "        print(f\"   - Datasets analysés: {len(datasets)}\")\n",
    "        print(f\"   - Datasets enrichis: {len(enriched_datasets)}\")\n",
    "        print(f\"   - Tendances identifiées: {len(trend_results)}\")\n",
    "        print(f\"   - Analyses spatiales: {len(spatial_results)}\")\n",
    "        print(f\"   - Matrices de corrélation: {len(correlation_results)}\")\n",
    "        print(\n",
    "            f\"   - Anomalies détectées: {len(anomaly_results.get('anomaly_report', []))}\"\n",
    "        )\n",
    "        print(f\"\\n📂 LIVRABLES:\")\n",
    "        print(f\"   - Données enrichies: {self.directories['enriched']}\")\n",
    "        print(f\"   - Analyses: {self.directories['analysis']}\")\n",
    "        print(f\"   - Rapports d'anomalies: {self.directories['anomalies']}\")\n",
    "        print(f\"   - Visualisations: {self.directories['visualizations']}\")\n",
    "        print(f\"   - Données agrégées: {self.directories['processed']}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"datasets\": datasets,\n",
    "            \"descriptive_analysis\": descriptive_results,\n",
    "            \"trends\": trend_results,\n",
    "            \"spatial_analysis\": spatial_results,\n",
    "            \"correlations\": correlation_results,\n",
    "            \"anomalies\": anomaly_results,\n",
    "            \"enriched_datasets\": enriched_datasets,\n",
    "            \"aggregations\": aggregation_results,\n",
    "            \"methodology\": methodology_doc,\n",
    "        }\n",
    "\n"
   ],
   "id": "cd2296e51372ec44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    \"\"\"Point d'entrée principal\"\"\"\n",
    "\n",
    "    # Configuration de l'environnement\n",
    "    setup_analysis_environment(log_dir=Path(\"logs_task_2\"))\n",
    "\n",
    "    # Création de l'orchestrateur\n",
    "    orchestrator = ExplorationOrchestrator()\n",
    "\n",
    "    # Exécution du pipeline complet\n",
    "    results = orchestrator.run_complete_analysis()\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "id": "453a9b8865694138"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = main()\n"
   ],
   "id": "dc54e6209e1bcd2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
