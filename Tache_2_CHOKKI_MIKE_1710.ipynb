{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import functools\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Tuple, Any, Union\n",
    "from dataclasses import dataclass, field\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ],
   "id": "cd7e9bfa83aff5b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    \"\"\"Configuration pour l'analyse exploratoire\"\"\"\n",
    "\n",
    "    COUNTRY_CODE: str = \"BJ\"\n",
    "    COUNTRY_NAME: str = \"B√©nin\"\n",
    "\n",
    "    # Seuils pour la d√©tection d'anomalies\n",
    "    ZSCORE_THRESHOLD: float = 3.0\n",
    "    IQR_MULTIPLIER: float = 1.5\n",
    "    CORRELATION_THRESHOLD: float = 0.7\n",
    "\n",
    "    # Configuration des indicateurs\n",
    "    POPULATION_AGE_YOUNG: int = 15\n",
    "    POPULATION_AGE_OLD: int = 65\n",
    "    GDP_BASE_YEAR: int = 2015\n",
    "\n",
    "    # R√©pertoires\n",
    "    DIRECTORY_STRUCTURE: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"input\": \"data_task_1/final_data\",\n",
    "            \"output\": \"data_task_2\",\n",
    "            \"processed\": \"data_task_2/processed\",\n",
    "            \"enriched\": \"data_task_2/enriched\",\n",
    "            \"analysis\": \"data_task_2/analysis\",\n",
    "            \"anomalies\": \"data_task_2/anomalies\",\n",
    "            \"visualizations\": \"data_task_2/visualizations\",\n",
    "            \"logs\": \"logs_task_2\",\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "b15f3c48b3af863a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def setup_analysis_environment(log_dir: Optional[Path] = None) -> None:\n",
    "    \"\"\"Configure l'environnement d'analyse\"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    log_format = \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\"\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=log_format,\n",
    "        datefmt=date_format,\n",
    "        handlers=[console_handler],\n",
    "    )\n",
    "\n",
    "    if log_dir:\n",
    "        log_dir = Path(log_dir)\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        file_handler = RotatingFileHandler(\n",
    "            log_dir / \"analysis.log\",\n",
    "            maxBytes=5_000_000,\n",
    "            backupCount=3,\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        file_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "        logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 100)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"Set2\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"figure.figsize\": (14, 8),\n",
    "            \"axes.titlesize\": 14,\n",
    "            \"axes.labelsize\": 12,\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "727559e67c14f3c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DirectoryManager:\n",
    "    \"\"\"Gestionnaire de r√©pertoires pour l'analyse\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_dir: Optional[Path] = None, config: Optional[AnalysisConfig] = None\n",
    "    ):\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._directories: Dict[str, Path] = {}\n",
    "\n",
    "    def initialize_structure(self) -> Dict[str, Path]:\n",
    "        \"\"\"Cr√©e la structure de r√©pertoires\"\"\"\n",
    "        for name, path in self.config.DIRECTORY_STRUCTURE.items():\n",
    "            full_path = self.base_dir / path\n",
    "            full_path.mkdir(parents=True, exist_ok=True)\n",
    "            self._directories[name] = full_path\n",
    "\n",
    "        self.logger.info(f\"‚úÖ {len(self._directories)} r√©pertoires cr√©√©s\")\n",
    "        return self._directories\n",
    "\n",
    "    def get_path(self, name: str) -> Optional[Path]:\n",
    "        \"\"\"R√©cup√®re un chemin de r√©pertoire\"\"\"\n",
    "        return self._directories.get(name)\n",
    "\n"
   ],
   "id": "5a51cd7b34fc4262"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class AnomalyReport:\n",
    "    \"\"\"Rapport de d√©tection d'anomalies\"\"\"\n",
    "\n",
    "    dataset_name: str\n",
    "    variable: str\n",
    "    anomaly_type: str\n",
    "    anomaly_count: int\n",
    "    anomaly_percentage: float\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"dataset\": self.dataset_name,\n",
    "            \"variable\": self.variable,\n",
    "            \"anomaly_type\": self.anomaly_type,\n",
    "            \"count\": self.anomaly_count,\n",
    "            \"percentage\": round(self.anomaly_percentage, 2),\n",
    "            \"details\": str(self.details),\n",
    "            \"timestamp\": self.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n"
   ],
   "id": "3cd8b74e3e0b04ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Chargeur de donn√©es consolid√©es\"\"\"\n",
    "\n",
    "    def __init__(self, input_dir: Path):\n",
    "        self.input_dir = input_dir\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def load_all_datasets(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Charge tous les datasets disponibles\"\"\"\n",
    "        datasets = {}\n",
    "\n",
    "        if not self.input_dir.exists():\n",
    "            self.logger.error(f\"‚ùå R√©pertoire introuvable: {self.input_dir}\")\n",
    "            return datasets\n",
    "\n",
    "        csv_files = list(self.input_dir.glob(\"*.csv\"))\n",
    "\n",
    "        if not csv_files:\n",
    "            self.logger.warning(f\"‚ö†Ô∏è Aucun fichier CSV trouv√© dans {self.input_dir}\")\n",
    "            return datasets\n",
    "\n",
    "        self.logger.info(f\"üìÇ Chargement de {len(csv_files)} fichiers...\")\n",
    "\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                dataset_name = file_path.stem\n",
    "                df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    datasets[dataset_name] = df\n",
    "                    self.logger.info(\n",
    "                        f\"‚úÖ {dataset_name}: {len(df)} lignes, {len(df.columns)} colonnes\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.logger.warning(f\"‚ö†Ô∏è {dataset_name}: fichier vide\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    f\"‚ùå Erreur lors du chargement de {file_path.name}: {e}\"\n",
    "                )\n",
    "\n",
    "        self.logger.info(f\"üìä Total: {len(datasets)} datasets charg√©s\")\n",
    "        return datasets\n",
    "\n"
   ],
   "id": "c67280c0e2db2d86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DescriptiveAnalyzer:\n",
    "    \"\"\"Analyseur descriptif des donn√©es\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_dataset(self, df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyse descriptive compl√®te d'un dataset\"\"\"\n",
    "        self.logger.info(f\"üìä Analyse descriptive: {dataset_name}\")\n",
    "\n",
    "        analysis = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"shape\": df.shape,\n",
    "            \"memory_usage_mb\": round(df.memory_usage(deep=True).sum() / (1024**2), 2),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"dtypes\": df.dtypes.value_counts().to_dict(),\n",
    "        }\n",
    "\n",
    "        # Statistiques de compl√©tude\n",
    "        analysis[\"completeness\"] = {\n",
    "            \"total_cells\": df.size,\n",
    "            \"non_null_cells\": df.notna().sum().sum(),\n",
    "            \"null_cells\": df.isna().sum().sum(),\n",
    "            \"completeness_rate\": round(df.notna().sum().sum() / df.size * 100, 2),\n",
    "        }\n",
    "\n",
    "        # Statistiques par colonne\n",
    "        column_stats = []\n",
    "        for col in df.columns:\n",
    "            stat = {\n",
    "                \"column\": col,\n",
    "                \"dtype\": str(df[col].dtype),\n",
    "                \"non_null\": int(df[col].notna().sum()),\n",
    "                \"null\": int(df[col].isna().sum()),\n",
    "                \"null_pct\": round(df[col].isna().sum() / len(df) * 100, 2),\n",
    "                \"unique\": int(df[col].nunique()),\n",
    "            }\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                stat.update(\n",
    "                    {\n",
    "                        \"min\": df[col].min(),\n",
    "                        \"max\": df[col].max(),\n",
    "                        \"mean\": round(df[col].mean(), 3),\n",
    "                        \"median\": df[col].median(),\n",
    "                        \"std\": round(df[col].std(), 3),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            column_stats.append(stat)\n",
    "\n",
    "        analysis[\"column_statistics\"] = pd.DataFrame(column_stats)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def identify_temporal_variables(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Identifie les variables temporelles\"\"\"\n",
    "        temporal_vars = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if any(\n",
    "                keyword in col.lower() for keyword in [\"year\", \"ann√©e\", \"date\", \"time\"]\n",
    "            ):\n",
    "                temporal_vars.append(col)\n",
    "            elif df[col].dtype == \"datetime64[ns]\":\n",
    "                temporal_vars.append(col)\n",
    "\n",
    "        return temporal_vars\n",
    "\n",
    "    def identify_spatial_variables(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Identifie les variables spatiales\"\"\"\n",
    "        spatial_vars = []\n",
    "\n",
    "        spatial_keywords = [\n",
    "            \"r√©gion\",\n",
    "            \"region\",\n",
    "            \"d√©partement\",\n",
    "            \"department\",\n",
    "            \"commune\",\n",
    "            \"ville\",\n",
    "            \"city\",\n",
    "            \"localit√©\",\n",
    "            \"locality\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"admin\",\n",
    "        ]\n",
    "\n",
    "        for col in df.columns:\n",
    "            if any(keyword in col.lower() for keyword in spatial_keywords):\n",
    "                spatial_vars.append(col)\n",
    "\n",
    "        return spatial_vars\n",
    "\n",
    "    def generate_summary_report(\n",
    "        self, analyses: Dict[str, Dict[str, Any]]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"G√©n√®re un rapport de synth√®se\"\"\"\n",
    "        summary_data = []\n",
    "\n",
    "        for dataset_name, analysis in analyses.items():\n",
    "            summary_data.append(\n",
    "                {\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"rows\": analysis[\"shape\"][0],\n",
    "                    \"columns\": analysis[\"shape\"][1],\n",
    "                    \"memory_mb\": analysis[\"memory_usage_mb\"],\n",
    "                    \"completeness_rate\": analysis[\"completeness\"][\"completeness_rate\"],\n",
    "                    \"null_cells\": analysis[\"completeness\"][\"null_cells\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n"
   ],
   "id": "556568494c50ca7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TrendAnalyzer:\n",
    "    \"\"\"Analyseur de tendances temporelles\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_temporal_trends(\n",
    "        self, df: pd.DataFrame, time_col: str, value_cols: List[str]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Analyse les tendances temporelles\"\"\"\n",
    "        self.logger.info(f\"üìà Analyse des tendances temporelles\")\n",
    "\n",
    "        trends = {}\n",
    "\n",
    "        for value_col in value_cols:\n",
    "            if value_col not in df.columns or not pd.api.types.is_numeric_dtype(\n",
    "                df[value_col]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Agr√©gation par p√©riode temporelle\n",
    "                trend_df = (\n",
    "                    df.groupby(time_col)[value_col]\n",
    "                    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                # Calcul des variations\n",
    "                trend_df[\"pct_change\"] = trend_df[\"mean\"].pct_change() * 100\n",
    "                trend_df[\"cumulative_change\"] = (\n",
    "                    (trend_df[\"mean\"] / trend_df[\"mean\"].iloc[0]) - 1\n",
    "                ) * 100\n",
    "\n",
    "                trends[value_col] = trend_df\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"‚ö†Ô∏è Erreur tendance {value_col}: {e}\")\n",
    "\n",
    "        return trends\n",
    "\n",
    "    def calculate_growth_rates(\n",
    "        self, df: pd.DataFrame, time_col: str, value_col: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Calcule les taux de croissance\"\"\"\n",
    "        result = df.copy()\n",
    "        result = result.sort_values(time_col)\n",
    "\n",
    "        # Taux de croissance annuel\n",
    "        result[\"growth_rate\"] = result[value_col].pct_change() * 100\n",
    "\n",
    "        # Taux de croissance annuel moyen (CAGR)\n",
    "        if len(result) > 1:\n",
    "            first_value = result[value_col].iloc[0]\n",
    "            last_value = result[value_col].iloc[-1]\n",
    "            n_periods = len(result) - 1\n",
    "\n",
    "            if first_value > 0 and last_value > 0:\n",
    "                cagr = (((last_value / first_value) ** (1 / n_periods)) - 1) * 100\n",
    "                result[\"cagr\"] = cagr\n",
    "\n",
    "        return result\n",
    "\n"
   ],
   "id": "b69e1f2eed384dd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SpatialAnalyzer:\n",
    "    \"\"\"Analyseur de dynamiques spatiales\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_spatial_distribution(\n",
    "        self, df: pd.DataFrame, spatial_col: str, value_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Analyse la distribution spatiale\"\"\"\n",
    "        self.logger.info(f\"üó∫Ô∏è Analyse spatiale sur {spatial_col}\")\n",
    "\n",
    "        spatial_stats = []\n",
    "\n",
    "        for value_col in value_cols:\n",
    "            if value_col not in df.columns or not pd.api.types.is_numeric_dtype(\n",
    "                df[value_col]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stats_by_location = (\n",
    "                    df.groupby(spatial_col)[value_col]\n",
    "                    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\", \"sum\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                stats_by_location[\"variable\"] = value_col\n",
    "                stats_by_location[\"cv\"] = (\n",
    "                    stats_by_location[\"std\"] / stats_by_location[\"mean\"]\n",
    "                ) * 100\n",
    "\n",
    "                spatial_stats.append(stats_by_location)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"‚ö†Ô∏è Erreur spatiale {value_col}: {e}\")\n",
    "\n",
    "        if spatial_stats:\n",
    "            return pd.concat(spatial_stats, ignore_index=True)\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def calculate_regional_disparities(\n",
    "        self, df: pd.DataFrame, spatial_col: str, value_col: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calcule les disparit√©s r√©gionales\"\"\"\n",
    "        regional_means = df.groupby(spatial_col)[value_col].mean()\n",
    "\n",
    "        disparities = {\n",
    "            \"gini_coefficient\": self._calculate_gini(regional_means.values),\n",
    "            \"coefficient_variation\": (regional_means.std() / regional_means.mean())\n",
    "            * 100,\n",
    "            \"range_ratio\": (\n",
    "                regional_means.max() / regional_means.min()\n",
    "                if regional_means.min() > 0\n",
    "                else np.nan\n",
    "            ),\n",
    "            \"max_value\": regional_means.max(),\n",
    "            \"min_value\": regional_means.min(),\n",
    "            \"mean_value\": regional_means.mean(),\n",
    "        }\n",
    "\n",
    "        return disparities\n",
    "\n",
    "    def _calculate_gini(self, values: np.ndarray) -> float:\n",
    "        \"\"\"Calcule le coefficient de Gini\"\"\"\n",
    "        values = np.array(values)\n",
    "        values = values[~np.isnan(values)]\n",
    "\n",
    "        if len(values) == 0:\n",
    "            return np.nan\n",
    "\n",
    "        values = np.sort(values)\n",
    "        n = len(values)\n",
    "        index = np.arange(1, n + 1)\n",
    "\n",
    "        return (2 * np.sum(index * values)) / (n * np.sum(values)) - (n + 1) / n\n",
    "\n"
   ],
   "id": "68eb14222d66865e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CorrelationAnalyzer:\n",
    "    \"\"\"Analyseur de corr√©lations\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def calculate_correlations(\n",
    "        self, df: pd.DataFrame, method: str = \"pearson\"\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Calcule la matrice de corr√©lation\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        if len(numeric_cols) < 2:\n",
    "            self.logger.warning(\n",
    "                \"‚ö†Ô∏è Pas assez de colonnes num√©riques pour la corr√©lation\"\n",
    "            )\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if method == \"pearson\":\n",
    "            corr_matrix = df[numeric_cols].corr(method=\"pearson\")\n",
    "        elif method == \"spearman\":\n",
    "            corr_matrix = df[numeric_cols].corr(method=\"spearman\")\n",
    "        else:\n",
    "            corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "        return corr_matrix\n",
    "\n",
    "    def find_strong_correlations(\n",
    "        self, corr_matrix: pd.DataFrame, threshold: Optional[float] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Identifie les corr√©lations fortes\"\"\"\n",
    "        threshold = threshold or self.config.CORRELATION_THRESHOLD\n",
    "\n",
    "        strong_corr = []\n",
    "\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                var1 = corr_matrix.columns[i]\n",
    "                var2 = corr_matrix.columns[j]\n",
    "                corr_value = corr_matrix.iloc[i, j]\n",
    "\n",
    "                if abs(corr_value) >= threshold:\n",
    "                    strong_corr.append(\n",
    "                        {\n",
    "                            \"variable_1\": var1,\n",
    "                            \"variable_2\": var2,\n",
    "                            \"correlation\": round(corr_value, 3),\n",
    "                            \"strength\": (\n",
    "                                \"forte\" if abs(corr_value) >= 0.8 else \"mod√©r√©e\"\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if not strong_corr:\n",
    "            return pd.DataFrame(\n",
    "                columns=[\"variable_1\", \"variable_2\", \"correlation\", \"strength\"]\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(strong_corr).sort_values(\n",
    "            \"correlation\", key=abs, ascending=False\n",
    "        )\n",
    "\n",
    "    def cross_dataset_correlation(\n",
    "        self, df1: pd.DataFrame, df2: pd.DataFrame, merge_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Corr√©lations crois√©es entre datasets\"\"\"\n",
    "        try:\n",
    "            merged = pd.merge(\n",
    "                df1, df2, on=merge_cols, how=\"inner\", suffixes=(\"_1\", \"_2\")\n",
    "            )\n",
    "\n",
    "            numeric_cols = merged.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "            if len(numeric_cols) >= 2:\n",
    "                return self.calculate_correlations(merged[numeric_cols])\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Erreur corr√©lation crois√©e: {e}\")\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n"
   ],
   "id": "a7ac0578378691dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AnomalyDetector:\n",
    "    \"\"\"D√©tecteur d'anomalies\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.anomaly_reports: List[AnomalyReport] = []\n",
    "\n",
    "    def detect_zscore_anomalies(\n",
    "        self, df: pd.DataFrame, dataset_name: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"D√©tecte les anomalies par Z-score\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        anomalies_df = pd.DataFrame()\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 3:\n",
    "                continue\n",
    "\n",
    "            z_scores = np.abs(zscore(df[col].dropna()))\n",
    "            anomaly_mask = z_scores > self.config.ZSCORE_THRESHOLD\n",
    "\n",
    "            if anomaly_mask.any():\n",
    "                anomaly_count = anomaly_mask.sum()\n",
    "                anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "\n",
    "                report = AnomalyReport(\n",
    "                    dataset_name=dataset_name,\n",
    "                    variable=col,\n",
    "                    anomaly_type=\"zscore\",\n",
    "                    anomaly_count=anomaly_count,\n",
    "                    anomaly_percentage=anomaly_pct,\n",
    "                    details={\"threshold\": self.config.ZSCORE_THRESHOLD},\n",
    "                )\n",
    "                self.anomaly_reports.append(report)\n",
    "\n",
    "        return anomalies_df\n",
    "\n",
    "    def detect_iqr_anomalies(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "        \"\"\"D√©tecte les anomalies par IQR\"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        anomalies_list = []\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 4:\n",
    "                continue\n",
    "\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            lower_bound = Q1 - self.config.IQR_MULTIPLIER * IQR\n",
    "            upper_bound = Q3 + self.config.IQR_MULTIPLIER * IQR\n",
    "\n",
    "            anomaly_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "\n",
    "            if anomaly_mask.any():\n",
    "                anomaly_count = anomaly_mask.sum()\n",
    "                anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "\n",
    "                report = AnomalyReport(\n",
    "                    dataset_name=dataset_name,\n",
    "                    variable=col,\n",
    "                    anomaly_type=\"iqr\",\n",
    "                    anomaly_count=anomaly_count,\n",
    "                    anomaly_percentage=anomaly_pct,\n",
    "                    details={\n",
    "                        \"lower_bound\": lower_bound,\n",
    "                        \"upper_bound\": upper_bound,\n",
    "                        \"Q1\": Q1,\n",
    "                        \"Q3\": Q3,\n",
    "                        \"IQR\": IQR,\n",
    "                    },\n",
    "                )\n",
    "                self.anomaly_reports.append(report)\n",
    "\n",
    "                anomalies_list.append(\n",
    "                    {\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"variable\": col,\n",
    "                        \"anomaly_indices\": df[anomaly_mask].index.tolist(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return pd.DataFrame(anomalies_list) if anomalies_list else pd.DataFrame()\n",
    "\n",
    "    def detect_inconsistencies(\n",
    "        self, df: pd.DataFrame, dataset_name: str\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"D√©tecte les incoh√©rences\"\"\"\n",
    "        inconsistencies = []\n",
    "\n",
    "        # V√©rification des valeurs n√©gatives inappropri√©es\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            if any(\n",
    "                keyword in col.lower()\n",
    "                for keyword in [\"population\", \"count\", \"nombre\", \"effectif\"]\n",
    "            ):\n",
    "                negative_count = (df[col] < 0).sum()\n",
    "                if negative_count > 0:\n",
    "                    inconsistencies.append(\n",
    "                        {\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variable\": col,\n",
    "                            \"issue\": \"valeurs_n√©gatives\",\n",
    "                            \"count\": negative_count,\n",
    "                            \"description\": f\"{negative_count} valeurs n√©gatives pour une variable de comptage\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # V√©rification des ann√©es invalides\n",
    "        for col in df.columns:\n",
    "            if \"year\" in col.lower() or \"ann√©e\" in col.lower():\n",
    "                invalid_years = df[(df[col] < 1900) | (df[col] > 2025)]\n",
    "                if len(invalid_years) > 0:\n",
    "                    inconsistencies.append(\n",
    "                        {\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variable\": col,\n",
    "                            \"issue\": \"ann√©es_invalides\",\n",
    "                            \"count\": len(invalid_years),\n",
    "                            \"description\": f\"{len(invalid_years)} ann√©es hors de la plage [1900-2025]\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return inconsistencies\n",
    "\n",
    "    def generate_anomaly_report(self) -> pd.DataFrame:\n",
    "        \"\"\"G√©n√®re le rapport d'anomalies\"\"\"\n",
    "        if not self.anomaly_reports:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return pd.DataFrame([report.to_dict() for report in self.anomaly_reports])\n",
    "\n"
   ],
   "id": "5d44a1e3489e05ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class IndicatorBuilder:\n",
    "    \"\"\"Constructeur d'indicateurs d√©riv√©s\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def build_demographic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Construit les indicateurs d√©mographiques\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        # Taux de croissance de la population\n",
    "        if \"population\" in df.columns or \"total_population\" in df.columns:\n",
    "            pop_col = \"population\" if \"population\" in df.columns else \"total_population\"\n",
    "\n",
    "            if \"year\" in df.columns or \"ann√©e\" in df.columns:\n",
    "                time_col = \"year\" if \"year\" in df.columns else \"ann√©e\"\n",
    "                result = result.sort_values(time_col)\n",
    "                result[\"population_growth_rate\"] = result[pop_col].pct_change() * 100\n",
    "\n",
    "        # Ratio de population jeune\n",
    "        if \"population_0_14\" in df.columns and \"total_population\" in df.columns:\n",
    "            result[\"youth_population_ratio\"] = (\n",
    "                result[\"population_0_14\"] / result[\"total_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        # Ratio de population √¢g√©e\n",
    "        if \"population_65_plus\" in df.columns and \"total_population\" in df.columns:\n",
    "            result[\"elderly_population_ratio\"] = (\n",
    "                result[\"population_65_plus\"] / result[\"total_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        # Densit√© de population (si superficie disponible)\n",
    "        if all(col in df.columns for col in [\"total_population\", \"surface_area_km2\"]):\n",
    "            result[\"population_density\"] = (\n",
    "                result[\"total_population\"] / result[\"surface_area_km2\"]\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"‚úÖ Indicateurs d√©mographiques cr√©√©s\")\n",
    "        return result\n",
    "\n",
    "    def build_economic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Construit les indicateurs √©conomiques\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        # PIB par habitant\n",
    "        if all(col in df.columns for col in [\"gdp\", \"total_population\"]):\n",
    "            result[\"gdp_per_capita\"] = result[\"gdp\"] / result[\"total_population\"]\n",
    "\n",
    "        # Taux de croissance du PIB\n",
    "        if \"gdp\" in df.columns:\n",
    "            if \"year\" in df.columns or \"ann√©e\" in df.columns:\n",
    "                time_col = \"year\" if \"year\" in df.columns else \"ann√©e\"\n",
    "                result = result.sort_values(time_col)\n",
    "                result[\"gdp_growth_rate\"] = result[\"gdp\"].pct_change() * 100\n",
    "\n",
    "        # Indice de PIB (base 100)\n",
    "        if \"gdp\" in df.columns and \"year\" in df.columns:\n",
    "            base_year = self.config.GDP_BASE_YEAR\n",
    "            base_gdp = (\n",
    "                result[result[\"year\"] == base_year][\"gdp\"].iloc[0]\n",
    "                if base_year in result[\"year\"].values\n",
    "                else result[\"gdp\"].iloc[0]\n",
    "            )\n",
    "            result[\"gdp_index\"] = (result[\"gdp\"] / base_gdp) * 100\n",
    "\n",
    "        self.logger.info(\"‚úÖ Indicateurs √©conomiques cr√©√©s\")\n",
    "        return result\n",
    "\n",
    "    def build_education_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Construit les indicateurs d'√©ducation\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        # Taux de scolarisation net\n",
    "        if all(\n",
    "            col in df.columns for col in [\"enrolled_students\", \"school_age_population\"]\n",
    "        ):\n",
    "            result[\"net_enrollment_rate\"] = (\n",
    "                result[\"enrolled_students\"] / result[\"school_age_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        # Ratio √©l√®ves-enseignants\n",
    "        if all(col in df.columns for col in [\"total_students\", \"total_teachers\"]):\n",
    "            result[\"student_teacher_ratio\"] = (\n",
    "                result[\"total_students\"] / result[\"total_teachers\"]\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"‚úÖ Indicateurs d'√©ducation cr√©√©s\")\n",
    "        return result\n",
    "\n",
    "    def build_composite_index(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        indicators: List[str],\n",
    "        weights: Optional[List[float]] = None,\n",
    "        index_name: str = \"composite_index\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Construit un indice composite\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        available_indicators = [ind for ind in indicators if ind in df.columns]\n",
    "\n",
    "        if not available_indicators:\n",
    "            self.logger.warning(f\"‚ö†Ô∏è Aucun indicateur disponible pour {index_name}\")\n",
    "            return result\n",
    "\n",
    "        # Normalisation des indicateurs (min-max)\n",
    "        scaler = StandardScaler()\n",
    "        normalized_data = scaler.fit_transform(df[available_indicators].fillna(0))\n",
    "        normalized_df = pd.DataFrame(\n",
    "            normalized_data, columns=available_indicators, index=df.index\n",
    "        )\n",
    "\n",
    "        # Application des poids\n",
    "        if weights is None:\n",
    "            weights = [1 / len(available_indicators)] * len(available_indicators)\n",
    "\n",
    "        # Calcul de l'indice composite\n",
    "        result[index_name] = sum(\n",
    "            normalized_df[ind] * weight\n",
    "            for ind, weight in zip(available_indicators, weights)\n",
    "        )\n",
    "\n",
    "        # Normalisation finale (0-100)\n",
    "        result[index_name] = (\n",
    "            (result[index_name] - result[index_name].min())\n",
    "            / (result[index_name].max() - result[index_name].min())\n",
    "        ) * 100\n",
    "\n",
    "        self.logger.info(f\"‚úÖ Indice composite '{index_name}' cr√©√©\")\n",
    "        return result\n",
    "\n",
    "    def build_regional_development_index(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Construit un indice de d√©veloppement r√©gional\"\"\"\n",
    "        indicators = []\n",
    "\n",
    "        # S√©lection automatique des indicateurs disponibles\n",
    "        if \"gdp_per_capita\" in df.columns:\n",
    "            indicators.append(\"gdp_per_capita\")\n",
    "        if \"net_enrollment_rate\" in df.columns:\n",
    "            indicators.append(\"net_enrollment_rate\")\n",
    "        if \"life_expectancy\" in df.columns:\n",
    "            indicators.append(\"life_expectancy\")\n",
    "        if \"access_electricity\" in df.columns:\n",
    "            indicators.append(\"access_electricity\")\n",
    "\n",
    "        if indicators:\n",
    "            return self.build_composite_index(\n",
    "                df, indicators, index_name=\"regional_development_index\"\n",
    "            )\n",
    "\n",
    "        self.logger.warning(\"‚ö†Ô∏è Pas assez d'indicateurs pour l'indice de d√©veloppement\")\n",
    "        return df\n",
    "\n"
   ],
   "id": "ade1dba5c1b5f6f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AggregationEngine:\n",
    "    \"\"\"Moteur d'agr√©gation temporelle et spatiale\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def temporal_aggregation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        time_col: str,\n",
    "        value_cols: List[str],\n",
    "        agg_functions: Optional[Dict[str, str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if agg_functions is None:\n",
    "            agg_functions = {col: \"mean\" for col in value_cols}\n",
    "\n",
    "        valid_agg = {\n",
    "            col: func\n",
    "            for col, func in agg_functions.items()\n",
    "            if col in df.columns and col != time_col\n",
    "        }\n",
    "\n",
    "        if not valid_agg:\n",
    "            self.logger.warning(\"‚ö†Ô∏è Aucune colonne valide pour l'agr√©gation\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        aggregated = df.groupby(time_col).agg(valid_agg).reset_index()\n",
    "\n",
    "        aggregated.columns = [\n",
    "            f\"{col}_{func}\" if col != time_col else col\n",
    "            for col, func in zip(\n",
    "                aggregated.columns, [time_col] + list(valid_agg.values())\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.logger.info(f\"‚úÖ Agr√©gation temporelle: {len(aggregated)} p√©riodes\")\n",
    "        return aggregated\n",
    "\n",
    "    def spatial_aggregation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        spatial_col: str,\n",
    "        value_cols: List[str],\n",
    "        agg_functions: Optional[Dict[str, str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Agr√©gation spatiale\"\"\"\n",
    "        if agg_functions is None:\n",
    "            agg_functions = {col: \"sum\" for col in value_cols}\n",
    "\n",
    "        valid_agg = {\n",
    "            col: func\n",
    "            for col, func in agg_functions.items()\n",
    "            if col in df.columns and col != spatial_col\n",
    "        }\n",
    "\n",
    "        if not valid_agg:\n",
    "            self.logger.warning(\"‚ö†Ô∏è Aucune colonne valide pour l'agr√©gation spatiale\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        aggregated = df.groupby(spatial_col).agg(valid_agg).reset_index()\n",
    "\n",
    "        self.logger.info(f\"‚úÖ Agr√©gation spatiale: {len(aggregated)} zones\")\n",
    "        return aggregated\n",
    "\n",
    "    def normalize_by_population(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        value_cols: List[str],\n",
    "        population_col: str = \"total_population\",\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Normalise les valeurs par habitant\"\"\"\n",
    "        result = df.copy()\n",
    "\n",
    "        if population_col not in df.columns:\n",
    "            self.logger.warning(f\"‚ö†Ô∏è Colonne {population_col} introuvable\")\n",
    "            return result\n",
    "\n",
    "        for col in value_cols:\n",
    "            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                new_col_name = f\"{col}_per_capita\"\n",
    "                result[new_col_name] = result[col] / result[population_col]\n",
    "                self.logger.info(f\"‚úÖ Cr√©√©: {new_col_name}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def multi_level_aggregation(\n",
    "        self, df: pd.DataFrame, group_cols: List[str], value_cols: List[str]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Agr√©gation multi-niveaux\"\"\"\n",
    "        aggregations = {}\n",
    "\n",
    "        for i in range(1, len(group_cols) + 1):\n",
    "            level_cols = group_cols[:i]\n",
    "            level_name = \"_\".join(level_cols)\n",
    "\n",
    "            agg_dict = {\n",
    "                col: [\"sum\", \"mean\", \"count\"] for col in value_cols if col in df.columns\n",
    "            }\n",
    "\n",
    "            if agg_dict:\n",
    "                agg_result = df.groupby(level_cols).agg(agg_dict).reset_index()\n",
    "                agg_result.columns = [\n",
    "                    \"_\".join(col).strip(\"_\") for col in agg_result.columns\n",
    "                ]\n",
    "                aggregations[level_name] = agg_result\n",
    "                self.logger.info(f\"‚úÖ Agr√©gation niveau {i}: {len(agg_result)} groupes\")\n",
    "\n",
    "        return aggregations\n",
    "\n"
   ],
   "id": "80fbe76e080a2f69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class VisualizationEngine:\n",
    "    \"\"\"Moteur de visualisation\"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def plot_temporal_trends(\n",
    "        self, trends: Dict[str, pd.DataFrame], time_col: str, save: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"Visualise les tendances temporelles\"\"\"\n",
    "        n_plots = len(trends)\n",
    "        if n_plots == 0:\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(min(n_plots, 3), 1, figsize=(14, 4 * min(n_plots, 3)))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for idx, (var_name, trend_df) in enumerate(list(trends.items())[:3]):\n",
    "            ax = axes[idx]\n",
    "            ax.plot(\n",
    "                trend_df[time_col],\n",
    "                trend_df[\"mean\"],\n",
    "                marker=\"o\",\n",
    "                linewidth=2,\n",
    "                label=\"Moyenne\",\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                trend_df[time_col],\n",
    "                trend_df[\"mean\"] - trend_df[\"std\"],\n",
    "                trend_df[\"mean\"] + trend_df[\"std\"],\n",
    "                alpha=0.3,\n",
    "                label=\"¬±1 √©cart-type\",\n",
    "            )\n",
    "            ax.set_title(\n",
    "                f\"√âvolution temporelle: {var_name}\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Ann√©e\")\n",
    "            ax.set_ylabel(\"Valeur\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"temporal_trends.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"üíæ Graphique sauvegard√©: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_correlation_heatmap(\n",
    "        self, corr_matrix: pd.DataFrame, save: bool = True\n",
    "    ) -> None:\n",
    "        \"\"\"Visualise la matrice de corr√©lation\"\"\"\n",
    "        if corr_matrix.empty:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "        sns.heatmap(\n",
    "            corr_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        ax.set_title(\"Matrice de corr√©lation\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"correlation_heatmap.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"üíæ Heatmap sauvegard√©e: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_spatial_distribution(\n",
    "        self,\n",
    "        spatial_stats: pd.DataFrame,\n",
    "        spatial_col: str,\n",
    "        value_col: str = \"mean\",\n",
    "        save: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Visualise la distribution spatiale\"\"\"\n",
    "        if spatial_stats.empty:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        spatial_stats_sorted = spatial_stats.sort_values(\n",
    "            value_col, ascending=False\n",
    "        ).head(20)\n",
    "\n",
    "        bars = ax.barh(\n",
    "            spatial_stats_sorted[spatial_col], spatial_stats_sorted[value_col]\n",
    "        )\n",
    "\n",
    "        # Gradient de couleurs\n",
    "        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(bars)))\n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "\n",
    "        ax.set_xlabel(\"Valeur moyenne\")\n",
    "        ax.set_title(f\"Distribution spatiale (Top 20)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"spatial_distribution.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"üíæ Distribution sauvegard√©e: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n"
   ],
   "id": "e7c8abd9018f3977"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ExplorationOrchestrator:\n",
    "    \"\"\"Orchestrateur de l'exploration et analyse\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config: Optional[AnalysisConfig] = None, base_dir: Optional[Path] = None\n",
    "    ):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialisation des composants\n",
    "        self.dir_manager = DirectoryManager(self.base_dir, self.config)\n",
    "        self.directories = self.dir_manager.initialize_structure()\n",
    "\n",
    "        self.loader = DataLoader(self.directories[\"input\"])\n",
    "        self.descriptive_analyzer = DescriptiveAnalyzer(self.config)\n",
    "        self.trend_analyzer = TrendAnalyzer(self.config)\n",
    "        self.spatial_analyzer = SpatialAnalyzer(self.config)\n",
    "        self.correlation_analyzer = CorrelationAnalyzer(self.config)\n",
    "        self.anomaly_detector = AnomalyDetector(self.config)\n",
    "        self.indicator_builder = IndicatorBuilder(self.config)\n",
    "        self.aggregation_engine = AggregationEngine(self.config)\n",
    "        self.viz_engine = VisualizationEngine(self.directories[\"visualizations\"])\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "    def run_descriptive_analysis(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 1: Analyse descriptive\"\"\"\n",
    "        self.logger.info(\"üìä PHASE 1: ANALYSE DESCRIPTIVE\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä PHASE 1: ANALYSE DESCRIPTIVE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        analyses = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            self.logger.info(f\"‚ñ∂Ô∏è Analyse: {name}\")\n",
    "            analysis = self.descriptive_analyzer.analyze_dataset(df, name)\n",
    "            analyses[name] = analysis\n",
    "\n",
    "            # Sauvegarde des statistiques\n",
    "            if \"column_statistics\" in analysis:\n",
    "                stats_path = self.directories[\"analysis\"] / f\"{name}_column_stats.csv\"\n",
    "                analysis[\"column_statistics\"].to_csv(stats_path, index=False)\n",
    "\n",
    "        # Rapport de synth√®se\n",
    "        summary_report = self.descriptive_analyzer.generate_summary_report(analyses)\n",
    "        summary_path = self.directories[\"analysis\"] / \"descriptive_summary.csv\"\n",
    "        summary_report.to_csv(summary_path, index=False)\n",
    "\n",
    "        print(\"\\n‚úÖ Analyse descriptive termin√©e\")\n",
    "        print(f\"   Datasets analys√©s: {len(analyses)}\")\n",
    "        print(f\"   Rapport: {summary_path.name}\")\n",
    "\n",
    "        return analyses\n",
    "\n",
    "    def run_trend_analysis(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 2: Analyse des tendances\"\"\"\n",
    "        self.logger.info(\"üìà PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìà PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        all_trends = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            temporal_vars = self.descriptive_analyzer.identify_temporal_variables(df)\n",
    "\n",
    "            if not temporal_vars:\n",
    "                continue\n",
    "\n",
    "            time_col = temporal_vars[0]\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            value_cols = [col for col in numeric_cols if col != time_col][:5]\n",
    "\n",
    "            if value_cols:\n",
    "                trends = self.trend_analyzer.analyze_temporal_trends(\n",
    "                    df, time_col, value_cols\n",
    "                )\n",
    "\n",
    "                if trends:\n",
    "                    all_trends[name] = trends\n",
    "\n",
    "                    # Sauvegarde\n",
    "                    for var, trend_df in trends.items():\n",
    "                        trend_path = (\n",
    "                            self.directories[\"analysis\"] / f\"{name}_{var}_trend.csv\"\n",
    "                        )\n",
    "                        trend_df.to_csv(trend_path, index=False)\n",
    "\n",
    "                    # Visualisation\n",
    "                    self.viz_engine.plot_temporal_trends(trends, time_col)\n",
    "\n",
    "        print(f\"\\n‚úÖ Analyse des tendances termin√©e\")\n",
    "        print(f\"   Datasets avec tendances: {len(all_trends)}\")\n",
    "\n",
    "        return all_trends\n",
    "\n",
    "    def run_spatial_analysis(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 3: Analyse spatiale\"\"\"\n",
    "        self.logger.info(\"üó∫Ô∏è PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üó∫Ô∏è PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        spatial_results = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            spatial_vars = self.descriptive_analyzer.identify_spatial_variables(df)\n",
    "\n",
    "            if not spatial_vars:\n",
    "                continue\n",
    "\n",
    "            spatial_col = spatial_vars[0]\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            value_cols = [col for col in numeric_cols if col != spatial_col][:5]\n",
    "\n",
    "            if value_cols:\n",
    "                spatial_dist = self.spatial_analyzer.analyze_spatial_distribution(\n",
    "                    df, spatial_col, value_cols\n",
    "                )\n",
    "\n",
    "                if not spatial_dist.empty:\n",
    "                    spatial_results[name] = spatial_dist\n",
    "\n",
    "                    # Sauvegarde\n",
    "                    spatial_path = (\n",
    "                        self.directories[\"analysis\"] / f\"{name}_spatial_analysis.csv\"\n",
    "                    )\n",
    "                    spatial_dist.to_csv(spatial_path, index=False)\n",
    "\n",
    "                    # Visualisation\n",
    "                    self.viz_engine.plot_spatial_distribution(spatial_dist, spatial_col)\n",
    "\n",
    "        print(f\"\\n‚úÖ Analyse spatiale termin√©e\")\n",
    "        print(f\"   Datasets analys√©s spatialement: {len(spatial_results)}\")\n",
    "\n",
    "        return spatial_results\n",
    "\n",
    "    def run_correlation_analysis(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 4: Analyse des corr√©lations\"\"\"\n",
    "        self.logger.info(\"üîó PHASE 4: ANALYSE DES CORR√âLATIONS\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîó PHASE 4: ANALYSE DES CORR√âLATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        correlations = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            corr_matrix = self.correlation_analyzer.calculate_correlations(df)\n",
    "\n",
    "            if not corr_matrix.empty:\n",
    "                correlations[name] = {\n",
    "                    \"matrix\": corr_matrix,\n",
    "                    \"strong_correlations\": self.correlation_analyzer.find_strong_correlations(\n",
    "                        corr_matrix\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                # Sauvegarde\n",
    "                corr_path = self.directories[\"analysis\"] / f\"{name}_correlations.csv\"\n",
    "                corr_matrix.to_csv(corr_path)\n",
    "\n",
    "                strong_corr_path = (\n",
    "                    self.directories[\"analysis\"] / f\"{name}_strong_correlations.csv\"\n",
    "                )\n",
    "                if not correlations[name][\"strong_correlations\"].empty:\n",
    "                    correlations[name][\"strong_correlations\"].to_csv(\n",
    "                        strong_corr_path, index=False\n",
    "                    )\n",
    "\n",
    "                # Visualisation\n",
    "                self.viz_engine.plot_correlation_heatmap(corr_matrix)\n",
    "\n",
    "        print(f\"\\n‚úÖ Analyse des corr√©lations termin√©e\")\n",
    "        print(f\"   Datasets analys√©s: {len(correlations)}\")\n",
    "\n",
    "        return correlations\n",
    "\n",
    "    def run_anomaly_detection(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 5: D√©tection des anomalies\"\"\"\n",
    "        self.logger.info(\"üîç PHASE 5: D√âTECTION DES ANOMALIES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîç PHASE 5: D√âTECTION DES ANOMALIES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        all_anomalies = {}\n",
    "        all_inconsistencies = []\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            # D√©tection Z-score\n",
    "            self.anomaly_detector.detect_zscore_anomalies(df, name)\n",
    "\n",
    "            # D√©tection IQR\n",
    "            iqr_anomalies = self.anomaly_detector.detect_iqr_anomalies(df, name)\n",
    "\n",
    "            # D√©tection d'incoh√©rences\n",
    "            inconsistencies = self.anomaly_detector.detect_inconsistencies(df, name)\n",
    "\n",
    "            if not iqr_anomalies.empty:\n",
    "                all_anomalies[name] = iqr_anomalies\n",
    "\n",
    "            if inconsistencies:\n",
    "                all_inconsistencies.extend(inconsistencies)\n",
    "\n",
    "        # Rapport global\n",
    "        anomaly_report = self.anomaly_detector.generate_anomaly_report()\n",
    "        if not anomaly_report.empty:\n",
    "            anomaly_path = self.directories[\"anomalies\"] / \"anomaly_report.csv\"\n",
    "            anomaly_report.to_csv(anomaly_path, index=False)\n",
    "            print(f\"\\nüìÑ Rapport d'anomalies: {anomaly_path.name}\")\n",
    "            print(f\"   Total anomalies d√©tect√©es: {len(anomaly_report)}\")\n",
    "\n",
    "        # Rapport d'incoh√©rences\n",
    "        if all_inconsistencies:\n",
    "            incon_df = pd.DataFrame(all_inconsistencies)\n",
    "            incon_path = self.directories[\"anomalies\"] / \"inconsistencies_report.csv\"\n",
    "            incon_df.to_csv(incon_path, index=False)\n",
    "            print(f\"üìÑ Rapport d'incoh√©rences: {incon_path.name}\")\n",
    "            print(f\"   Total incoh√©rences: {len(all_inconsistencies)}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ D√©tection des anomalies termin√©e\")\n",
    "\n",
    "        return {\n",
    "            \"anomalies\": all_anomalies,\n",
    "            \"anomaly_report\": anomaly_report,\n",
    "            \"inconsistencies\": all_inconsistencies,\n",
    "        }\n",
    "\n",
    "    def run_indicator_creation(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Phase 6: Cr√©ation d'indicateurs\"\"\"\n",
    "        self.logger.info(\"üîß PHASE 6: CR√âATION D'INDICATEURS D√âRIV√âS\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîß PHASE 6: CR√âATION D'INDICATEURS D√âRIV√âS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        enriched_datasets = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            enriched = df.copy()\n",
    "\n",
    "            # Indicateurs d√©mographiques\n",
    "            enriched = self.indicator_builder.build_demographic_indicators(enriched)\n",
    "\n",
    "            # Indicateurs √©conomiques\n",
    "            enriched = self.indicator_builder.build_economic_indicators(enriched)\n",
    "\n",
    "            # Indicateurs d'√©ducation\n",
    "            enriched = self.indicator_builder.build_education_indicators(enriched)\n",
    "\n",
    "            # Indice de d√©veloppement r√©gional\n",
    "            enriched = self.indicator_builder.build_regional_development_index(enriched)\n",
    "\n",
    "            # Comptage des nouvelles colonnes\n",
    "            new_cols = set(enriched.columns) - set(df.columns)\n",
    "\n",
    "            if new_cols:\n",
    "                enriched_datasets[name] = enriched\n",
    "\n",
    "                # Sauvegarde\n",
    "                enriched_path = self.directories[\"enriched\"] / f\"{name}_enriched.csv\"\n",
    "                enriched.to_csv(enriched_path, index=False)\n",
    "\n",
    "                print(f\"\\n‚úÖ {name}: {len(new_cols)} nouveaux indicateurs\")\n",
    "                for col in sorted(new_cols):\n",
    "                    print(f\"   - {col}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Cr√©ation d'indicateurs termin√©e\")\n",
    "        print(f\"   Datasets enrichis: {len(enriched_datasets)}\")\n",
    "\n",
    "        return enriched_datasets\n",
    "\n",
    "    def run_aggregations(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        \"\"\"Phase 7: Agr√©gations\"\"\"\n",
    "        self.logger.info(\"üìä PHASE 7: AGR√âGATIONS TEMPORELLES ET SPATIALES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä PHASE 7: AGR√âGATIONS TEMPORELLES ET SPATIALES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        aggregations = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            dataset_aggs = {}\n",
    "\n",
    "            # Agr√©gation temporelle\n",
    "            temporal_vars = self.descriptive_analyzer.identify_temporal_variables(df)\n",
    "            if temporal_vars:\n",
    "                time_col = temporal_vars[0]\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols if col != time_col][:10]\n",
    "\n",
    "                if value_cols:\n",
    "                    temp_agg = self.aggregation_engine.temporal_aggregation(\n",
    "                        df, time_col, value_cols\n",
    "                    )\n",
    "                    if not temp_agg.empty:\n",
    "                        dataset_aggs[\"temporal\"] = temp_agg\n",
    "                        temp_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_temporal_agg.csv\"\n",
    "                        )\n",
    "                        temp_agg.to_csv(temp_path, index=False)\n",
    "\n",
    "            # Agr√©gation spatiale\n",
    "            spatial_vars = self.descriptive_analyzer.identify_spatial_variables(df)\n",
    "            if spatial_vars:\n",
    "                spatial_col = spatial_vars[0]\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols][:10]\n",
    "\n",
    "                if value_cols:\n",
    "                    spatial_agg = self.aggregation_engine.spatial_aggregation(\n",
    "                        df, spatial_col, value_cols\n",
    "                    )\n",
    "                    if not spatial_agg.empty:\n",
    "                        dataset_aggs[\"spatial\"] = spatial_agg\n",
    "                        spatial_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_spatial_agg.csv\"\n",
    "                        )\n",
    "                        spatial_agg.to_csv(spatial_path, index=False)\n",
    "\n",
    "            # Normalisation par population\n",
    "            if \"total_population\" in df.columns or \"population\" in df.columns:\n",
    "                pop_col = (\n",
    "                    \"total_population\"\n",
    "                    if \"total_population\" in df.columns\n",
    "                    else \"population\"\n",
    "                )\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols if col != pop_col][:5]\n",
    "\n",
    "                if value_cols:\n",
    "                    normalized = self.aggregation_engine.normalize_by_population(\n",
    "                        df, value_cols, pop_col\n",
    "                    )\n",
    "                    new_cols = [\n",
    "                        col for col in normalized.columns if \"_per_capita\" in col\n",
    "                    ]\n",
    "\n",
    "                    if new_cols:\n",
    "                        dataset_aggs[\"per_capita\"] = normalized[new_cols + [pop_col]]\n",
    "                        norm_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_per_capita.csv\"\n",
    "                        )\n",
    "                        normalized.to_csv(norm_path, index=False)\n",
    "\n",
    "            if dataset_aggs:\n",
    "                aggregations[name] = dataset_aggs\n",
    "                print(f\"\\n‚úÖ {name}: {len(dataset_aggs)} types d'agr√©gation\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Agr√©gations termin√©es\")\n",
    "        print(f\"   Datasets agr√©g√©s: {len(aggregations)}\")\n",
    "\n",
    "        return aggregations\n",
    "\n",
    "    def generate_methodology_document(self) -> pd.DataFrame:\n",
    "        \"\"\"G√©n√®re la documentation m√©thodologique\"\"\"\n",
    "        self.logger.info(\"üìù G√©n√©ration de la documentation m√©thodologique\")\n",
    "\n",
    "        methodology = []\n",
    "\n",
    "        # Anomalies\n",
    "        anomaly_report = self.anomaly_detector.generate_anomaly_report()\n",
    "        if not anomaly_report.empty:\n",
    "            for _, row in anomaly_report.iterrows():\n",
    "                methodology.append(\n",
    "                    {\n",
    "                        \"category\": \"Anomalie\",\n",
    "                        \"dataset\": row[\"dataset\"],\n",
    "                        \"variable\": row[\"variable\"],\n",
    "                        \"type\": row[\"anomaly_type\"],\n",
    "                        \"action\": f\"{row['count']} valeurs d√©tect√©es ({row['percentage']}%)\",\n",
    "                        \"justification\": f\"Seuil: {self.config.ZSCORE_THRESHOLD if row['anomaly_type'] == 'zscore' else self.config.IQR_MULTIPLIER}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Indicateurs cr√©√©s\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"population_growth_rate\",\n",
    "                \"type\": \"D√©riv√©\",\n",
    "                \"action\": \"Taux de croissance calcul√©\",\n",
    "                \"justification\": \"Variation annuelle en pourcentage\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"gdp_per_capita\",\n",
    "                \"type\": \"Ratio\",\n",
    "                \"action\": \"PIB / Population\",\n",
    "                \"justification\": \"Normalisation √©conomique\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"regional_development_index\",\n",
    "                \"type\": \"Composite\",\n",
    "                \"action\": \"Indice multi-dimensionnel\",\n",
    "                \"justification\": \"Combinaison normalis√©e de plusieurs indicateurs\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Agr√©gations\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Agr√©gation\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"temporal_aggregation\",\n",
    "                \"type\": \"Temporelle\",\n",
    "                \"action\": \"Agr√©gation par ann√©e\",\n",
    "                \"justification\": \"Analyse des tendances historiques\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Agr√©gation\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"spatial_aggregation\",\n",
    "                \"type\": \"Spatiale\",\n",
    "                \"action\": \"Agr√©gation par r√©gion\",\n",
    "                \"justification\": \"Comparaisons g√©ographiques\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology_df = pd.DataFrame(methodology)\n",
    "\n",
    "        # Sauvegarde\n",
    "        method_path = self.directories[\"analysis\"] / \"methodology_documentation.csv\"\n",
    "        methodology_df.to_csv(method_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"‚úÖ Documentation: {method_path.name}\")\n",
    "\n",
    "        return methodology_df\n",
    "\n",
    "    def run_complete_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"Ex√©cute le pipeline complet d'analyse\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üöÄ D√âMARRAGE PIPELINE ANALYSE COMPL√àTE - T√ÇCHE 2\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Chargement des donn√©es\n",
    "        datasets = self.loader.load_all_datasets()\n",
    "\n",
    "        if not datasets:\n",
    "            self.logger.error(\n",
    "                \"‚ùå Aucune donn√©e charg√©e. V√©rifiez le r√©pertoire d'entr√©e.\"\n",
    "            )\n",
    "            return {}\n",
    "\n",
    "        # Phase 1: Analyse descriptive\n",
    "        descriptive_results = self.run_descriptive_analysis(datasets)\n",
    "\n",
    "        # Phase 2: Tendances temporelles\n",
    "        trend_results = self.run_trend_analysis(datasets)\n",
    "\n",
    "        # Phase 3: Dynamiques spatiales\n",
    "        spatial_results = self.run_spatial_analysis(datasets)\n",
    "\n",
    "        # Phase 4: Corr√©lations\n",
    "        correlation_results = self.run_correlation_analysis(datasets)\n",
    "\n",
    "        # Phase 5: Anomalies\n",
    "        anomaly_results = self.run_anomaly_detection(datasets)\n",
    "\n",
    "        # Phase 6: Indicateurs d√©riv√©s\n",
    "        enriched_datasets = self.run_indicator_creation(datasets)\n",
    "\n",
    "        # Phase 7: Agr√©gations\n",
    "        aggregation_results = self.run_aggregations(\n",
    "            enriched_datasets if enriched_datasets else datasets\n",
    "        )\n",
    "\n",
    "        # Documentation m√©thodologique\n",
    "        methodology_doc = self.generate_methodology_document()\n",
    "\n",
    "        # Rapport final\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nüìä R√âSULTATS:\")\n",
    "        print(f\"   - Datasets analys√©s: {len(datasets)}\")\n",
    "        print(f\"   - Datasets enrichis: {len(enriched_datasets)}\")\n",
    "        print(f\"   - Tendances identifi√©es: {len(trend_results)}\")\n",
    "        print(f\"   - Analyses spatiales: {len(spatial_results)}\")\n",
    "        print(f\"   - Matrices de corr√©lation: {len(correlation_results)}\")\n",
    "        print(\n",
    "            f\"   - Anomalies d√©tect√©es: {len(anomaly_results.get('anomaly_report', []))}\"\n",
    "        )\n",
    "        print(f\"\\nüìÇ LIVRABLES:\")\n",
    "        print(f\"   - Donn√©es enrichies: {self.directories['enriched']}\")\n",
    "        print(f\"   - Analyses: {self.directories['analysis']}\")\n",
    "        print(f\"   - Rapports d'anomalies: {self.directories['anomalies']}\")\n",
    "        print(f\"   - Visualisations: {self.directories['visualizations']}\")\n",
    "        print(f\"   - Donn√©es agr√©g√©es: {self.directories['processed']}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"datasets\": datasets,\n",
    "            \"descriptive_analysis\": descriptive_results,\n",
    "            \"trends\": trend_results,\n",
    "            \"spatial_analysis\": spatial_results,\n",
    "            \"correlations\": correlation_results,\n",
    "            \"anomalies\": anomaly_results,\n",
    "            \"enriched_datasets\": enriched_datasets,\n",
    "            \"aggregations\": aggregation_results,\n",
    "            \"methodology\": methodology_doc,\n",
    "        }\n",
    "\n"
   ],
   "id": "cd2296e51372ec44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main():\n",
    "    \"\"\"Point d'entr√©e principal\"\"\"\n",
    "\n",
    "    # Configuration de l'environnement\n",
    "    setup_analysis_environment(log_dir=Path(\"logs_task_2\"))\n",
    "\n",
    "    # Cr√©ation de l'orchestrateur\n",
    "    orchestrator = ExplorationOrchestrator()\n",
    "\n",
    "    # Ex√©cution du pipeline complet\n",
    "    results = orchestrator.run_complete_analysis()\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "id": "453a9b8865694138"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = main()\n"
   ],
   "id": "dc54e6209e1bcd2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
