{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.234329Z",
     "start_time": "2025-10-17T21:58:32.535743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import functools\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Tuple, Any, Union\n",
    "from dataclasses import dataclass, field\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n"
   ],
   "id": "dec957620be239ce",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.258489Z",
     "start_time": "2025-10-17T21:58:33.249963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for analysis tasks.\n",
    "\n",
    "    Provides configurations such as country details, anomaly detection\n",
    "    thresholds, indicator configurations, and directory structure\n",
    "    required for data analysis tasks.\n",
    "\n",
    "    Attributes:\n",
    "        COUNTRY_CODE: ISO 3166-1 alpha-2 country code.\n",
    "        COUNTRY_NAME: Full country name.\n",
    "        ZSCORE_THRESHOLD: Threshold value for anomaly detection using Z-score.\n",
    "        IQR_MULTIPLIER: Multiplier value for anomaly detection using the\n",
    "            interquartile range (IQR) method.\n",
    "        CORRELATION_THRESHOLD: Threshold to identify significant\n",
    "            correlations between datasets.\n",
    "        POPULATION_AGE_YOUNG: Age defining the younger population boundary.\n",
    "        POPULATION_AGE_OLD: Age defining the older population boundary.\n",
    "        GDP_BASE_YEAR: Base year for GDP calculations.\n",
    "        DIRECTORY_STRUCTURE: A dictionary specifying paths for input, output,\n",
    "            processed, enriched, analysis, anomalies, visualizations,\n",
    "            and logs directories.\n",
    "    \"\"\"\n",
    "\n",
    "    COUNTRY_CODE: str = \"BJ\"\n",
    "    COUNTRY_NAME: str = \"Bénin\"\n",
    "\n",
    "    ZSCORE_THRESHOLD: float = 3.0\n",
    "    IQR_MULTIPLIER: float = 1.5\n",
    "    CORRELATION_THRESHOLD: float = 0.7\n",
    "\n",
    "    POPULATION_AGE_YOUNG: int = 15\n",
    "    POPULATION_AGE_OLD: int = 65\n",
    "    GDP_BASE_YEAR: int = 2015\n",
    "\n",
    "    DIRECTORY_STRUCTURE: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"input\": \"data_task_1/final_data\",\n",
    "            \"output\": \"data_task_2\",\n",
    "            \"processed\": \"data_task_2/processed\",\n",
    "            \"enriched\": \"data_task_2/enriched\",\n",
    "            \"analysis\": \"data_task_2/analysis\",\n",
    "            \"anomalies\": \"data_task_2/anomalies\",\n",
    "            \"visualizations\": \"data_task_2/visualizations\",\n",
    "            \"logs\": \"logs_task_2\",\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "9fc5792ea2c7a95d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.314364Z",
     "start_time": "2025-10-17T21:58:33.305598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_analysis_environment(log_dir: Optional[Path] = None) -> None:\n",
    "    \"\"\"\n",
    "    Sets up the analysis environment with logging, display configurations for\n",
    "    pandas, and visualization settings for seaborn and matplotlib. Ensures a\n",
    "    consistent and user-friendly environment for data analysis and visualization.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If the specified log directory cannot be created or accessed.\n",
    "\n",
    "    Parameters:\n",
    "        log_dir (Optional[Path]): Path to the directory for saving log files.\n",
    "                                  If None, logging to a file is skipped.\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    log_format = \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\"\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=log_format,\n",
    "        datefmt=date_format,\n",
    "        handlers=[console_handler],\n",
    "    )\n",
    "\n",
    "    if log_dir:\n",
    "        log_dir = Path(log_dir)\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        file_handler = RotatingFileHandler(\n",
    "            log_dir / \"analysis.log\",\n",
    "            maxBytes=5_000_000,\n",
    "            backupCount=3,\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        file_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "        logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 100)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"Set2\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"figure.figsize\": (14, 8),\n",
    "            \"axes.titlesize\": 14,\n",
    "            \"axes.labelsize\": 12,\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "5583c07e76a855c1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.373049Z",
     "start_time": "2025-10-17T21:58:33.363458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DirectoryManager:\n",
    "    \"\"\"\n",
    "    Handles directory structure management and initialization.\n",
    "\n",
    "    DirectoryManager is a utility class designed to manage and organize\n",
    "    a set of directories based on a predefined structure. It allows for\n",
    "    initializing directory structures, retrieving directory paths by\n",
    "    name, and logging activity. It can be configured using an AnalysisConfig\n",
    "    object or defaults to a predefined configuration.\n",
    "\n",
    "    Attributes:\n",
    "        base_dir (Path): The base directory where the directory structure\n",
    "            will be created. Defaults to the current working directory if\n",
    "            not specified.\n",
    "        config (AnalysisConfig): The configuration object containing the\n",
    "            directory structure specifications.\n",
    "        logger (Logger): Logger instance for logging activities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_dir: Optional[Path] = None, config: Optional[AnalysisConfig] = None\n",
    "    ):\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._directories: Dict[str, Path] = {}\n",
    "\n",
    "    def initialize_structure(self) -> Dict[str, Path]:\n",
    "        for name, path in self.config.DIRECTORY_STRUCTURE.items():\n",
    "            full_path = self.base_dir / path\n",
    "            full_path.mkdir(parents=True, exist_ok=True)\n",
    "            self._directories[name] = full_path\n",
    "\n",
    "        self.logger.info(f\"✅ {len(self._directories)} répertoires créés\")\n",
    "        return self._directories\n",
    "\n",
    "    def get_path(self, name: str) -> Optional[Path]:\n",
    "        return self._directories.get(name)\n",
    "\n"
   ],
   "id": "3a574cd36d719766",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.424388Z",
     "start_time": "2025-10-17T21:58:33.416964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class AnomalyReport:\n",
    "    \"\"\"\n",
    "    Represents a report for detected anomalies in a dataset.\n",
    "\n",
    "    This class encapsulates details about anomalies identified in a specific dataset\n",
    "    and variable. It provides a structure to store the summary of anomalies including\n",
    "    anomaly count, percentage, type, and additional details. The timestamp is\n",
    "    automatically captured when an object of this class is instantiated.\n",
    "\n",
    "    Attributes:\n",
    "        dataset_name: Name of the dataset where anomalies are detected.\n",
    "        variable: The specific variable in the dataset associated with the anomalies.\n",
    "        anomaly_type: Type of the anomaly identified (e.g., outlier, missing data).\n",
    "        anomaly_count: Total number of anomalies detected.\n",
    "        anomaly_percentage: Percentage of anomalies relative to the total data points.\n",
    "        details: Additional information about the anomalies.\n",
    "        timestamp: The time and date when the anomaly report was created.\n",
    "\n",
    "    Methods:\n",
    "        to_dict:\n",
    "            Converts the anomaly report instance into a dictionary representation.\n",
    "            The dictionary includes dataset name, variable, anomaly type, count,\n",
    "            percentage, details, and timestamp for easier serialization or logging.\n",
    "    \"\"\"\n",
    "    dataset_name: str\n",
    "    variable: str\n",
    "    anomaly_type: str\n",
    "    anomaly_count: int\n",
    "    anomaly_percentage: float\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"dataset\": self.dataset_name,\n",
    "            \"variable\": self.variable,\n",
    "            \"anomaly_type\": self.anomaly_type,\n",
    "            \"count\": self.anomaly_count,\n",
    "            \"percentage\": round(self.anomaly_percentage, 2),\n",
    "            \"details\": str(self.details),\n",
    "            \"timestamp\": self.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n"
   ],
   "id": "9f9e70b8b2154f86",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.483742Z",
     "start_time": "2025-10-17T21:58:33.471744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Class responsible for loading datasets from a specified directory of CSV files.\n",
    "\n",
    "    This class provides functionality for reading multiple CSV files in a given directory and\n",
    "    parsing them into pandas DataFrame objects. The primary purpose of the class is to facilitate\n",
    "    organized dataset loading along with maintaining logging for errors, warnings, and processing\n",
    "    information. It supports UTF-8 encoded files and processes only files with a .csv extension,\n",
    "    skipping any empty files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dir: Path):\n",
    "        self.input_dir = input_dir\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def load_all_datasets(self) -> Dict[str, pd.DataFrame]:\n",
    "        datasets = {}\n",
    "\n",
    "        if not self.input_dir.exists():\n",
    "            self.logger.error(f\"❌ Répertoire introuvable: {self.input_dir}\")\n",
    "            return datasets\n",
    "\n",
    "        csv_files = list(self.input_dir.glob(\"*.csv\"))\n",
    "\n",
    "        if not csv_files:\n",
    "            self.logger.warning(f\"⚠️ Aucun fichier CSV trouvé dans {self.input_dir}\")\n",
    "            return datasets\n",
    "\n",
    "        self.logger.info(f\"📂 Chargement de {len(csv_files)} fichiers...\")\n",
    "\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                dataset_name = file_path.stem\n",
    "                df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    datasets[dataset_name] = df\n",
    "                    self.logger.info(\n",
    "                        f\"✅ {dataset_name}: {len(df)} lignes, {len(df.columns)} colonnes\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.logger.warning(f\"⚠️ {dataset_name}: fichier vide\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    f\"❌ Erreur lors du chargement de {file_path.name}: {e}\"\n",
    "                )\n",
    "\n",
    "        self.logger.info(f\"📊 Total: {len(datasets)} datasets chargés\")\n",
    "        return datasets\n",
    "\n"
   ],
   "id": "b2094d0a8b976485",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.551116Z",
     "start_time": "2025-10-17T21:58:33.529406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DescriptiveAnalyzer:\n",
    "    \"\"\"Performs comprehensive analysis of datasets, including descriptive analytics,\n",
    "    temporal and spatial variable identification, and summary reports.\n",
    "\n",
    "    This class is designed to provide an in-depth look into the structure and\n",
    "    content of datasets. It helps pinpoint missing data, identify variable types,\n",
    "    and generate summarized views of analytic results. Primarily used for\n",
    "    exploratory data analysis, the class includes methods for handling datasets\n",
    "    with temporal and spatial attributes. The results are returned in structured\n",
    "    formats for downstream usage.\n",
    "\n",
    "    Attributes:\n",
    "        config (Optional[AnalysisConfig]): Configuration object for specifying\n",
    "            analysis settings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_dataset(self, df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:\n",
    "        self.logger.info(f\"📊 Analyse descriptive: {dataset_name}\")\n",
    "\n",
    "        analysis: Dict[str, Any] = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"shape\": df.shape,\n",
    "            \"memory_usage_mb\": round(df.memory_usage(deep=True).sum() / (1024**2), 2),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"dtypes\": df.dtypes.value_counts().to_dict(),\n",
    "        }\n",
    "\n",
    "        analysis[\"completeness\"] = {\n",
    "            \"total_cells\": df.size,\n",
    "            \"non_null_cells\": df.notna().sum().sum(),\n",
    "            \"null_cells\": df.isna().sum().sum(),\n",
    "            \"completeness_rate\": round(df.notna().sum().sum() / df.size * 100, 2),\n",
    "        }\n",
    "\n",
    "        column_stats = []\n",
    "        for col in df.columns:\n",
    "            stat = {\n",
    "                \"column\": col,\n",
    "                \"dtype\": str(df[col].dtype),\n",
    "                \"non_null\": int(df[col].notna().sum()),\n",
    "                \"null\": int(df[col].isna().sum()),\n",
    "                \"null_pct\": round(df[col].isna().sum() / len(df) * 100, 2),\n",
    "                \"unique\": int(df[col].nunique()),\n",
    "            }\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                stat.update(\n",
    "                    {\n",
    "                        \"min\": df[col].min(),\n",
    "                        \"max\": df[col].max(),\n",
    "                        \"mean\": round(df[col].mean(), 3),\n",
    "                        \"median\": df[col].median(),\n",
    "                        \"std\": round(df[col].std(), 3),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            column_stats.append(stat)\n",
    "\n",
    "        analysis[\"column_statistics\"] = pd.DataFrame(column_stats)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def identify_temporal_variables(self, df: pd.DataFrame) -> List[str]:\n",
    "        temporal_vars = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if any(\n",
    "                keyword in col.lower() for keyword in [\"year\", \"année\", \"date\", \"time\"]\n",
    "            ):\n",
    "                temporal_vars.append(col)\n",
    "            elif df[col].dtype == \"datetime64[ns]\":\n",
    "                temporal_vars.append(col)\n",
    "\n",
    "        return temporal_vars\n",
    "\n",
    "    def identify_spatial_variables(self, df: pd.DataFrame) -> List[str]:\n",
    "        spatial_vars = []\n",
    "\n",
    "        spatial_keywords = [\n",
    "            \"région\",\n",
    "            \"region\",\n",
    "            \"département\",\n",
    "            \"department\",\n",
    "            \"commune\",\n",
    "            \"ville\",\n",
    "            \"city\",\n",
    "            \"localité\",\n",
    "            \"locality\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"admin\",\n",
    "        ]\n",
    "\n",
    "        for col in df.columns:\n",
    "            if any(keyword in col.lower() for keyword in spatial_keywords):\n",
    "                spatial_vars.append(col)\n",
    "\n",
    "        return spatial_vars\n",
    "\n",
    "    def generate_summary_report(\n",
    "        self, analyses: Dict[str, Dict[str, Any]]\n",
    "    ) -> pd.DataFrame:\n",
    "        summary_data = []\n",
    "\n",
    "        for dataset_name, analysis in analyses.items():\n",
    "            summary_data.append(\n",
    "                {\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"rows\": analysis[\"shape\"][0],\n",
    "                    \"columns\": analysis[\"shape\"][1],\n",
    "                    \"memory_mb\": analysis[\"memory_usage_mb\"],\n",
    "                    \"completeness_rate\": analysis[\"completeness\"][\"completeness_rate\"],\n",
    "                    \"null_cells\": analysis[\"completeness\"][\"null_cells\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n"
   ],
   "id": "f1d0617c72b93e45",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.588630Z",
     "start_time": "2025-10-17T21:58:33.576743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrendAnalyzer:\n",
    "    \"\"\"\n",
    "    TrendAnalyzer\n",
    "\n",
    "    The TrendAnalyzer class provides tools to analyze temporal trends and calculate\n",
    "    growth rates from data. It is designed to operate on pandas DataFrame objects\n",
    "    and assists in processing time-series data for meaningful insights. It features\n",
    "    capabilities such as temporal aggregation, growth rate computations, and trend\n",
    "    analysis across multiple value columns.\n",
    "\n",
    "    Attributes:\n",
    "        config (Optional[AnalysisConfig]): Configuration object for analysis.\n",
    "        logger (logging.Logger): Logger instance for the class, utilized for\n",
    "        logging informational messages and warnings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_temporal_trends(\n",
    "        self, df: pd.DataFrame, time_col: str, value_cols: List[str]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        self.logger.info(f\"📈 Analyse des tendances temporelles\")\n",
    "\n",
    "        trends = {}\n",
    "\n",
    "        for value_col in value_cols:\n",
    "            if value_col not in df.columns or not pd.api.types.is_numeric_dtype(\n",
    "                df[value_col]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                trend_df = (\n",
    "                    df.groupby(time_col)[value_col]\n",
    "                    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                trend_df[\"pct_change\"] = trend_df[\"mean\"].pct_change() * 100\n",
    "                trend_df[\"cumulative_change\"] = (\n",
    "                    (trend_df[\"mean\"] / trend_df[\"mean\"].iloc[0]) - 1\n",
    "                ) * 100\n",
    "\n",
    "                trends[value_col] = trend_df\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"⚠️ Erreur tendance {value_col}: {e}\")\n",
    "\n",
    "        return trends\n",
    "\n",
    "    def calculate_growth_rates(\n",
    "        self, df: pd.DataFrame, time_col: str, value_col: str\n",
    "    ) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "        result = result.sort_values(time_col)\n",
    "\n",
    "        result[\"growth_rate\"] = result[value_col].pct_change() * 100\n",
    "\n",
    "        if len(result) > 1:\n",
    "            first_value = result[value_col].iloc[0]\n",
    "            last_value = result[value_col].iloc[-1]\n",
    "            n_periods = len(result) - 1\n",
    "\n",
    "            if first_value > 0 and last_value > 0:\n",
    "                cagr = (((last_value / first_value) ** (1 / n_periods)) - 1) * 100\n",
    "                result[\"cagr\"] = cagr\n",
    "\n",
    "        return result\n",
    "\n"
   ],
   "id": "da4b9620784a8042",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.633710Z",
     "start_time": "2025-10-17T21:58:33.624146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpatialAnalyzer:\n",
    "    \"\"\"\n",
    "    Handles spatial data analysis tasks related to distributions and regional disparities.\n",
    "\n",
    "    The SpatialAnalyzer class provides methods for analyzing spatial distributions within a\n",
    "    DataFrame and calculating various metrics to identify regional disparities. It enables\n",
    "    users to compute descriptive statistics and evaluate inequality measures such as the Gini\n",
    "    coefficient for spatially grouped datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_spatial_distribution(\n",
    "        self, df: pd.DataFrame, spatial_col: str, value_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        self.logger.info(f\"🗺️ Analyse spatiale sur {spatial_col}\")\n",
    "\n",
    "        spatial_stats = []\n",
    "\n",
    "        for value_col in value_cols:\n",
    "            if value_col not in df.columns or not pd.api.types.is_numeric_dtype(\n",
    "                df[value_col]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stats_by_location = (\n",
    "                    df.groupby(spatial_col)[value_col]\n",
    "                    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\", \"sum\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                stats_by_location[\"variable\"] = value_col\n",
    "                stats_by_location[\"cv\"] = (\n",
    "                    stats_by_location[\"std\"] / stats_by_location[\"mean\"]\n",
    "                ) * 100\n",
    "\n",
    "                spatial_stats.append(stats_by_location)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"⚠️ Erreur spatiale {value_col}: {e}\")\n",
    "\n",
    "        if spatial_stats:\n",
    "            return pd.concat(spatial_stats, ignore_index=True)\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def calculate_regional_disparities(\n",
    "        self, df: pd.DataFrame, spatial_col: str, value_col: str\n",
    "    ) -> Dict[str, float]:\n",
    "        regional_means = df.groupby(spatial_col)[value_col].mean()\n",
    "\n",
    "        disparities = {\n",
    "            \"gini_coefficient\": self._calculate_gini(regional_means.values),\n",
    "            \"coefficient_variation\": (regional_means.std() / regional_means.mean())\n",
    "            * 100,\n",
    "            \"range_ratio\": (\n",
    "                regional_means.max() / regional_means.min()\n",
    "                if regional_means.min() > 0\n",
    "                else np.nan\n",
    "            ),\n",
    "            \"max_value\": regional_means.max(),\n",
    "            \"min_value\": regional_means.min(),\n",
    "            \"mean_value\": regional_means.mean(),\n",
    "        }\n",
    "\n",
    "        return disparities\n",
    "\n",
    "    def _calculate_gini(self, values: np.ndarray) -> float:\n",
    "        values = np.array(values)\n",
    "        values = values[~np.isnan(values)]\n",
    "\n",
    "        if len(values) == 0:\n",
    "            return np.nan\n",
    "\n",
    "        values = np.sort(values)\n",
    "        n = len(values)\n",
    "        index = np.arange(1, n + 1)\n",
    "\n",
    "        return (2 * np.sum(index * values)) / (n * np.sum(values)) - (n + 1) / n\n",
    "\n"
   ],
   "id": "4a95d3703c573058",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.691318Z",
     "start_time": "2025-10-17T21:58:33.681011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CorrelationAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes correlations within datasets and across multiple datasets.\n",
    "\n",
    "    The CorrelationAnalyzer class provides utilities for calculating correlation matrices,\n",
    "    identifying strong correlations, and analyzing relationships between datasets. It\n",
    "    supports various correlation methods and can work with user-defined configurations.\n",
    "\n",
    "    Attributes:\n",
    "        config (AnalysisConfig): Configuration used for correlation analysis.\n",
    "        logger (logging.Logger): Logger for tracking analysis process logs.\n",
    "\n",
    "    Methods:\n",
    "        calculate_correlations: Computes the correlation matrix for the numeric columns in a dataset.\n",
    "        find_strong_correlations: Identifies strong correlations meeting or exceeding a threshold\n",
    "                                   within a correlation matrix.\n",
    "        cross_dataset_correlation: Analyzes correlations between two datasets by merging them\n",
    "                                    on specified columns and examining their numeric columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def calculate_correlations(\n",
    "        self, df: pd.DataFrame, method: str = \"pearson\"\n",
    "    ) -> pd.DataFrame:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        if len(numeric_cols) < 2:\n",
    "            self.logger.warning(\n",
    "                \"⚠️ Pas assez de colonnes numériques pour la corrélation\"\n",
    "            )\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if method == \"pearson\":\n",
    "            corr_matrix = df[numeric_cols].corr(method=\"pearson\")\n",
    "        elif method == \"spearman\":\n",
    "            corr_matrix = df[numeric_cols].corr(method=\"spearman\")\n",
    "        else:\n",
    "            corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "        return corr_matrix\n",
    "\n",
    "    def find_strong_correlations(\n",
    "        self, corr_matrix: pd.DataFrame, threshold: Optional[float] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        threshold = threshold or self.config.CORRELATION_THRESHOLD\n",
    "\n",
    "        strong_corr = []\n",
    "\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                var1 = corr_matrix.columns[i]\n",
    "                var2 = corr_matrix.columns[j]\n",
    "                corr_value = corr_matrix.iloc[i, j]\n",
    "\n",
    "                if abs(corr_value) >= threshold:\n",
    "                    strong_corr.append(\n",
    "                        {\n",
    "                            \"variable_1\": var1,\n",
    "                            \"variable_2\": var2,\n",
    "                            \"correlation\": round(corr_value, 3),\n",
    "                            \"strength\": (\n",
    "                                \"forte\" if abs(corr_value) >= 0.8 else \"modérée\"\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if not strong_corr:\n",
    "            return pd.DataFrame(\n",
    "                columns=[\"variable_1\", \"variable_2\", \"correlation\", \"strength\"]\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(strong_corr).sort_values(\n",
    "            \"correlation\", key=abs, ascending=False\n",
    "        )\n",
    "\n",
    "    def cross_dataset_correlation(\n",
    "        self, df1: pd.DataFrame, df2: pd.DataFrame, merge_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        try:\n",
    "            merged = pd.merge(\n",
    "                df1, df2, on=merge_cols, how=\"inner\", suffixes=(\"_1\", \"_2\")\n",
    "            )\n",
    "\n",
    "            numeric_cols = merged.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "            if len(numeric_cols) >= 2:\n",
    "                return self.calculate_correlations(merged[numeric_cols])\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"❌ Erreur corrélation croisée: {e}\")\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n"
   ],
   "id": "d1e1280baa2160bf",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.749471Z",
     "start_time": "2025-10-17T21:58:33.736404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    Class for detecting anomalies in data using statistical methods.\n",
    "\n",
    "    This class provides methods to identify anomalies in datasets by applying Z-score and\n",
    "    IQR-based anomaly detection techniques. It also detects inconsistencies in data, such\n",
    "    as negative values in numerical columns or invalid year values outside a specified range.\n",
    "    The class is designed to handle datasets in the form of pandas DataFrames and can generate\n",
    "    reports summarizing detected anomalies.\n",
    "\n",
    "    Attributes:\n",
    "        config (AnalysisConfig): Configuration settings for the anomaly detection process.\n",
    "        anomaly_reports (List[AnomalyReport]): List of anomaly reports generated during\n",
    "            the detection process.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.anomaly_reports: List[AnomalyReport] = []\n",
    "\n",
    "    def detect_zscore_anomalies(\n",
    "        self, df: pd.DataFrame, dataset_name: str\n",
    "    ) -> pd.DataFrame:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        anomalies_df = pd.DataFrame()\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 3:\n",
    "                continue\n",
    "\n",
    "            z_scores = np.abs(zscore(df[col].dropna()))\n",
    "            anomaly_mask = z_scores > self.config.ZSCORE_THRESHOLD\n",
    "\n",
    "            if anomaly_mask.any():\n",
    "                anomaly_count = anomaly_mask.sum()\n",
    "                anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "\n",
    "                report = AnomalyReport(\n",
    "                    dataset_name=dataset_name,\n",
    "                    variable=col,\n",
    "                    anomaly_type=\"zscore\",\n",
    "                    anomaly_count=anomaly_count,\n",
    "                    anomaly_percentage=anomaly_pct,\n",
    "                    details={\"threshold\": self.config.ZSCORE_THRESHOLD},\n",
    "                )\n",
    "                self.anomaly_reports.append(report)\n",
    "\n",
    "        return anomalies_df\n",
    "\n",
    "    def detect_iqr_anomalies(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        anomalies_list = []\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 4:\n",
    "                continue\n",
    "\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            lower_bound = Q1 - self.config.IQR_MULTIPLIER * IQR\n",
    "            upper_bound = Q3 + self.config.IQR_MULTIPLIER * IQR\n",
    "\n",
    "            anomaly_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "\n",
    "            if anomaly_mask.any():\n",
    "                anomaly_count = anomaly_mask.sum()\n",
    "                anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "\n",
    "                report = AnomalyReport(\n",
    "                    dataset_name=dataset_name,\n",
    "                    variable=col,\n",
    "                    anomaly_type=\"iqr\",\n",
    "                    anomaly_count=anomaly_count,\n",
    "                    anomaly_percentage=anomaly_pct,\n",
    "                    details={\n",
    "                        \"lower_bound\": lower_bound,\n",
    "                        \"upper_bound\": upper_bound,\n",
    "                        \"Q1\": Q1,\n",
    "                        \"Q3\": Q3,\n",
    "                        \"IQR\": IQR,\n",
    "                    },\n",
    "                )\n",
    "                self.anomaly_reports.append(report)\n",
    "\n",
    "                anomalies_list.append(\n",
    "                    {\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"variable\": col,\n",
    "                        \"anomaly_indices\": df[anomaly_mask].index.tolist(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return pd.DataFrame(anomalies_list) if anomalies_list else pd.DataFrame()\n",
    "\n",
    "    def detect_inconsistencies(\n",
    "        self, df: pd.DataFrame, dataset_name: str\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        inconsistencies = []\n",
    "\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            if any(\n",
    "                keyword in col.lower()\n",
    "                for keyword in [\"population\", \"count\", \"nombre\", \"effectif\"]\n",
    "            ):\n",
    "                negative_count = (df[col] < 0).sum()\n",
    "                if negative_count > 0:\n",
    "                    inconsistencies.append(\n",
    "                        {\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variable\": col,\n",
    "                            \"issue\": \"valeurs_négatives\",\n",
    "                            \"count\": negative_count,\n",
    "                            \"description\": f\"{negative_count} valeurs négatives pour une variable de comptage\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        for col in df.columns:\n",
    "            if \"year\" in col.lower() or \"année\" in col.lower():\n",
    "                invalid_years = df[(df[col] < 1900) | (df[col] > 2025)]\n",
    "                if len(invalid_years) > 0:\n",
    "                    inconsistencies.append(\n",
    "                        {\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variable\": col,\n",
    "                            \"issue\": \"années_invalides\",\n",
    "                            \"count\": len(invalid_years),\n",
    "                            \"description\": f\"{len(invalid_years)} années hors de la plage [1900-2025]\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return inconsistencies\n",
    "\n",
    "    def generate_anomaly_report(self) -> pd.DataFrame:\n",
    "        if not self.anomaly_reports:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return pd.DataFrame([report.to_dict() for report in self.anomaly_reports])\n",
    "\n"
   ],
   "id": "52d617724840d60a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.803037Z",
     "start_time": "2025-10-17T21:58:33.788470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndicatorBuilder:\n",
    "    \"\"\"\n",
    "    This class is responsible for constructing various socio-economic statistical indicators.\n",
    "\n",
    "    The IndicatorBuilder class provides methods to generate demographic, economic, education,\n",
    "    and composite indicators from a provided dataset. These indicators can be further used\n",
    "    for statistical analysis, regional development monitoring, or economic assessments. The\n",
    "    class allows customization through configuration settings.\n",
    "\n",
    "    Attributes:\n",
    "        config (AnalysisConfig): Configuration object for the builder, including settings\n",
    "            like base year for GDP calculations.\n",
    "        logger (Logger): Logger instance for capturing events during the indicator-building\n",
    "            process.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def build_demographic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        if \"population\" in df.columns or \"total_population\" in df.columns:\n",
    "            pop_col = \"population\" if \"population\" in df.columns else \"total_population\"\n",
    "\n",
    "            if \"year\" in df.columns or \"année\" in df.columns:\n",
    "                time_col = \"year\" if \"year\" in df.columns else \"année\"\n",
    "                result = result.sort_values(time_col)\n",
    "                result[\"population_growth_rate\"] = result[pop_col].pct_change() * 100\n",
    "\n",
    "        if \"population_0_14\" in df.columns and \"total_population\" in df.columns:\n",
    "            result[\"youth_population_ratio\"] = (\n",
    "                result[\"population_0_14\"] / result[\"total_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        if \"population_65_plus\" in df.columns and \"total_population\" in df.columns:\n",
    "            result[\"elderly_population_ratio\"] = (\n",
    "                result[\"population_65_plus\"] / result[\"total_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        if all(col in df.columns for col in [\"total_population\", \"surface_area_km2\"]):\n",
    "            result[\"population_density\"] = (\n",
    "                result[\"total_population\"] / result[\"surface_area_km2\"]\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"✅ Indicateurs démographiques créés\")\n",
    "        return result\n",
    "\n",
    "    def build_economic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        if all(col in df.columns for col in [\"gdp\", \"total_population\"]):\n",
    "            result[\"gdp_per_capita\"] = result[\"gdp\"] / result[\"total_population\"]\n",
    "\n",
    "        if \"gdp\" in df.columns:\n",
    "            if \"year\" in df.columns or \"année\" in df.columns:\n",
    "                time_col = \"year\" if \"year\" in df.columns else \"année\"\n",
    "                result = result.sort_values(time_col)\n",
    "                result[\"gdp_growth_rate\"] = result[\"gdp\"].pct_change() * 100\n",
    "\n",
    "        if \"gdp\" in df.columns and \"year\" in df.columns:\n",
    "            base_year = self.config.GDP_BASE_YEAR\n",
    "            base_gdp = (\n",
    "                result[result[\"year\"] == base_year][\"gdp\"].iloc[0]\n",
    "                if base_year in result[\"year\"].values\n",
    "                else result[\"gdp\"].iloc[0]\n",
    "            )\n",
    "            result[\"gdp_index\"] = (result[\"gdp\"] / base_gdp) * 100\n",
    "\n",
    "        self.logger.info(\"✅ Indicateurs économiques créés\")\n",
    "        return result\n",
    "\n",
    "    def build_education_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        if all(\n",
    "            col in df.columns for col in [\"enrolled_students\", \"school_age_population\"]\n",
    "        ):\n",
    "            result[\"net_enrollment_rate\"] = (\n",
    "                result[\"enrolled_students\"] / result[\"school_age_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        if all(col in df.columns for col in [\"total_students\", \"total_teachers\"]):\n",
    "            result[\"student_teacher_ratio\"] = (\n",
    "                result[\"total_students\"] / result[\"total_teachers\"]\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"✅ Indicateurs d'éducation créés\")\n",
    "        return result\n",
    "\n",
    "    def build_composite_index(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        indicators: List[str],\n",
    "        weights: Optional[List[float]] = None,\n",
    "        index_name: str = \"composite_index\",\n",
    "    ) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        available_indicators = [ind for ind in indicators if ind in df.columns]\n",
    "\n",
    "        if not available_indicators:\n",
    "            self.logger.warning(f\"⚠️ Aucun indicateur disponible pour {index_name}\")\n",
    "            return result\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        normalized_data = scaler.fit_transform(df[available_indicators].fillna(0))\n",
    "        normalized_df = pd.DataFrame(\n",
    "            normalized_data, columns=available_indicators, index=df.index\n",
    "        )\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1 / len(available_indicators)] * len(available_indicators)\n",
    "\n",
    "        result[index_name] = sum(\n",
    "            normalized_df[ind] * weight\n",
    "            for ind, weight in zip(available_indicators, weights)\n",
    "        )\n",
    "\n",
    "        result[index_name] = (\n",
    "            (result[index_name] - result[index_name].min())\n",
    "            / (result[index_name].max() - result[index_name].min())\n",
    "        ) * 100\n",
    "\n",
    "        self.logger.info(f\"✅ Indice composite '{index_name}' créé\")\n",
    "        return result\n",
    "\n",
    "    def build_regional_development_index(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        indicators = []\n",
    "\n",
    "        if \"gdp_per_capita\" in df.columns:\n",
    "            indicators.append(\"gdp_per_capita\")\n",
    "        if \"net_enrollment_rate\" in df.columns:\n",
    "            indicators.append(\"net_enrollment_rate\")\n",
    "        if \"life_expectancy\" in df.columns:\n",
    "            indicators.append(\"life_expectancy\")\n",
    "        if \"access_electricity\" in df.columns:\n",
    "            indicators.append(\"access_electricity\")\n",
    "\n",
    "        if indicators:\n",
    "            return self.build_composite_index(\n",
    "                df, indicators, index_name=\"regional_development_index\"\n",
    "            )\n",
    "\n",
    "        self.logger.warning(\"⚠️ Pas assez d'indicateurs pour l'indice de développement\")\n",
    "        return df\n",
    "\n"
   ],
   "id": "ec3c5d7226443745",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.853229Z",
     "start_time": "2025-10-17T21:58:33.841225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AggregationEngine:\n",
    "    \"\"\"\n",
    "    Facilitates various types of data aggregation, including temporal, spatial, normalization\n",
    "    by population, and multi-level aggregation.\n",
    "\n",
    "    The class provides methods to manipulate, aggregate, and normalize data in structured\n",
    "    tabular formats, such as those handled by pandas DataFrames. Aggregation can be performed\n",
    "    based on specific time, spatial, or hierarchical grouping levels, with support for custom\n",
    "    aggregation functions. Additionally, normalization of values by population is available\n",
    "    to standardize metrics for better interpretability across datasets.\n",
    "\n",
    "    Attributes:\n",
    "        config: Optional AnalysisConfig instance to configure the behavior of the engine.\n",
    "        logger: Logging instance used for debugging and information purposes.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def temporal_aggregation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        time_col: str,\n",
    "        value_cols: List[str],\n",
    "        agg_functions: Optional[Dict[str, str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if agg_functions is None:\n",
    "            agg_functions = {col: \"mean\" for col in value_cols}\n",
    "\n",
    "        valid_agg = {\n",
    "            col: func\n",
    "            for col, func in agg_functions.items()\n",
    "            if col in df.columns and col != time_col\n",
    "        }\n",
    "\n",
    "        if not valid_agg:\n",
    "            self.logger.warning(\"⚠️ Aucune colonne valide pour l'agrégation\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        aggregated = df.groupby(time_col).agg(valid_agg).reset_index()\n",
    "\n",
    "        aggregated.columns = [\n",
    "            f\"{col}_{func}\" if col != time_col else col\n",
    "            for col, func in zip(\n",
    "                aggregated.columns, [time_col] + list(valid_agg.values())\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.logger.info(f\"✅ Agrégation temporelle: {len(aggregated)} périodes\")\n",
    "        return aggregated\n",
    "\n",
    "    def spatial_aggregation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        spatial_col: str,\n",
    "        value_cols: List[str],\n",
    "        agg_functions: Optional[Dict[str, str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if agg_functions is None:\n",
    "            agg_functions = {col: \"sum\" for col in value_cols}\n",
    "\n",
    "        valid_agg = {\n",
    "            col: func\n",
    "            for col, func in agg_functions.items()\n",
    "            if col in df.columns and col != spatial_col\n",
    "        }\n",
    "\n",
    "        if not valid_agg:\n",
    "            self.logger.warning(\"⚠️ Aucune colonne valide pour l'agrégation spatiale\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        aggregated = df.groupby(spatial_col).agg(valid_agg).reset_index()\n",
    "\n",
    "        self.logger.info(f\"✅ Agrégation spatiale: {len(aggregated)} zones\")\n",
    "        return aggregated\n",
    "\n",
    "    def normalize_by_population(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        value_cols: List[str],\n",
    "        population_col: str = \"total_population\",\n",
    "    ) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        if population_col not in df.columns:\n",
    "            self.logger.warning(f\"⚠️ Colonne {population_col} introuvable\")\n",
    "            return result\n",
    "\n",
    "        for col in value_cols:\n",
    "            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                new_col_name = f\"{col}_per_capita\"\n",
    "                result[new_col_name] = result[col] / result[population_col]\n",
    "                self.logger.info(f\"✅ Créé: {new_col_name}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def multi_level_aggregation(\n",
    "        self, df: pd.DataFrame, group_cols: List[str], value_cols: List[str]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        aggregations = {}\n",
    "\n",
    "        for i in range(1, len(group_cols) + 1):\n",
    "            level_cols = group_cols[:i]\n",
    "            level_name = \"_\".join(level_cols)\n",
    "\n",
    "            agg_dict = {\n",
    "                col: [\"sum\", \"mean\", \"count\"] for col in value_cols if col in df.columns\n",
    "            }\n",
    "\n",
    "            if agg_dict:\n",
    "                agg_result = df.groupby(level_cols).agg(agg_dict).reset_index()\n",
    "                agg_result.columns = [\n",
    "                    \"_\".join(col).strip(\"_\") for col in agg_result.columns\n",
    "                ]\n",
    "                aggregations[level_name] = agg_result\n",
    "                self.logger.info(f\"✅ Agrégation niveau {i}: {len(agg_result)} groupes\")\n",
    "\n",
    "        return aggregations\n",
    "\n"
   ],
   "id": "6960b731be31e781",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.911587Z",
     "start_time": "2025-10-17T21:58:33.898063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VisualizationEngine:\n",
    "    \"\"\"\n",
    "    Handles the visualization of various datasets and trends.\n",
    "\n",
    "    This class provides methods to create and save visualizations for temporal trends, correlation matrices,\n",
    "    and spatial distributions. The class is designed to facilitate exploratory data analysis and better\n",
    "    understanding of data patterns. It takes care of rendering plots and saving them to a specified output\n",
    "    directory.\n",
    "\n",
    "    Attributes:\n",
    "        output_dir (Path): Directory where the plots will be saved.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def plot_temporal_trends(\n",
    "        self, trends: Dict[str, pd.DataFrame], time_col: str, save: bool = True\n",
    "    ) -> None:\n",
    "        n_plots = len(trends)\n",
    "        if n_plots == 0:\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(min(n_plots, 3), 1, figsize=(14, 4 * min(n_plots, 3)))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for idx, (var_name, trend_df) in enumerate(list(trends.items())[:3]):\n",
    "            ax = axes[idx]\n",
    "            ax.plot(\n",
    "                trend_df[time_col],\n",
    "                trend_df[\"mean\"],\n",
    "                marker=\"o\",\n",
    "                linewidth=2,\n",
    "                label=\"Moyenne\",\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                trend_df[time_col],\n",
    "                trend_df[\"mean\"] - trend_df[\"std\"],\n",
    "                trend_df[\"mean\"] + trend_df[\"std\"],\n",
    "                alpha=0.3,\n",
    "                label=\"±1 écart-type\",\n",
    "            )\n",
    "            ax.set_title(\n",
    "                f\"Évolution temporelle: {var_name}\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Année\")\n",
    "            ax.set_ylabel(\"Valeur\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"temporal_trends.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"💾 Graphique sauvegardé: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_correlation_heatmap(\n",
    "        self, corr_matrix: pd.DataFrame, save: bool = True\n",
    "    ) -> None:\n",
    "        if corr_matrix.empty:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "        sns.heatmap(\n",
    "            corr_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        ax.set_title(\"Matrice de corrélation\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"correlation_heatmap.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"💾 Heatmap sauvegardée: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_spatial_distribution(\n",
    "        self,\n",
    "        spatial_stats: pd.DataFrame,\n",
    "        spatial_col: str,\n",
    "        value_col: str = \"mean\",\n",
    "        save: bool = True,\n",
    "    ) -> None:\n",
    "        if spatial_stats.empty:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        spatial_stats_sorted = spatial_stats.sort_values(\n",
    "            value_col, ascending=False\n",
    "        ).head(20)\n",
    "\n",
    "        bars = ax.barh(\n",
    "            spatial_stats_sorted[spatial_col], spatial_stats_sorted[value_col]\n",
    "        )\n",
    "\n",
    "        # Gradient de couleurs\n",
    "        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(bars)))\n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "\n",
    "        ax.set_xlabel(\"Valeur moyenne\")\n",
    "        ax.set_title(f\"Distribution spatiale (Top 20)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"spatial_distribution.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"💾 Distribution sauvegardée: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n"
   ],
   "id": "fd0369ef7b7f916f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:34.039656Z",
     "start_time": "2025-10-17T21:58:33.950291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ExplorationOrchestrator:\n",
    "    \"\"\"\n",
    "    Handles orchestration of various data analysis phases including descriptive analysis,\n",
    "    trend analysis, spatial analysis, correlation analysis, and anomaly detection.\n",
    "\n",
    "    This class is responsible for coordinating the execution and management of different\n",
    "    analysis modules. It provides functionalities to analyze datasets in stages,\n",
    "    producing reports, visualizations, and other results as outputs for each phase. It\n",
    "    integrates functionalities such as data loading, managing file directories, and\n",
    "    logging progress during analysis.\n",
    "\n",
    "    Attributes:\n",
    "        config (Optional[AnalysisConfig]): Configuration settings for the analysis.\n",
    "        base_dir (Optional[Path]): Base directory for organization of analysis files and outputs.\n",
    "        logger: Logger instance for tracking execution details.\n",
    "        dir_manager: Manages directory structure for analysis input and output files.\n",
    "        directories: Dictionary containing paths to different directories for input, output, etc.\n",
    "        loader: Loader module for importing datasets.\n",
    "        descriptive_analyzer: Module for conducting descriptive statistics analysis.\n",
    "        trend_analyzer: Module for analyzing temporal trends in data.\n",
    "        spatial_analyzer: Module for analyzing spatial patterns in data.\n",
    "        correlation_analyzer: Module for detecting correlations and generating correlation matrices.\n",
    "        anomaly_detector: Module for identifying anomalies and inconsistencies within datasets.\n",
    "        indicator_builder: Module for building custom indicators or metrics.\n",
    "        aggregation_engine: Module for aggregating results and data.\n",
    "        viz_engine: Visualization engine for generating plots and visual insights.\n",
    "        results: Stores output of executed analysis stages.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, config: Optional[AnalysisConfig] = None, base_dir: Optional[Path] = None\n",
    "    ):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialisation des composants\n",
    "        self.dir_manager = DirectoryManager(self.base_dir, self.config)\n",
    "        self.directories = self.dir_manager.initialize_structure()\n",
    "\n",
    "        self.loader = DataLoader(self.directories[\"input\"])\n",
    "        self.descriptive_analyzer = DescriptiveAnalyzer(self.config)\n",
    "        self.trend_analyzer = TrendAnalyzer(self.config)\n",
    "        self.spatial_analyzer = SpatialAnalyzer(self.config)\n",
    "        self.correlation_analyzer = CorrelationAnalyzer(self.config)\n",
    "        self.anomaly_detector = AnomalyDetector(self.config)\n",
    "        self.indicator_builder = IndicatorBuilder(self.config)\n",
    "        self.aggregation_engine = AggregationEngine(self.config)\n",
    "        self.viz_engine = VisualizationEngine(self.directories[\"visualizations\"])\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "    def run_descriptive_analysis(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        self.logger.info(\"📊 PHASE 1: ANALYSE DESCRIPTIVE\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"📊 PHASE 1: ANALYSE DESCRIPTIVE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        analyses = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            self.logger.info(f\"▶️ Analyse: {name}\")\n",
    "            analysis = self.descriptive_analyzer.analyze_dataset(df, name)\n",
    "            analyses[name] = analysis\n",
    "\n",
    "            if \"column_statistics\" in analysis:\n",
    "                stats_path = self.directories[\"analysis\"] / f\"{name}_column_stats.csv\"\n",
    "                analysis[\"column_statistics\"].to_csv(stats_path, index=False)\n",
    "\n",
    "        summary_report = self.descriptive_analyzer.generate_summary_report(analyses)\n",
    "        summary_path = self.directories[\"analysis\"] / \"descriptive_summary.csv\"\n",
    "        summary_report.to_csv(summary_path, index=False)\n",
    "\n",
    "        print(\"\\n✅ Analyse descriptive terminée\")\n",
    "        print(f\"   Datasets analysés: {len(analyses)}\")\n",
    "        print(f\"   Rapport: {summary_path.name}\")\n",
    "\n",
    "        return analyses\n",
    "\n",
    "    def run_trend_analysis(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        self.logger.info(\"📈 PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"📈 PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        all_trends = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            temporal_vars = self.descriptive_analyzer.identify_temporal_variables(df)\n",
    "\n",
    "            if not temporal_vars:\n",
    "                continue\n",
    "\n",
    "            time_col = temporal_vars[0]\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            value_cols = [col for col in numeric_cols if col != time_col][:5]\n",
    "\n",
    "            if value_cols:\n",
    "                trends = self.trend_analyzer.analyze_temporal_trends(\n",
    "                    df, time_col, value_cols\n",
    "                )\n",
    "\n",
    "                if trends:\n",
    "                    all_trends[name] = trends\n",
    "\n",
    "                    for var, trend_df in trends.items():\n",
    "                        trend_path = (\n",
    "                            self.directories[\"analysis\"] / f\"{name}_{var}_trend.csv\"\n",
    "                        )\n",
    "                        trend_df.to_csv(trend_path, index=False)\n",
    "\n",
    "                    # Visualisation\n",
    "                    self.viz_engine.plot_temporal_trends(trends, time_col)\n",
    "\n",
    "        print(f\"\\n✅ Analyse des tendances terminée\")\n",
    "        print(f\"   Datasets avec tendances: {len(all_trends)}\")\n",
    "\n",
    "        return all_trends\n",
    "\n",
    "    def run_spatial_analysis(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        self.logger.info(\"🗺️ PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🗺️ PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        spatial_results = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            spatial_vars = self.descriptive_analyzer.identify_spatial_variables(df)\n",
    "\n",
    "            if not spatial_vars:\n",
    "                continue\n",
    "\n",
    "            spatial_col = spatial_vars[0]\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            value_cols = [col for col in numeric_cols if col != spatial_col][:5]\n",
    "\n",
    "            if value_cols:\n",
    "                spatial_dist = self.spatial_analyzer.analyze_spatial_distribution(\n",
    "                    df, spatial_col, value_cols\n",
    "                )\n",
    "\n",
    "                if not spatial_dist.empty:\n",
    "                    spatial_results[name] = spatial_dist\n",
    "\n",
    "                    spatial_path = (\n",
    "                        self.directories[\"analysis\"] / f\"{name}_spatial_analysis.csv\"\n",
    "                    )\n",
    "                    spatial_dist.to_csv(spatial_path, index=False)\n",
    "\n",
    "                    # Visualisation\n",
    "                    self.viz_engine.plot_spatial_distribution(spatial_dist, spatial_col)\n",
    "\n",
    "        print(f\"\\n✅ Analyse spatiale terminée\")\n",
    "        print(f\"   Datasets analysés spatialement: {len(spatial_results)}\")\n",
    "\n",
    "        return spatial_results\n",
    "\n",
    "    def run_correlation_analysis(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        self.logger.info(\"🔗 PHASE 4: ANALYSE DES CORRÉLATIONS\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔗 PHASE 4: ANALYSE DES CORRÉLATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        correlations = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            corr_matrix = self.correlation_analyzer.calculate_correlations(df)\n",
    "\n",
    "            if not corr_matrix.empty:\n",
    "                correlations[name] = {\n",
    "                    \"matrix\": corr_matrix,\n",
    "                    \"strong_correlations\": self.correlation_analyzer.find_strong_correlations(\n",
    "                        corr_matrix\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                corr_path = self.directories[\"analysis\"] / f\"{name}_correlations.csv\"\n",
    "                corr_matrix.to_csv(corr_path)\n",
    "\n",
    "                strong_corr_path = (\n",
    "                    self.directories[\"analysis\"] / f\"{name}_strong_correlations.csv\"\n",
    "                )\n",
    "                if not correlations[name][\"strong_correlations\"].empty:\n",
    "                    correlations[name][\"strong_correlations\"].to_csv(\n",
    "                        strong_corr_path, index=False\n",
    "                    )\n",
    "\n",
    "                self.viz_engine.plot_correlation_heatmap(corr_matrix)\n",
    "\n",
    "        print(f\"\\n✅ Analyse des corrélations terminée\")\n",
    "        print(f\"   Datasets analysés: {len(correlations)}\")\n",
    "\n",
    "        return correlations\n",
    "\n",
    "    def run_anomaly_detection(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        self.logger.info(\"🔍 PHASE 5: DÉTECTION DES ANOMALIES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔍 PHASE 5: DÉTECTION DES ANOMALIES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        all_anomalies = {}\n",
    "        all_inconsistencies = []\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            self.anomaly_detector.detect_zscore_anomalies(df, name)\n",
    "\n",
    "            iqr_anomalies = self.anomaly_detector.detect_iqr_anomalies(df, name)\n",
    "\n",
    "            inconsistencies = self.anomaly_detector.detect_inconsistencies(df, name)\n",
    "\n",
    "            if not iqr_anomalies.empty:\n",
    "                all_anomalies[name] = iqr_anomalies\n",
    "\n",
    "            if inconsistencies:\n",
    "                all_inconsistencies.extend(inconsistencies)\n",
    "\n",
    "        anomaly_report = self.anomaly_detector.generate_anomaly_report()\n",
    "        if not anomaly_report.empty:\n",
    "            anomaly_path = self.directories[\"anomalies\"] / \"anomaly_report.csv\"\n",
    "            anomaly_report.to_csv(anomaly_path, index=False)\n",
    "            print(f\"\\n📄 Rapport d'anomalies: {anomaly_path.name}\")\n",
    "            print(f\"   Total anomalies détectées: {len(anomaly_report)}\")\n",
    "\n",
    "        if all_inconsistencies:\n",
    "            incon_df = pd.DataFrame(all_inconsistencies)\n",
    "            incon_path = self.directories[\"anomalies\"] / \"inconsistencies_report.csv\"\n",
    "            incon_df.to_csv(incon_path, index=False)\n",
    "            print(f\"📄 Rapport d'incohérences: {incon_path.name}\")\n",
    "            print(f\"   Total incohérences: {len(all_inconsistencies)}\")\n",
    "\n",
    "        print(f\"\\n✅ Détection des anomalies terminée\")\n",
    "\n",
    "        return {\n",
    "            \"anomalies\": all_anomalies,\n",
    "            \"anomaly_report\": anomaly_report,\n",
    "            \"inconsistencies\": all_inconsistencies,\n",
    "        }\n",
    "\n",
    "    def run_indicator_creation(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        self.logger.info(\"🔧 PHASE 6: CRÉATION D'INDICATEURS DÉRIVÉS\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔧 PHASE 6: CRÉATION D'INDICATEURS DÉRIVÉS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        enriched_datasets = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            enriched = df.copy()\n",
    "\n",
    "            enriched = self.indicator_builder.build_demographic_indicators(enriched)\n",
    "\n",
    "            enriched = self.indicator_builder.build_economic_indicators(enriched)\n",
    "\n",
    "            enriched = self.indicator_builder.build_education_indicators(enriched)\n",
    "\n",
    "            enriched = self.indicator_builder.build_regional_development_index(enriched)\n",
    "\n",
    "            new_cols = set(enriched.columns) - set(df.columns)\n",
    "\n",
    "            if new_cols:\n",
    "                enriched_datasets[name] = enriched\n",
    "\n",
    "                enriched_path = self.directories[\"enriched\"] / f\"{name}_enriched.csv\"\n",
    "                enriched.to_csv(enriched_path, index=False)\n",
    "\n",
    "                print(f\"\\n✅ {name}: {len(new_cols)} nouveaux indicateurs\")\n",
    "                for col in sorted(new_cols):\n",
    "                    print(f\"   - {col}\")\n",
    "\n",
    "        print(f\"\\n✅ Création d'indicateurs terminée\")\n",
    "        print(f\"   Datasets enrichis: {len(enriched_datasets)}\")\n",
    "\n",
    "        return enriched_datasets\n",
    "\n",
    "    def run_aggregations(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        self.logger.info(\"📊 PHASE 7: AGRÉGATIONS TEMPORELLES ET SPATIALES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"📊 PHASE 7: AGRÉGATIONS TEMPORELLES ET SPATIALES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        aggregations = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            dataset_aggs = {}\n",
    "\n",
    "            temporal_vars = self.descriptive_analyzer.identify_temporal_variables(df)\n",
    "            if temporal_vars:\n",
    "                time_col = temporal_vars[0]\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols if col != time_col][:10]\n",
    "\n",
    "                if value_cols:\n",
    "                    temp_agg = self.aggregation_engine.temporal_aggregation(\n",
    "                        df, time_col, value_cols\n",
    "                    )\n",
    "                    if not temp_agg.empty:\n",
    "                        dataset_aggs[\"temporal\"] = temp_agg\n",
    "                        temp_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_temporal_agg.csv\"\n",
    "                        )\n",
    "                        temp_agg.to_csv(temp_path, index=False)\n",
    "\n",
    "            spatial_vars = self.descriptive_analyzer.identify_spatial_variables(df)\n",
    "            if spatial_vars:\n",
    "                spatial_col = spatial_vars[0]\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols][:10]\n",
    "\n",
    "                if value_cols:\n",
    "                    spatial_agg = self.aggregation_engine.spatial_aggregation(\n",
    "                        df, spatial_col, value_cols\n",
    "                    )\n",
    "                    if not spatial_agg.empty:\n",
    "                        dataset_aggs[\"spatial\"] = spatial_agg\n",
    "                        spatial_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_spatial_agg.csv\"\n",
    "                        )\n",
    "                        spatial_agg.to_csv(spatial_path, index=False)\n",
    "\n",
    "            if \"total_population\" in df.columns or \"population\" in df.columns:\n",
    "                pop_col = (\n",
    "                    \"total_population\"\n",
    "                    if \"total_population\" in df.columns\n",
    "                    else \"population\"\n",
    "                )\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols if col != pop_col][:5]\n",
    "\n",
    "                if value_cols:\n",
    "                    normalized = self.aggregation_engine.normalize_by_population(\n",
    "                        df, value_cols, pop_col\n",
    "                    )\n",
    "                    new_cols = [\n",
    "                        col for col in normalized.columns if \"_per_capita\" in col\n",
    "                    ]\n",
    "\n",
    "                    if new_cols:\n",
    "                        dataset_aggs[\"per_capita\"] = normalized[new_cols + [pop_col]]\n",
    "                        norm_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_per_capita.csv\"\n",
    "                        )\n",
    "                        normalized.to_csv(norm_path, index=False)\n",
    "\n",
    "            if dataset_aggs:\n",
    "                aggregations[name] = dataset_aggs\n",
    "                print(f\"\\n✅ {name}: {len(dataset_aggs)} types d'agrégation\")\n",
    "\n",
    "        print(f\"\\n✅ Agrégations terminées\")\n",
    "        print(f\"   Datasets agrégés: {len(aggregations)}\")\n",
    "\n",
    "        return aggregations\n",
    "\n",
    "    def generate_methodology_document(self) -> pd.DataFrame:\n",
    "        self.logger.info(\"📝 Génération de la documentation méthodologique\")\n",
    "\n",
    "        methodology = []\n",
    "\n",
    "        anomaly_report = self.anomaly_detector.generate_anomaly_report()\n",
    "        if not anomaly_report.empty:\n",
    "            for _, row in anomaly_report.iterrows():\n",
    "                methodology.append(\n",
    "                    {\n",
    "                        \"category\": \"Anomalie\",\n",
    "                        \"dataset\": row[\"dataset\"],\n",
    "                        \"variable\": row[\"variable\"],\n",
    "                        \"type\": row[\"anomaly_type\"],\n",
    "                        \"action\": f\"{row['count']} valeurs détectées ({row['percentage']}%)\",\n",
    "                        \"justification\": f\"Seuil: {self.config.ZSCORE_THRESHOLD if row['anomaly_type'] == 'zscore' else self.config.IQR_MULTIPLIER}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"population_growth_rate\",\n",
    "                \"type\": \"Dérivé\",\n",
    "                \"action\": \"Taux de croissance calculé\",\n",
    "                \"justification\": \"Variation annuelle en pourcentage\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"gdp_per_capita\",\n",
    "                \"type\": \"Ratio\",\n",
    "                \"action\": \"PIB / Population\",\n",
    "                \"justification\": \"Normalisation économique\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"regional_development_index\",\n",
    "                \"type\": \"Composite\",\n",
    "                \"action\": \"Indice multi-dimensionnel\",\n",
    "                \"justification\": \"Combinaison normalisée de plusieurs indicateurs\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Agrégation\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"temporal_aggregation\",\n",
    "                \"type\": \"Temporelle\",\n",
    "                \"action\": \"Agrégation par année\",\n",
    "                \"justification\": \"Analyse des tendances historiques\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Agrégation\",\n",
    "                \"dataset\": \"Général\",\n",
    "                \"variable\": \"spatial_aggregation\",\n",
    "                \"type\": \"Spatiale\",\n",
    "                \"action\": \"Agrégation par région\",\n",
    "                \"justification\": \"Comparaisons géographiques\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology_df = pd.DataFrame(methodology)\n",
    "\n",
    "        method_path = self.directories[\"analysis\"] / \"methodology_documentation.csv\"\n",
    "        methodology_df.to_csv(method_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"✅ Documentation: {method_path.name}\")\n",
    "\n",
    "        return methodology_df\n",
    "\n",
    "    def run_complete_analysis(self) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🚀 DÉMARRAGE PIPELINE ANALYSE COMPLÈTE - TÂCHE 2\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        datasets = self.loader.load_all_datasets()\n",
    "\n",
    "        if not datasets:\n",
    "            self.logger.error(\n",
    "                \"❌ Aucune donnée chargée. Vérifiez le répertoire d'entrée.\"\n",
    "            )\n",
    "            return {}\n",
    "\n",
    "        descriptive_results = self.run_descriptive_analysis(datasets)\n",
    "\n",
    "        trend_results = self.run_trend_analysis(datasets)\n",
    "\n",
    "        spatial_results = self.run_spatial_analysis(datasets)\n",
    "\n",
    "        correlation_results = self.run_correlation_analysis(datasets)\n",
    "\n",
    "        anomaly_results = self.run_anomaly_detection(datasets)\n",
    "\n",
    "        enriched_datasets = self.run_indicator_creation(datasets)\n",
    "\n",
    "        aggregation_results = self.run_aggregations(\n",
    "            enriched_datasets if enriched_datasets else datasets\n",
    "        )\n",
    "\n",
    "        methodology_doc = self.generate_methodology_document()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"✅ PIPELINE TERMINÉ AVEC SUCCÈS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n📊 RÉSULTATS:\")\n",
    "        print(f\"   - Datasets analysés: {len(datasets)}\")\n",
    "        print(f\"   - Datasets enrichis: {len(enriched_datasets)}\")\n",
    "        print(f\"   - Tendances identifiées: {len(trend_results)}\")\n",
    "        print(f\"   - Analyses spatiales: {len(spatial_results)}\")\n",
    "        print(f\"   - Matrices de corrélation: {len(correlation_results)}\")\n",
    "        print(\n",
    "            f\"   - Anomalies détectées: {len(anomaly_results.get('anomaly_report', []))}\"\n",
    "        )\n",
    "        print(f\"\\n📂 LIVRABLES:\")\n",
    "        print(f\"   - Données enrichies: {self.directories['enriched']}\")\n",
    "        print(f\"   - Analyses: {self.directories['analysis']}\")\n",
    "        print(f\"   - Rapports d'anomalies: {self.directories['anomalies']}\")\n",
    "        print(f\"   - Visualisations: {self.directories['visualizations']}\")\n",
    "        print(f\"   - Données agrégées: {self.directories['processed']}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"datasets\": datasets,\n",
    "            \"descriptive_analysis\": descriptive_results,\n",
    "            \"trends\": trend_results,\n",
    "            \"spatial_analysis\": spatial_results,\n",
    "            \"correlations\": correlation_results,\n",
    "            \"anomalies\": anomaly_results,\n",
    "            \"enriched_datasets\": enriched_datasets,\n",
    "            \"aggregations\": aggregation_results,\n",
    "            \"methodology\": methodology_doc,\n",
    "        }\n",
    "\n"
   ],
   "id": "99a25c5038dd1f86",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:34.047380Z",
     "start_time": "2025-10-17T21:58:34.044656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to set up the environment, initialize the orchestrator, and execute\n",
    "    a complete analysis.\n",
    "\n",
    "    Return:\n",
    "        The results of the complete analysis.\n",
    "\n",
    "    \"\"\"\n",
    "    setup_analysis_environment(log_dir=Path(\"logs_task_2\"))\n",
    "\n",
    "    orchestrator = ExplorationOrchestrator()\n",
    "\n",
    "    results = orchestrator.run_complete_analysis()\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "id": "ac46019f024e2d22",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:41.459413Z",
     "start_time": "2025-10-17T21:58:34.091879Z"
    }
   },
   "cell_type": "code",
   "source": "_ = main()\n",
   "id": "a929ff10c7e56e5f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:58:34 | INFO     | DirectoryManager | ✅ 8 répertoires créés\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | 📂 Chargement de 5 fichiers...\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ✅ geographic_cities: 3172 lignes, 7 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ✅ geographic_admin_pays: 1 lignes, 8 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ✅ web_scraping: 148 lignes, 7 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ✅ economic_indicators: 59 lignes, 8 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ✅ geographic: 3173 lignes, 7 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | 📊 Total: 5 datasets chargés\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | 📊 PHASE 1: ANALYSE DESCRIPTIVE\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ▶️ Analyse: geographic_cities\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | 📊 Analyse descriptive: geographic_cities\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ▶️ Analyse: geographic_admin_pays\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | 📊 Analyse descriptive: geographic_admin_pays\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ▶️ Analyse: web_scraping\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | 📊 Analyse descriptive: web_scraping\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ▶️ Analyse: economic_indicators\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | 📊 Analyse descriptive: economic_indicators\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ▶️ Analyse: geographic\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | 📊 Analyse descriptive: geographic\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | 📈 PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\n",
      "2025-10-17 22:58:34 | INFO     | TrendAnalyzer | 📈 Analyse des tendances temporelles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 DÉMARRAGE PIPELINE ANALYSE COMPLÈTE - TÂCHE 2\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "📊 PHASE 1: ANALYSE DESCRIPTIVE\n",
      "================================================================================\n",
      "\n",
      "✅ Analyse descriptive terminée\n",
      "   Datasets analysés: 5\n",
      "   Rapport: descriptive_summary.csv\n",
      "\n",
      "================================================================================\n",
      "📈 PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:58:34 | INFO     | VisualizationEngine | 💾 Graphique sauvegardé: temporal_trends.png\n",
      "2025-10-17 22:58:34 | INFO     | TrendAnalyzer | 📈 Analyse des tendances temporelles\n",
      "2025-10-17 22:58:35 | INFO     | VisualizationEngine | 💾 Graphique sauvegardé: temporal_trends.png\n",
      "2025-10-17 22:58:35 | INFO     | __main__ | 🗺️ PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\n",
      "2025-10-17 22:58:35 | INFO     | SpatialAnalyzer | 🗺️ Analyse spatiale sur latitude\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Analyse des tendances terminée\n",
      "   Datasets avec tendances: 2\n",
      "\n",
      "================================================================================\n",
      "🗺️ PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:58:35 | INFO     | VisualizationEngine | 💾 Distribution sauvegardée: spatial_distribution.png\n",
      "2025-10-17 22:58:35 | INFO     | SpatialAnalyzer | 🗺️ Analyse spatiale sur latitude\n",
      "2025-10-17 22:58:36 | INFO     | VisualizationEngine | 💾 Distribution sauvegardée: spatial_distribution.png\n",
      "2025-10-17 22:58:36 | INFO     | SpatialAnalyzer | 🗺️ Analyse spatiale sur latitude\n",
      "2025-10-17 22:58:37 | INFO     | VisualizationEngine | 💾 Distribution sauvegardée: spatial_distribution.png\n",
      "2025-10-17 22:58:37 | INFO     | __main__ | 🔗 PHASE 4: ANALYSE DES CORRÉLATIONS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Analyse spatiale terminée\n",
      "   Datasets analysés spatialement: 3\n",
      "\n",
      "================================================================================\n",
      "🔗 PHASE 4: ANALYSE DES CORRÉLATIONS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:58:38 | INFO     | VisualizationEngine | 💾 Heatmap sauvegardée: correlation_heatmap.png\n",
      "2025-10-17 22:58:38 | INFO     | VisualizationEngine | 💾 Heatmap sauvegardée: correlation_heatmap.png\n",
      "2025-10-17 22:58:39 | INFO     | VisualizationEngine | 💾 Heatmap sauvegardée: correlation_heatmap.png\n",
      "2025-10-17 22:58:40 | INFO     | VisualizationEngine | 💾 Heatmap sauvegardée: correlation_heatmap.png\n",
      "2025-10-17 22:58:41 | INFO     | VisualizationEngine | 💾 Heatmap sauvegardée: correlation_heatmap.png\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | 🔍 PHASE 5: DÉTECTION DES ANOMALIES\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | 🔧 PHASE 6: CRÉATION D'INDICATEURS DÉRIVÉS\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs démographiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs économiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs d'éducation créés\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ⚠️ Pas assez d'indicateurs pour l'indice de développement\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs démographiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs économiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs d'éducation créés\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ⚠️ Pas assez d'indicateurs pour l'indice de développement\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs démographiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs économiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs d'éducation créés\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ⚠️ Pas assez d'indicateurs pour l'indice de développement\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs démographiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs économiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs d'éducation créés\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ⚠️ Pas assez d'indicateurs pour l'indice de développement\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs démographiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs économiques créés\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ✅ Indicateurs d'éducation créés\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ⚠️ Pas assez d'indicateurs pour l'indice de développement\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | 📊 PHASE 7: AGRÉGATIONS TEMPORELLES ET SPATIALES\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ✅ Agrégation spatiale: 3172 zones\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ✅ Agrégation spatiale: 1 zones\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ✅ Agrégation temporelle: 1 périodes\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ✅ Agrégation temporelle: 10 périodes\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ✅ Agrégation spatiale: 3173 zones\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | 📝 Génération de la documentation méthodologique\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | ✅ Documentation: methodology_documentation.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Analyse des corrélations terminée\n",
      "   Datasets analysés: 5\n",
      "\n",
      "================================================================================\n",
      "🔍 PHASE 5: DÉTECTION DES ANOMALIES\n",
      "================================================================================\n",
      "\n",
      "📄 Rapport d'anomalies: anomaly_report.csv\n",
      "   Total anomalies détectées: 3\n",
      "\n",
      "✅ Détection des anomalies terminée\n",
      "\n",
      "================================================================================\n",
      "🔧 PHASE 6: CRÉATION D'INDICATEURS DÉRIVÉS\n",
      "================================================================================\n",
      "\n",
      "✅ Création d'indicateurs terminée\n",
      "   Datasets enrichis: 0\n",
      "\n",
      "================================================================================\n",
      "📊 PHASE 7: AGRÉGATIONS TEMPORELLES ET SPATIALES\n",
      "================================================================================\n",
      "\n",
      "✅ geographic_cities: 1 types d'agrégation\n",
      "\n",
      "✅ geographic_admin_pays: 1 types d'agrégation\n",
      "\n",
      "✅ web_scraping: 1 types d'agrégation\n",
      "\n",
      "✅ economic_indicators: 1 types d'agrégation\n",
      "\n",
      "✅ geographic: 1 types d'agrégation\n",
      "\n",
      "✅ Agrégations terminées\n",
      "   Datasets agrégés: 5\n",
      "\n",
      "================================================================================\n",
      "✅ PIPELINE TERMINÉ AVEC SUCCÈS\n",
      "================================================================================\n",
      "\n",
      "📊 RÉSULTATS:\n",
      "   - Datasets analysés: 5\n",
      "   - Datasets enrichis: 0\n",
      "   - Tendances identifiées: 2\n",
      "   - Analyses spatiales: 3\n",
      "   - Matrices de corrélation: 5\n",
      "   - Anomalies détectées: 3\n",
      "\n",
      "📂 LIVRABLES:\n",
      "   - Données enrichies: data_task_2/enriched\n",
      "   - Analyses: data_task_2/analysis\n",
      "   - Rapports d'anomalies: data_task_2/anomalies\n",
      "   - Visualisations: data_task_2/visualizations\n",
      "   - Données agrégées: data_task_2/processed\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
