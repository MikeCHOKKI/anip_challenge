{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.234329Z",
     "start_time": "2025-10-17T21:58:32.535743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import functools\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, List, Tuple, Any, Union\n",
    "from dataclasses import dataclass, field\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, pearsonr, spearmanr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n"
   ],
   "id": "dec957620be239ce",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.258489Z",
     "start_time": "2025-10-17T21:58:33.249963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for analysis tasks.\n",
    "\n",
    "    Provides configurations such as country details, anomaly detection\n",
    "    thresholds, indicator configurations, and directory structure\n",
    "    required for data analysis tasks.\n",
    "\n",
    "    Attributes:\n",
    "        COUNTRY_CODE: ISO 3166-1 alpha-2 country code.\n",
    "        COUNTRY_NAME: Full country name.\n",
    "        ZSCORE_THRESHOLD: Threshold value for anomaly detection using Z-score.\n",
    "        IQR_MULTIPLIER: Multiplier value for anomaly detection using the\n",
    "            interquartile range (IQR) method.\n",
    "        CORRELATION_THRESHOLD: Threshold to identify significant\n",
    "            correlations between datasets.\n",
    "        POPULATION_AGE_YOUNG: Age defining the younger population boundary.\n",
    "        POPULATION_AGE_OLD: Age defining the older population boundary.\n",
    "        GDP_BASE_YEAR: Base year for GDP calculations.\n",
    "        DIRECTORY_STRUCTURE: A dictionary specifying paths for input, output,\n",
    "            processed, enriched, analysis, anomalies, visualizations,\n",
    "            and logs directories.\n",
    "    \"\"\"\n",
    "\n",
    "    COUNTRY_CODE: str = \"BJ\"\n",
    "    COUNTRY_NAME: str = \"B√©nin\"\n",
    "\n",
    "    ZSCORE_THRESHOLD: float = 3.0\n",
    "    IQR_MULTIPLIER: float = 1.5\n",
    "    CORRELATION_THRESHOLD: float = 0.7\n",
    "\n",
    "    POPULATION_AGE_YOUNG: int = 15\n",
    "    POPULATION_AGE_OLD: int = 65\n",
    "    GDP_BASE_YEAR: int = 2015\n",
    "\n",
    "    DIRECTORY_STRUCTURE: Dict[str, str] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"input\": \"data_task_1/final_data\",\n",
    "            \"output\": \"data_task_2\",\n",
    "            \"processed\": \"data_task_2/processed\",\n",
    "            \"enriched\": \"data_task_2/enriched\",\n",
    "            \"analysis\": \"data_task_2/analysis\",\n",
    "            \"anomalies\": \"data_task_2/anomalies\",\n",
    "            \"visualizations\": \"data_task_2/visualizations\",\n",
    "            \"logs\": \"logs_task_2\",\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "9fc5792ea2c7a95d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.314364Z",
     "start_time": "2025-10-17T21:58:33.305598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_analysis_environment(log_dir: Optional[Path] = None) -> None:\n",
    "    \"\"\"\n",
    "    Sets up the analysis environment with logging, display configurations for\n",
    "    pandas, and visualization settings for seaborn and matplotlib. Ensures a\n",
    "    consistent and user-friendly environment for data analysis and visualization.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If the specified log directory cannot be created or accessed.\n",
    "\n",
    "    Parameters:\n",
    "        log_dir (Optional[Path]): Path to the directory for saving log files.\n",
    "                                  If None, logging to a file is skipped.\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    log_format = \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\"\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=log_format,\n",
    "        datefmt=date_format,\n",
    "        handlers=[console_handler],\n",
    "    )\n",
    "\n",
    "    if log_dir:\n",
    "        log_dir = Path(log_dir)\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        file_handler = RotatingFileHandler(\n",
    "            log_dir / \"analysis.log\",\n",
    "            maxBytes=5_000_000,\n",
    "            backupCount=3,\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        file_handler.setFormatter(logging.Formatter(log_format, date_format))\n",
    "        logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "    pd.set_option(\"display.max_rows\", 100)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.float_format\", \"{:.3f}\".format)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"Set2\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"figure.figsize\": (14, 8),\n",
    "            \"axes.titlesize\": 14,\n",
    "            \"axes.labelsize\": 12,\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "5583c07e76a855c1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.373049Z",
     "start_time": "2025-10-17T21:58:33.363458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DirectoryManager:\n",
    "    \"\"\"\n",
    "    Handles directory structure management and initialization.\n",
    "\n",
    "    DirectoryManager is a utility class designed to manage and organize\n",
    "    a set of directories based on a predefined structure. It allows for\n",
    "    initializing directory structures, retrieving directory paths by\n",
    "    name, and logging activity. It can be configured using an AnalysisConfig\n",
    "    object or defaults to a predefined configuration.\n",
    "\n",
    "    Attributes:\n",
    "        base_dir (Path): The base directory where the directory structure\n",
    "            will be created. Defaults to the current working directory if\n",
    "            not specified.\n",
    "        config (AnalysisConfig): The configuration object containing the\n",
    "            directory structure specifications.\n",
    "        logger (Logger): Logger instance for logging activities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, base_dir: Optional[Path] = None, config: Optional[AnalysisConfig] = None\n",
    "    ):\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._directories: Dict[str, Path] = {}\n",
    "\n",
    "    def initialize_structure(self) -> Dict[str, Path]:\n",
    "        for name, path in self.config.DIRECTORY_STRUCTURE.items():\n",
    "            full_path = self.base_dir / path\n",
    "            full_path.mkdir(parents=True, exist_ok=True)\n",
    "            self._directories[name] = full_path\n",
    "\n",
    "        self.logger.info(f\"‚úÖ {len(self._directories)} r√©pertoires cr√©√©s\")\n",
    "        return self._directories\n",
    "\n",
    "    def get_path(self, name: str) -> Optional[Path]:\n",
    "        return self._directories.get(name)\n",
    "\n"
   ],
   "id": "3a574cd36d719766",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.424388Z",
     "start_time": "2025-10-17T21:58:33.416964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class AnomalyReport:\n",
    "    \"\"\"\n",
    "    Represents a report for detected anomalies in a dataset.\n",
    "\n",
    "    This class encapsulates details about anomalies identified in a specific dataset\n",
    "    and variable. It provides a structure to store the summary of anomalies including\n",
    "    anomaly count, percentage, type, and additional details. The timestamp is\n",
    "    automatically captured when an object of this class is instantiated.\n",
    "\n",
    "    Attributes:\n",
    "        dataset_name: Name of the dataset where anomalies are detected.\n",
    "        variable: The specific variable in the dataset associated with the anomalies.\n",
    "        anomaly_type: Type of the anomaly identified (e.g., outlier, missing data).\n",
    "        anomaly_count: Total number of anomalies detected.\n",
    "        anomaly_percentage: Percentage of anomalies relative to the total data points.\n",
    "        details: Additional information about the anomalies.\n",
    "        timestamp: The time and date when the anomaly report was created.\n",
    "\n",
    "    Methods:\n",
    "        to_dict:\n",
    "            Converts the anomaly report instance into a dictionary representation.\n",
    "            The dictionary includes dataset name, variable, anomaly type, count,\n",
    "            percentage, details, and timestamp for easier serialization or logging.\n",
    "    \"\"\"\n",
    "    dataset_name: str\n",
    "    variable: str\n",
    "    anomaly_type: str\n",
    "    anomaly_count: int\n",
    "    anomaly_percentage: float\n",
    "    details: Dict[str, Any] = field(default_factory=dict)\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"dataset\": self.dataset_name,\n",
    "            \"variable\": self.variable,\n",
    "            \"anomaly_type\": self.anomaly_type,\n",
    "            \"count\": self.anomaly_count,\n",
    "            \"percentage\": round(self.anomaly_percentage, 2),\n",
    "            \"details\": str(self.details),\n",
    "            \"timestamp\": self.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n"
   ],
   "id": "9f9e70b8b2154f86",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.483742Z",
     "start_time": "2025-10-17T21:58:33.471744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Class responsible for loading datasets from a specified directory of CSV files.\n",
    "\n",
    "    This class provides functionality for reading multiple CSV files in a given directory and\n",
    "    parsing them into pandas DataFrame objects. The primary purpose of the class is to facilitate\n",
    "    organized dataset loading along with maintaining logging for errors, warnings, and processing\n",
    "    information. It supports UTF-8 encoded files and processes only files with a .csv extension,\n",
    "    skipping any empty files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dir: Path):\n",
    "        self.input_dir = input_dir\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def load_all_datasets(self) -> Dict[str, pd.DataFrame]:\n",
    "        datasets = {}\n",
    "\n",
    "        if not self.input_dir.exists():\n",
    "            self.logger.error(f\"‚ùå R√©pertoire introuvable: {self.input_dir}\")\n",
    "            return datasets\n",
    "\n",
    "        csv_files = list(self.input_dir.glob(\"*.csv\"))\n",
    "\n",
    "        if not csv_files:\n",
    "            self.logger.warning(f\"‚ö†Ô∏è Aucun fichier CSV trouv√© dans {self.input_dir}\")\n",
    "            return datasets\n",
    "\n",
    "        self.logger.info(f\"üìÇ Chargement de {len(csv_files)} fichiers...\")\n",
    "\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                dataset_name = file_path.stem\n",
    "                df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    datasets[dataset_name] = df\n",
    "                    self.logger.info(\n",
    "                        f\"‚úÖ {dataset_name}: {len(df)} lignes, {len(df.columns)} colonnes\"\n",
    "                    )\n",
    "                else:\n",
    "                    self.logger.warning(f\"‚ö†Ô∏è {dataset_name}: fichier vide\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    f\"‚ùå Erreur lors du chargement de {file_path.name}: {e}\"\n",
    "                )\n",
    "\n",
    "        self.logger.info(f\"üìä Total: {len(datasets)} datasets charg√©s\")\n",
    "        return datasets\n",
    "\n"
   ],
   "id": "b2094d0a8b976485",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.551116Z",
     "start_time": "2025-10-17T21:58:33.529406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DescriptiveAnalyzer:\n",
    "    \"\"\"Performs comprehensive analysis of datasets, including descriptive analytics,\n",
    "    temporal and spatial variable identification, and summary reports.\n",
    "\n",
    "    This class is designed to provide an in-depth look into the structure and\n",
    "    content of datasets. It helps pinpoint missing data, identify variable types,\n",
    "    and generate summarized views of analytic results. Primarily used for\n",
    "    exploratory data analysis, the class includes methods for handling datasets\n",
    "    with temporal and spatial attributes. The results are returned in structured\n",
    "    formats for downstream usage.\n",
    "\n",
    "    Attributes:\n",
    "        config (Optional[AnalysisConfig]): Configuration object for specifying\n",
    "            analysis settings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_dataset(self, df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:\n",
    "        self.logger.info(f\"üìä Analyse descriptive: {dataset_name}\")\n",
    "\n",
    "        analysis: Dict[str, Any] = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"shape\": df.shape,\n",
    "            \"memory_usage_mb\": round(df.memory_usage(deep=True).sum() / (1024**2), 2),\n",
    "            \"columns\": list(df.columns),\n",
    "            \"dtypes\": df.dtypes.value_counts().to_dict(),\n",
    "        }\n",
    "\n",
    "        analysis[\"completeness\"] = {\n",
    "            \"total_cells\": df.size,\n",
    "            \"non_null_cells\": df.notna().sum().sum(),\n",
    "            \"null_cells\": df.isna().sum().sum(),\n",
    "            \"completeness_rate\": round(df.notna().sum().sum() / df.size * 100, 2),\n",
    "        }\n",
    "\n",
    "        column_stats = []\n",
    "        for col in df.columns:\n",
    "            stat = {\n",
    "                \"column\": col,\n",
    "                \"dtype\": str(df[col].dtype),\n",
    "                \"non_null\": int(df[col].notna().sum()),\n",
    "                \"null\": int(df[col].isna().sum()),\n",
    "                \"null_pct\": round(df[col].isna().sum() / len(df) * 100, 2),\n",
    "                \"unique\": int(df[col].nunique()),\n",
    "            }\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                stat.update(\n",
    "                    {\n",
    "                        \"min\": df[col].min(),\n",
    "                        \"max\": df[col].max(),\n",
    "                        \"mean\": round(df[col].mean(), 3),\n",
    "                        \"median\": df[col].median(),\n",
    "                        \"std\": round(df[col].std(), 3),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            column_stats.append(stat)\n",
    "\n",
    "        analysis[\"column_statistics\"] = pd.DataFrame(column_stats)\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def identify_temporal_variables(self, df: pd.DataFrame) -> List[str]:\n",
    "        temporal_vars = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if any(\n",
    "                keyword in col.lower() for keyword in [\"year\", \"ann√©e\", \"date\", \"time\"]\n",
    "            ):\n",
    "                temporal_vars.append(col)\n",
    "            elif df[col].dtype == \"datetime64[ns]\":\n",
    "                temporal_vars.append(col)\n",
    "\n",
    "        return temporal_vars\n",
    "\n",
    "    def identify_spatial_variables(self, df: pd.DataFrame) -> List[str]:\n",
    "        spatial_vars = []\n",
    "\n",
    "        spatial_keywords = [\n",
    "            \"r√©gion\",\n",
    "            \"region\",\n",
    "            \"d√©partement\",\n",
    "            \"department\",\n",
    "            \"commune\",\n",
    "            \"ville\",\n",
    "            \"city\",\n",
    "            \"localit√©\",\n",
    "            \"locality\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"admin\",\n",
    "        ]\n",
    "\n",
    "        for col in df.columns:\n",
    "            if any(keyword in col.lower() for keyword in spatial_keywords):\n",
    "                spatial_vars.append(col)\n",
    "\n",
    "        return spatial_vars\n",
    "\n",
    "    def generate_summary_report(\n",
    "        self, analyses: Dict[str, Dict[str, Any]]\n",
    "    ) -> pd.DataFrame:\n",
    "        summary_data = []\n",
    "\n",
    "        for dataset_name, analysis in analyses.items():\n",
    "            summary_data.append(\n",
    "                {\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"rows\": analysis[\"shape\"][0],\n",
    "                    \"columns\": analysis[\"shape\"][1],\n",
    "                    \"memory_mb\": analysis[\"memory_usage_mb\"],\n",
    "                    \"completeness_rate\": analysis[\"completeness\"][\"completeness_rate\"],\n",
    "                    \"null_cells\": analysis[\"completeness\"][\"null_cells\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n"
   ],
   "id": "f1d0617c72b93e45",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.588630Z",
     "start_time": "2025-10-17T21:58:33.576743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrendAnalyzer:\n",
    "    \"\"\"\n",
    "    TrendAnalyzer\n",
    "\n",
    "    The TrendAnalyzer class provides tools to analyze temporal trends and calculate\n",
    "    growth rates from data. It is designed to operate on pandas DataFrame objects\n",
    "    and assists in processing time-series data for meaningful insights. It features\n",
    "    capabilities such as temporal aggregation, growth rate computations, and trend\n",
    "    analysis across multiple value columns.\n",
    "\n",
    "    Attributes:\n",
    "        config (Optional[AnalysisConfig]): Configuration object for analysis.\n",
    "        logger (logging.Logger): Logger instance for the class, utilized for\n",
    "        logging informational messages and warnings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_temporal_trends(\n",
    "        self, df: pd.DataFrame, time_col: str, value_cols: List[str]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        self.logger.info(f\"üìà Analyse des tendances temporelles\")\n",
    "\n",
    "        trends = {}\n",
    "\n",
    "        for value_col in value_cols:\n",
    "            if value_col not in df.columns or not pd.api.types.is_numeric_dtype(\n",
    "                df[value_col]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                trend_df = (\n",
    "                    df.groupby(time_col)[value_col]\n",
    "                    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                trend_df[\"pct_change\"] = trend_df[\"mean\"].pct_change() * 100\n",
    "                trend_df[\"cumulative_change\"] = (\n",
    "                    (trend_df[\"mean\"] / trend_df[\"mean\"].iloc[0]) - 1\n",
    "                ) * 100\n",
    "\n",
    "                trends[value_col] = trend_df\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"‚ö†Ô∏è Erreur tendance {value_col}: {e}\")\n",
    "\n",
    "        return trends\n",
    "\n",
    "    def calculate_growth_rates(\n",
    "        self, df: pd.DataFrame, time_col: str, value_col: str\n",
    "    ) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "        result = result.sort_values(time_col)\n",
    "\n",
    "        result[\"growth_rate\"] = result[value_col].pct_change() * 100\n",
    "\n",
    "        if len(result) > 1:\n",
    "            first_value = result[value_col].iloc[0]\n",
    "            last_value = result[value_col].iloc[-1]\n",
    "            n_periods = len(result) - 1\n",
    "\n",
    "            if first_value > 0 and last_value > 0:\n",
    "                cagr = (((last_value / first_value) ** (1 / n_periods)) - 1) * 100\n",
    "                result[\"cagr\"] = cagr\n",
    "\n",
    "        return result\n",
    "\n"
   ],
   "id": "da4b9620784a8042",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.633710Z",
     "start_time": "2025-10-17T21:58:33.624146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SpatialAnalyzer:\n",
    "    \"\"\"\n",
    "    Handles spatial data analysis tasks related to distributions and regional disparities.\n",
    "\n",
    "    The SpatialAnalyzer class provides methods for analyzing spatial distributions within a\n",
    "    DataFrame and calculating various metrics to identify regional disparities. It enables\n",
    "    users to compute descriptive statistics and evaluate inequality measures such as the Gini\n",
    "    coefficient for spatially grouped datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def analyze_spatial_distribution(\n",
    "        self, df: pd.DataFrame, spatial_col: str, value_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        self.logger.info(f\"üó∫Ô∏è Analyse spatiale sur {spatial_col}\")\n",
    "\n",
    "        spatial_stats = []\n",
    "\n",
    "        for value_col in value_cols:\n",
    "            if value_col not in df.columns or not pd.api.types.is_numeric_dtype(\n",
    "                df[value_col]\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                stats_by_location = (\n",
    "                    df.groupby(spatial_col)[value_col]\n",
    "                    .agg([\"count\", \"mean\", \"median\", \"std\", \"min\", \"max\", \"sum\"])\n",
    "                    .reset_index()\n",
    "                )\n",
    "\n",
    "                stats_by_location[\"variable\"] = value_col\n",
    "                stats_by_location[\"cv\"] = (\n",
    "                    stats_by_location[\"std\"] / stats_by_location[\"mean\"]\n",
    "                ) * 100\n",
    "\n",
    "                spatial_stats.append(stats_by_location)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"‚ö†Ô∏è Erreur spatiale {value_col}: {e}\")\n",
    "\n",
    "        if spatial_stats:\n",
    "            return pd.concat(spatial_stats, ignore_index=True)\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def calculate_regional_disparities(\n",
    "        self, df: pd.DataFrame, spatial_col: str, value_col: str\n",
    "    ) -> Dict[str, float]:\n",
    "        regional_means = df.groupby(spatial_col)[value_col].mean()\n",
    "\n",
    "        disparities = {\n",
    "            \"gini_coefficient\": self._calculate_gini(regional_means.values),\n",
    "            \"coefficient_variation\": (regional_means.std() / regional_means.mean())\n",
    "            * 100,\n",
    "            \"range_ratio\": (\n",
    "                regional_means.max() / regional_means.min()\n",
    "                if regional_means.min() > 0\n",
    "                else np.nan\n",
    "            ),\n",
    "            \"max_value\": regional_means.max(),\n",
    "            \"min_value\": regional_means.min(),\n",
    "            \"mean_value\": regional_means.mean(),\n",
    "        }\n",
    "\n",
    "        return disparities\n",
    "\n",
    "    def _calculate_gini(self, values: np.ndarray) -> float:\n",
    "        values = np.array(values)\n",
    "        values = values[~np.isnan(values)]\n",
    "\n",
    "        if len(values) == 0:\n",
    "            return np.nan\n",
    "\n",
    "        values = np.sort(values)\n",
    "        n = len(values)\n",
    "        index = np.arange(1, n + 1)\n",
    "\n",
    "        return (2 * np.sum(index * values)) / (n * np.sum(values)) - (n + 1) / n\n",
    "\n"
   ],
   "id": "4a95d3703c573058",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.691318Z",
     "start_time": "2025-10-17T21:58:33.681011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CorrelationAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes correlations within datasets and across multiple datasets.\n",
    "\n",
    "    The CorrelationAnalyzer class provides utilities for calculating correlation matrices,\n",
    "    identifying strong correlations, and analyzing relationships between datasets. It\n",
    "    supports various correlation methods and can work with user-defined configurations.\n",
    "\n",
    "    Attributes:\n",
    "        config (AnalysisConfig): Configuration used for correlation analysis.\n",
    "        logger (logging.Logger): Logger for tracking analysis process logs.\n",
    "\n",
    "    Methods:\n",
    "        calculate_correlations: Computes the correlation matrix for the numeric columns in a dataset.\n",
    "        find_strong_correlations: Identifies strong correlations meeting or exceeding a threshold\n",
    "                                   within a correlation matrix.\n",
    "        cross_dataset_correlation: Analyzes correlations between two datasets by merging them\n",
    "                                    on specified columns and examining their numeric columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def calculate_correlations(\n",
    "        self, df: pd.DataFrame, method: str = \"pearson\"\n",
    "    ) -> pd.DataFrame:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        if len(numeric_cols) < 2:\n",
    "            self.logger.warning(\n",
    "                \"‚ö†Ô∏è Pas assez de colonnes num√©riques pour la corr√©lation\"\n",
    "            )\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        if method == \"pearson\":\n",
    "            corr_matrix = df[numeric_cols].corr(method=\"pearson\")\n",
    "        elif method == \"spearman\":\n",
    "            corr_matrix = df[numeric_cols].corr(method=\"spearman\")\n",
    "        else:\n",
    "            corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "        return corr_matrix\n",
    "\n",
    "    def find_strong_correlations(\n",
    "        self, corr_matrix: pd.DataFrame, threshold: Optional[float] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        threshold = threshold or self.config.CORRELATION_THRESHOLD\n",
    "\n",
    "        strong_corr = []\n",
    "\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                var1 = corr_matrix.columns[i]\n",
    "                var2 = corr_matrix.columns[j]\n",
    "                corr_value = corr_matrix.iloc[i, j]\n",
    "\n",
    "                if abs(corr_value) >= threshold:\n",
    "                    strong_corr.append(\n",
    "                        {\n",
    "                            \"variable_1\": var1,\n",
    "                            \"variable_2\": var2,\n",
    "                            \"correlation\": round(corr_value, 3),\n",
    "                            \"strength\": (\n",
    "                                \"forte\" if abs(corr_value) >= 0.8 else \"mod√©r√©e\"\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if not strong_corr:\n",
    "            return pd.DataFrame(\n",
    "                columns=[\"variable_1\", \"variable_2\", \"correlation\", \"strength\"]\n",
    "            )\n",
    "\n",
    "        return pd.DataFrame(strong_corr).sort_values(\n",
    "            \"correlation\", key=abs, ascending=False\n",
    "        )\n",
    "\n",
    "    def cross_dataset_correlation(\n",
    "        self, df1: pd.DataFrame, df2: pd.DataFrame, merge_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        try:\n",
    "            merged = pd.merge(\n",
    "                df1, df2, on=merge_cols, how=\"inner\", suffixes=(\"_1\", \"_2\")\n",
    "            )\n",
    "\n",
    "            numeric_cols = merged.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "            if len(numeric_cols) >= 2:\n",
    "                return self.calculate_correlations(merged[numeric_cols])\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Erreur corr√©lation crois√©e: {e}\")\n",
    "\n",
    "        return pd.DataFrame()\n",
    "\n"
   ],
   "id": "d1e1280baa2160bf",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.749471Z",
     "start_time": "2025-10-17T21:58:33.736404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    Class for detecting anomalies in data using statistical methods.\n",
    "\n",
    "    This class provides methods to identify anomalies in datasets by applying Z-score and\n",
    "    IQR-based anomaly detection techniques. It also detects inconsistencies in data, such\n",
    "    as negative values in numerical columns or invalid year values outside a specified range.\n",
    "    The class is designed to handle datasets in the form of pandas DataFrames and can generate\n",
    "    reports summarizing detected anomalies.\n",
    "\n",
    "    Attributes:\n",
    "        config (AnalysisConfig): Configuration settings for the anomaly detection process.\n",
    "        anomaly_reports (List[AnomalyReport]): List of anomaly reports generated during\n",
    "            the detection process.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.anomaly_reports: List[AnomalyReport] = []\n",
    "\n",
    "    def detect_zscore_anomalies(\n",
    "        self, df: pd.DataFrame, dataset_name: str\n",
    "    ) -> pd.DataFrame:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        anomalies_df = pd.DataFrame()\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 3:\n",
    "                continue\n",
    "\n",
    "            z_scores = np.abs(zscore(df[col].dropna()))\n",
    "            anomaly_mask = z_scores > self.config.ZSCORE_THRESHOLD\n",
    "\n",
    "            if anomaly_mask.any():\n",
    "                anomaly_count = anomaly_mask.sum()\n",
    "                anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "\n",
    "                report = AnomalyReport(\n",
    "                    dataset_name=dataset_name,\n",
    "                    variable=col,\n",
    "                    anomaly_type=\"zscore\",\n",
    "                    anomaly_count=anomaly_count,\n",
    "                    anomaly_percentage=anomaly_pct,\n",
    "                    details={\"threshold\": self.config.ZSCORE_THRESHOLD},\n",
    "                )\n",
    "                self.anomaly_reports.append(report)\n",
    "\n",
    "        return anomalies_df\n",
    "\n",
    "    def detect_iqr_anomalies(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        anomalies_list = []\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if df[col].notna().sum() < 4:\n",
    "                continue\n",
    "\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            lower_bound = Q1 - self.config.IQR_MULTIPLIER * IQR\n",
    "            upper_bound = Q3 + self.config.IQR_MULTIPLIER * IQR\n",
    "\n",
    "            anomaly_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "\n",
    "            if anomaly_mask.any():\n",
    "                anomaly_count = anomaly_mask.sum()\n",
    "                anomaly_pct = (anomaly_count / len(df)) * 100\n",
    "\n",
    "                report = AnomalyReport(\n",
    "                    dataset_name=dataset_name,\n",
    "                    variable=col,\n",
    "                    anomaly_type=\"iqr\",\n",
    "                    anomaly_count=anomaly_count,\n",
    "                    anomaly_percentage=anomaly_pct,\n",
    "                    details={\n",
    "                        \"lower_bound\": lower_bound,\n",
    "                        \"upper_bound\": upper_bound,\n",
    "                        \"Q1\": Q1,\n",
    "                        \"Q3\": Q3,\n",
    "                        \"IQR\": IQR,\n",
    "                    },\n",
    "                )\n",
    "                self.anomaly_reports.append(report)\n",
    "\n",
    "                anomalies_list.append(\n",
    "                    {\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"variable\": col,\n",
    "                        \"anomaly_indices\": df[anomaly_mask].index.tolist(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return pd.DataFrame(anomalies_list) if anomalies_list else pd.DataFrame()\n",
    "\n",
    "    def detect_inconsistencies(\n",
    "        self, df: pd.DataFrame, dataset_name: str\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        inconsistencies = []\n",
    "\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            if any(\n",
    "                keyword in col.lower()\n",
    "                for keyword in [\"population\", \"count\", \"nombre\", \"effectif\"]\n",
    "            ):\n",
    "                negative_count = (df[col] < 0).sum()\n",
    "                if negative_count > 0:\n",
    "                    inconsistencies.append(\n",
    "                        {\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variable\": col,\n",
    "                            \"issue\": \"valeurs_n√©gatives\",\n",
    "                            \"count\": negative_count,\n",
    "                            \"description\": f\"{negative_count} valeurs n√©gatives pour une variable de comptage\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        for col in df.columns:\n",
    "            if \"year\" in col.lower() or \"ann√©e\" in col.lower():\n",
    "                invalid_years = df[(df[col] < 1900) | (df[col] > 2025)]\n",
    "                if len(invalid_years) > 0:\n",
    "                    inconsistencies.append(\n",
    "                        {\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variable\": col,\n",
    "                            \"issue\": \"ann√©es_invalides\",\n",
    "                            \"count\": len(invalid_years),\n",
    "                            \"description\": f\"{len(invalid_years)} ann√©es hors de la plage [1900-2025]\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return inconsistencies\n",
    "\n",
    "    def generate_anomaly_report(self) -> pd.DataFrame:\n",
    "        if not self.anomaly_reports:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        return pd.DataFrame([report.to_dict() for report in self.anomaly_reports])\n",
    "\n"
   ],
   "id": "52d617724840d60a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.803037Z",
     "start_time": "2025-10-17T21:58:33.788470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IndicatorBuilder:\n",
    "    \"\"\"\n",
    "    This class is responsible for constructing various socio-economic statistical indicators.\n",
    "\n",
    "    The IndicatorBuilder class provides methods to generate demographic, economic, education,\n",
    "    and composite indicators from a provided dataset. These indicators can be further used\n",
    "    for statistical analysis, regional development monitoring, or economic assessments. The\n",
    "    class allows customization through configuration settings.\n",
    "\n",
    "    Attributes:\n",
    "        config (AnalysisConfig): Configuration object for the builder, including settings\n",
    "            like base year for GDP calculations.\n",
    "        logger (Logger): Logger instance for capturing events during the indicator-building\n",
    "            process.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def build_demographic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        if \"population\" in df.columns or \"total_population\" in df.columns:\n",
    "            pop_col = \"population\" if \"population\" in df.columns else \"total_population\"\n",
    "\n",
    "            if \"year\" in df.columns or \"ann√©e\" in df.columns:\n",
    "                time_col = \"year\" if \"year\" in df.columns else \"ann√©e\"\n",
    "                result = result.sort_values(time_col)\n",
    "                result[\"population_growth_rate\"] = result[pop_col].pct_change() * 100\n",
    "\n",
    "        if \"population_0_14\" in df.columns and \"total_population\" in df.columns:\n",
    "            result[\"youth_population_ratio\"] = (\n",
    "                result[\"population_0_14\"] / result[\"total_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        if \"population_65_plus\" in df.columns and \"total_population\" in df.columns:\n",
    "            result[\"elderly_population_ratio\"] = (\n",
    "                result[\"population_65_plus\"] / result[\"total_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        if all(col in df.columns for col in [\"total_population\", \"surface_area_km2\"]):\n",
    "            result[\"population_density\"] = (\n",
    "                result[\"total_population\"] / result[\"surface_area_km2\"]\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"‚úÖ Indicateurs d√©mographiques cr√©√©s\")\n",
    "        return result\n",
    "\n",
    "    def build_economic_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        if all(col in df.columns for col in [\"gdp\", \"total_population\"]):\n",
    "            result[\"gdp_per_capita\"] = result[\"gdp\"] / result[\"total_population\"]\n",
    "\n",
    "        if \"gdp\" in df.columns:\n",
    "            if \"year\" in df.columns or \"ann√©e\" in df.columns:\n",
    "                time_col = \"year\" if \"year\" in df.columns else \"ann√©e\"\n",
    "                result = result.sort_values(time_col)\n",
    "                result[\"gdp_growth_rate\"] = result[\"gdp\"].pct_change() * 100\n",
    "\n",
    "        if \"gdp\" in df.columns and \"year\" in df.columns:\n",
    "            base_year = self.config.GDP_BASE_YEAR\n",
    "            base_gdp = (\n",
    "                result[result[\"year\"] == base_year][\"gdp\"].iloc[0]\n",
    "                if base_year in result[\"year\"].values\n",
    "                else result[\"gdp\"].iloc[0]\n",
    "            )\n",
    "            result[\"gdp_index\"] = (result[\"gdp\"] / base_gdp) * 100\n",
    "\n",
    "        self.logger.info(\"‚úÖ Indicateurs √©conomiques cr√©√©s\")\n",
    "        return result\n",
    "\n",
    "    def build_education_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        if all(\n",
    "            col in df.columns for col in [\"enrolled_students\", \"school_age_population\"]\n",
    "        ):\n",
    "            result[\"net_enrollment_rate\"] = (\n",
    "                result[\"enrolled_students\"] / result[\"school_age_population\"]\n",
    "            ) * 100\n",
    "\n",
    "        if all(col in df.columns for col in [\"total_students\", \"total_teachers\"]):\n",
    "            result[\"student_teacher_ratio\"] = (\n",
    "                result[\"total_students\"] / result[\"total_teachers\"]\n",
    "            )\n",
    "\n",
    "        self.logger.info(\"‚úÖ Indicateurs d'√©ducation cr√©√©s\")\n",
    "        return result\n",
    "\n",
    "    def build_composite_index(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        indicators: List[str],\n",
    "        weights: Optional[List[float]] = None,\n",
    "        index_name: str = \"composite_index\",\n",
    "    ) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        available_indicators = [ind for ind in indicators if ind in df.columns]\n",
    "\n",
    "        if not available_indicators:\n",
    "            self.logger.warning(f\"‚ö†Ô∏è Aucun indicateur disponible pour {index_name}\")\n",
    "            return result\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        normalized_data = scaler.fit_transform(df[available_indicators].fillna(0))\n",
    "        normalized_df = pd.DataFrame(\n",
    "            normalized_data, columns=available_indicators, index=df.index\n",
    "        )\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1 / len(available_indicators)] * len(available_indicators)\n",
    "\n",
    "        result[index_name] = sum(\n",
    "            normalized_df[ind] * weight\n",
    "            for ind, weight in zip(available_indicators, weights)\n",
    "        )\n",
    "\n",
    "        result[index_name] = (\n",
    "            (result[index_name] - result[index_name].min())\n",
    "            / (result[index_name].max() - result[index_name].min())\n",
    "        ) * 100\n",
    "\n",
    "        self.logger.info(f\"‚úÖ Indice composite '{index_name}' cr√©√©\")\n",
    "        return result\n",
    "\n",
    "    def build_regional_development_index(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        indicators = []\n",
    "\n",
    "        if \"gdp_per_capita\" in df.columns:\n",
    "            indicators.append(\"gdp_per_capita\")\n",
    "        if \"net_enrollment_rate\" in df.columns:\n",
    "            indicators.append(\"net_enrollment_rate\")\n",
    "        if \"life_expectancy\" in df.columns:\n",
    "            indicators.append(\"life_expectancy\")\n",
    "        if \"access_electricity\" in df.columns:\n",
    "            indicators.append(\"access_electricity\")\n",
    "\n",
    "        if indicators:\n",
    "            return self.build_composite_index(\n",
    "                df, indicators, index_name=\"regional_development_index\"\n",
    "            )\n",
    "\n",
    "        self.logger.warning(\"‚ö†Ô∏è Pas assez d'indicateurs pour l'indice de d√©veloppement\")\n",
    "        return df\n",
    "\n"
   ],
   "id": "ec3c5d7226443745",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.853229Z",
     "start_time": "2025-10-17T21:58:33.841225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AggregationEngine:\n",
    "    \"\"\"\n",
    "    Facilitates various types of data aggregation, including temporal, spatial, normalization\n",
    "    by population, and multi-level aggregation.\n",
    "\n",
    "    The class provides methods to manipulate, aggregate, and normalize data in structured\n",
    "    tabular formats, such as those handled by pandas DataFrames. Aggregation can be performed\n",
    "    based on specific time, spatial, or hierarchical grouping levels, with support for custom\n",
    "    aggregation functions. Additionally, normalization of values by population is available\n",
    "    to standardize metrics for better interpretability across datasets.\n",
    "\n",
    "    Attributes:\n",
    "        config: Optional AnalysisConfig instance to configure the behavior of the engine.\n",
    "        logger: Logging instance used for debugging and information purposes.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[AnalysisConfig] = None):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def temporal_aggregation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        time_col: str,\n",
    "        value_cols: List[str],\n",
    "        agg_functions: Optional[Dict[str, str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if agg_functions is None:\n",
    "            agg_functions = {col: \"mean\" for col in value_cols}\n",
    "\n",
    "        valid_agg = {\n",
    "            col: func\n",
    "            for col, func in agg_functions.items()\n",
    "            if col in df.columns and col != time_col\n",
    "        }\n",
    "\n",
    "        if not valid_agg:\n",
    "            self.logger.warning(\"‚ö†Ô∏è Aucune colonne valide pour l'agr√©gation\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        aggregated = df.groupby(time_col).agg(valid_agg).reset_index()\n",
    "\n",
    "        aggregated.columns = [\n",
    "            f\"{col}_{func}\" if col != time_col else col\n",
    "            for col, func in zip(\n",
    "                aggregated.columns, [time_col] + list(valid_agg.values())\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.logger.info(f\"‚úÖ Agr√©gation temporelle: {len(aggregated)} p√©riodes\")\n",
    "        return aggregated\n",
    "\n",
    "    def spatial_aggregation(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        spatial_col: str,\n",
    "        value_cols: List[str],\n",
    "        agg_functions: Optional[Dict[str, str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        if agg_functions is None:\n",
    "            agg_functions = {col: \"sum\" for col in value_cols}\n",
    "\n",
    "        valid_agg = {\n",
    "            col: func\n",
    "            for col, func in agg_functions.items()\n",
    "            if col in df.columns and col != spatial_col\n",
    "        }\n",
    "\n",
    "        if not valid_agg:\n",
    "            self.logger.warning(\"‚ö†Ô∏è Aucune colonne valide pour l'agr√©gation spatiale\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        aggregated = df.groupby(spatial_col).agg(valid_agg).reset_index()\n",
    "\n",
    "        self.logger.info(f\"‚úÖ Agr√©gation spatiale: {len(aggregated)} zones\")\n",
    "        return aggregated\n",
    "\n",
    "    def normalize_by_population(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        value_cols: List[str],\n",
    "        population_col: str = \"total_population\",\n",
    "    ) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "\n",
    "        if population_col not in df.columns:\n",
    "            self.logger.warning(f\"‚ö†Ô∏è Colonne {population_col} introuvable\")\n",
    "            return result\n",
    "\n",
    "        for col in value_cols:\n",
    "            if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                new_col_name = f\"{col}_per_capita\"\n",
    "                result[new_col_name] = result[col] / result[population_col]\n",
    "                self.logger.info(f\"‚úÖ Cr√©√©: {new_col_name}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def multi_level_aggregation(\n",
    "        self, df: pd.DataFrame, group_cols: List[str], value_cols: List[str]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        aggregations = {}\n",
    "\n",
    "        for i in range(1, len(group_cols) + 1):\n",
    "            level_cols = group_cols[:i]\n",
    "            level_name = \"_\".join(level_cols)\n",
    "\n",
    "            agg_dict = {\n",
    "                col: [\"sum\", \"mean\", \"count\"] for col in value_cols if col in df.columns\n",
    "            }\n",
    "\n",
    "            if agg_dict:\n",
    "                agg_result = df.groupby(level_cols).agg(agg_dict).reset_index()\n",
    "                agg_result.columns = [\n",
    "                    \"_\".join(col).strip(\"_\") for col in agg_result.columns\n",
    "                ]\n",
    "                aggregations[level_name] = agg_result\n",
    "                self.logger.info(f\"‚úÖ Agr√©gation niveau {i}: {len(agg_result)} groupes\")\n",
    "\n",
    "        return aggregations\n",
    "\n"
   ],
   "id": "6960b731be31e781",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:33.911587Z",
     "start_time": "2025-10-17T21:58:33.898063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VisualizationEngine:\n",
    "    \"\"\"\n",
    "    Handles the visualization of various datasets and trends.\n",
    "\n",
    "    This class provides methods to create and save visualizations for temporal trends, correlation matrices,\n",
    "    and spatial distributions. The class is designed to facilitate exploratory data analysis and better\n",
    "    understanding of data patterns. It takes care of rendering plots and saving them to a specified output\n",
    "    directory.\n",
    "\n",
    "    Attributes:\n",
    "        output_dir (Path): Directory where the plots will be saved.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def plot_temporal_trends(\n",
    "        self, trends: Dict[str, pd.DataFrame], time_col: str, save: bool = True\n",
    "    ) -> None:\n",
    "        n_plots = len(trends)\n",
    "        if n_plots == 0:\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(min(n_plots, 3), 1, figsize=(14, 4 * min(n_plots, 3)))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for idx, (var_name, trend_df) in enumerate(list(trends.items())[:3]):\n",
    "            ax = axes[idx]\n",
    "            ax.plot(\n",
    "                trend_df[time_col],\n",
    "                trend_df[\"mean\"],\n",
    "                marker=\"o\",\n",
    "                linewidth=2,\n",
    "                label=\"Moyenne\",\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                trend_df[time_col],\n",
    "                trend_df[\"mean\"] - trend_df[\"std\"],\n",
    "                trend_df[\"mean\"] + trend_df[\"std\"],\n",
    "                alpha=0.3,\n",
    "                label=\"¬±1 √©cart-type\",\n",
    "            )\n",
    "            ax.set_title(\n",
    "                f\"√âvolution temporelle: {var_name}\", fontsize=12, fontweight=\"bold\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Ann√©e\")\n",
    "            ax.set_ylabel(\"Valeur\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"temporal_trends.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"üíæ Graphique sauvegard√©: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_correlation_heatmap(\n",
    "        self, corr_matrix: pd.DataFrame, save: bool = True\n",
    "    ) -> None:\n",
    "        if corr_matrix.empty:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "        sns.heatmap(\n",
    "            corr_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        ax.set_title(\"Matrice de corr√©lation\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"correlation_heatmap.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"üíæ Heatmap sauvegard√©e: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_spatial_distribution(\n",
    "        self,\n",
    "        spatial_stats: pd.DataFrame,\n",
    "        spatial_col: str,\n",
    "        value_col: str = \"mean\",\n",
    "        save: bool = True,\n",
    "    ) -> None:\n",
    "        if spatial_stats.empty:\n",
    "            return\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        spatial_stats_sorted = spatial_stats.sort_values(\n",
    "            value_col, ascending=False\n",
    "        ).head(20)\n",
    "\n",
    "        bars = ax.barh(\n",
    "            spatial_stats_sorted[spatial_col], spatial_stats_sorted[value_col]\n",
    "        )\n",
    "\n",
    "        # Gradient de couleurs\n",
    "        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(bars)))\n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "\n",
    "        ax.set_xlabel(\"Valeur moyenne\")\n",
    "        ax.set_title(f\"Distribution spatiale (Top 20)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            filepath = self.output_dir / \"spatial_distribution.png\"\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            self.logger.info(f\"üíæ Distribution sauvegard√©e: {filepath.name}\")\n",
    "\n",
    "        plt.close()\n",
    "\n"
   ],
   "id": "fd0369ef7b7f916f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:34.039656Z",
     "start_time": "2025-10-17T21:58:33.950291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ExplorationOrchestrator:\n",
    "    \"\"\"\n",
    "    Handles orchestration of various data analysis phases including descriptive analysis,\n",
    "    trend analysis, spatial analysis, correlation analysis, and anomaly detection.\n",
    "\n",
    "    This class is responsible for coordinating the execution and management of different\n",
    "    analysis modules. It provides functionalities to analyze datasets in stages,\n",
    "    producing reports, visualizations, and other results as outputs for each phase. It\n",
    "    integrates functionalities such as data loading, managing file directories, and\n",
    "    logging progress during analysis.\n",
    "\n",
    "    Attributes:\n",
    "        config (Optional[AnalysisConfig]): Configuration settings for the analysis.\n",
    "        base_dir (Optional[Path]): Base directory for organization of analysis files and outputs.\n",
    "        logger: Logger instance for tracking execution details.\n",
    "        dir_manager: Manages directory structure for analysis input and output files.\n",
    "        directories: Dictionary containing paths to different directories for input, output, etc.\n",
    "        loader: Loader module for importing datasets.\n",
    "        descriptive_analyzer: Module for conducting descriptive statistics analysis.\n",
    "        trend_analyzer: Module for analyzing temporal trends in data.\n",
    "        spatial_analyzer: Module for analyzing spatial patterns in data.\n",
    "        correlation_analyzer: Module for detecting correlations and generating correlation matrices.\n",
    "        anomaly_detector: Module for identifying anomalies and inconsistencies within datasets.\n",
    "        indicator_builder: Module for building custom indicators or metrics.\n",
    "        aggregation_engine: Module for aggregating results and data.\n",
    "        viz_engine: Visualization engine for generating plots and visual insights.\n",
    "        results: Stores output of executed analysis stages.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, config: Optional[AnalysisConfig] = None, base_dir: Optional[Path] = None\n",
    "    ):\n",
    "        self.config = config or AnalysisConfig()\n",
    "        self.base_dir = base_dir or Path(\".\")\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Initialisation des composants\n",
    "        self.dir_manager = DirectoryManager(self.base_dir, self.config)\n",
    "        self.directories = self.dir_manager.initialize_structure()\n",
    "\n",
    "        self.loader = DataLoader(self.directories[\"input\"])\n",
    "        self.descriptive_analyzer = DescriptiveAnalyzer(self.config)\n",
    "        self.trend_analyzer = TrendAnalyzer(self.config)\n",
    "        self.spatial_analyzer = SpatialAnalyzer(self.config)\n",
    "        self.correlation_analyzer = CorrelationAnalyzer(self.config)\n",
    "        self.anomaly_detector = AnomalyDetector(self.config)\n",
    "        self.indicator_builder = IndicatorBuilder(self.config)\n",
    "        self.aggregation_engine = AggregationEngine(self.config)\n",
    "        self.viz_engine = VisualizationEngine(self.directories[\"visualizations\"])\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "    def run_descriptive_analysis(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        self.logger.info(\"üìä PHASE 1: ANALYSE DESCRIPTIVE\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä PHASE 1: ANALYSE DESCRIPTIVE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        analyses = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            self.logger.info(f\"‚ñ∂Ô∏è Analyse: {name}\")\n",
    "            analysis = self.descriptive_analyzer.analyze_dataset(df, name)\n",
    "            analyses[name] = analysis\n",
    "\n",
    "            if \"column_statistics\" in analysis:\n",
    "                stats_path = self.directories[\"analysis\"] / f\"{name}_column_stats.csv\"\n",
    "                analysis[\"column_statistics\"].to_csv(stats_path, index=False)\n",
    "\n",
    "        summary_report = self.descriptive_analyzer.generate_summary_report(analyses)\n",
    "        summary_path = self.directories[\"analysis\"] / \"descriptive_summary.csv\"\n",
    "        summary_report.to_csv(summary_path, index=False)\n",
    "\n",
    "        print(\"\\n‚úÖ Analyse descriptive termin√©e\")\n",
    "        print(f\"   Datasets analys√©s: {len(analyses)}\")\n",
    "        print(f\"   Rapport: {summary_path.name}\")\n",
    "\n",
    "        return analyses\n",
    "\n",
    "    def run_trend_analysis(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        self.logger.info(\"üìà PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìà PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        all_trends = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            temporal_vars = self.descriptive_analyzer.identify_temporal_variables(df)\n",
    "\n",
    "            if not temporal_vars:\n",
    "                continue\n",
    "\n",
    "            time_col = temporal_vars[0]\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            value_cols = [col for col in numeric_cols if col != time_col][:5]\n",
    "\n",
    "            if value_cols:\n",
    "                trends = self.trend_analyzer.analyze_temporal_trends(\n",
    "                    df, time_col, value_cols\n",
    "                )\n",
    "\n",
    "                if trends:\n",
    "                    all_trends[name] = trends\n",
    "\n",
    "                    for var, trend_df in trends.items():\n",
    "                        trend_path = (\n",
    "                            self.directories[\"analysis\"] / f\"{name}_{var}_trend.csv\"\n",
    "                        )\n",
    "                        trend_df.to_csv(trend_path, index=False)\n",
    "\n",
    "                    # Visualisation\n",
    "                    self.viz_engine.plot_temporal_trends(trends, time_col)\n",
    "\n",
    "        print(f\"\\n‚úÖ Analyse des tendances termin√©e\")\n",
    "        print(f\"   Datasets avec tendances: {len(all_trends)}\")\n",
    "\n",
    "        return all_trends\n",
    "\n",
    "    def run_spatial_analysis(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        self.logger.info(\"üó∫Ô∏è PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üó∫Ô∏è PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        spatial_results = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            spatial_vars = self.descriptive_analyzer.identify_spatial_variables(df)\n",
    "\n",
    "            if not spatial_vars:\n",
    "                continue\n",
    "\n",
    "            spatial_col = spatial_vars[0]\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            value_cols = [col for col in numeric_cols if col != spatial_col][:5]\n",
    "\n",
    "            if value_cols:\n",
    "                spatial_dist = self.spatial_analyzer.analyze_spatial_distribution(\n",
    "                    df, spatial_col, value_cols\n",
    "                )\n",
    "\n",
    "                if not spatial_dist.empty:\n",
    "                    spatial_results[name] = spatial_dist\n",
    "\n",
    "                    spatial_path = (\n",
    "                        self.directories[\"analysis\"] / f\"{name}_spatial_analysis.csv\"\n",
    "                    )\n",
    "                    spatial_dist.to_csv(spatial_path, index=False)\n",
    "\n",
    "                    # Visualisation\n",
    "                    self.viz_engine.plot_spatial_distribution(spatial_dist, spatial_col)\n",
    "\n",
    "        print(f\"\\n‚úÖ Analyse spatiale termin√©e\")\n",
    "        print(f\"   Datasets analys√©s spatialement: {len(spatial_results)}\")\n",
    "\n",
    "        return spatial_results\n",
    "\n",
    "    def run_correlation_analysis(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        self.logger.info(\"üîó PHASE 4: ANALYSE DES CORR√âLATIONS\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîó PHASE 4: ANALYSE DES CORR√âLATIONS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        correlations = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            corr_matrix = self.correlation_analyzer.calculate_correlations(df)\n",
    "\n",
    "            if not corr_matrix.empty:\n",
    "                correlations[name] = {\n",
    "                    \"matrix\": corr_matrix,\n",
    "                    \"strong_correlations\": self.correlation_analyzer.find_strong_correlations(\n",
    "                        corr_matrix\n",
    "                    ),\n",
    "                }\n",
    "\n",
    "                corr_path = self.directories[\"analysis\"] / f\"{name}_correlations.csv\"\n",
    "                corr_matrix.to_csv(corr_path)\n",
    "\n",
    "                strong_corr_path = (\n",
    "                    self.directories[\"analysis\"] / f\"{name}_strong_correlations.csv\"\n",
    "                )\n",
    "                if not correlations[name][\"strong_correlations\"].empty:\n",
    "                    correlations[name][\"strong_correlations\"].to_csv(\n",
    "                        strong_corr_path, index=False\n",
    "                    )\n",
    "\n",
    "                self.viz_engine.plot_correlation_heatmap(corr_matrix)\n",
    "\n",
    "        print(f\"\\n‚úÖ Analyse des corr√©lations termin√©e\")\n",
    "        print(f\"   Datasets analys√©s: {len(correlations)}\")\n",
    "\n",
    "        return correlations\n",
    "\n",
    "    def run_anomaly_detection(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, Any]:\n",
    "        self.logger.info(\"üîç PHASE 5: D√âTECTION DES ANOMALIES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîç PHASE 5: D√âTECTION DES ANOMALIES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        all_anomalies = {}\n",
    "        all_inconsistencies = []\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            self.anomaly_detector.detect_zscore_anomalies(df, name)\n",
    "\n",
    "            iqr_anomalies = self.anomaly_detector.detect_iqr_anomalies(df, name)\n",
    "\n",
    "            inconsistencies = self.anomaly_detector.detect_inconsistencies(df, name)\n",
    "\n",
    "            if not iqr_anomalies.empty:\n",
    "                all_anomalies[name] = iqr_anomalies\n",
    "\n",
    "            if inconsistencies:\n",
    "                all_inconsistencies.extend(inconsistencies)\n",
    "\n",
    "        anomaly_report = self.anomaly_detector.generate_anomaly_report()\n",
    "        if not anomaly_report.empty:\n",
    "            anomaly_path = self.directories[\"anomalies\"] / \"anomaly_report.csv\"\n",
    "            anomaly_report.to_csv(anomaly_path, index=False)\n",
    "            print(f\"\\nüìÑ Rapport d'anomalies: {anomaly_path.name}\")\n",
    "            print(f\"   Total anomalies d√©tect√©es: {len(anomaly_report)}\")\n",
    "\n",
    "        if all_inconsistencies:\n",
    "            incon_df = pd.DataFrame(all_inconsistencies)\n",
    "            incon_path = self.directories[\"anomalies\"] / \"inconsistencies_report.csv\"\n",
    "            incon_df.to_csv(incon_path, index=False)\n",
    "            print(f\"üìÑ Rapport d'incoh√©rences: {incon_path.name}\")\n",
    "            print(f\"   Total incoh√©rences: {len(all_inconsistencies)}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ D√©tection des anomalies termin√©e\")\n",
    "\n",
    "        return {\n",
    "            \"anomalies\": all_anomalies,\n",
    "            \"anomaly_report\": anomaly_report,\n",
    "            \"inconsistencies\": all_inconsistencies,\n",
    "        }\n",
    "\n",
    "    def run_indicator_creation(\n",
    "        self, datasets: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "        self.logger.info(\"üîß PHASE 6: CR√âATION D'INDICATEURS D√âRIV√âS\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîß PHASE 6: CR√âATION D'INDICATEURS D√âRIV√âS\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        enriched_datasets = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            enriched = df.copy()\n",
    "\n",
    "            enriched = self.indicator_builder.build_demographic_indicators(enriched)\n",
    "\n",
    "            enriched = self.indicator_builder.build_economic_indicators(enriched)\n",
    "\n",
    "            enriched = self.indicator_builder.build_education_indicators(enriched)\n",
    "\n",
    "            enriched = self.indicator_builder.build_regional_development_index(enriched)\n",
    "\n",
    "            new_cols = set(enriched.columns) - set(df.columns)\n",
    "\n",
    "            if new_cols:\n",
    "                enriched_datasets[name] = enriched\n",
    "\n",
    "                enriched_path = self.directories[\"enriched\"] / f\"{name}_enriched.csv\"\n",
    "                enriched.to_csv(enriched_path, index=False)\n",
    "\n",
    "                print(f\"\\n‚úÖ {name}: {len(new_cols)} nouveaux indicateurs\")\n",
    "                for col in sorted(new_cols):\n",
    "                    print(f\"   - {col}\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Cr√©ation d'indicateurs termin√©e\")\n",
    "        print(f\"   Datasets enrichis: {len(enriched_datasets)}\")\n",
    "\n",
    "        return enriched_datasets\n",
    "\n",
    "    def run_aggregations(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, Any]:\n",
    "        self.logger.info(\"üìä PHASE 7: AGR√âGATIONS TEMPORELLES ET SPATIALES\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä PHASE 7: AGR√âGATIONS TEMPORELLES ET SPATIALES\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        aggregations = {}\n",
    "\n",
    "        for name, df in datasets.items():\n",
    "            dataset_aggs = {}\n",
    "\n",
    "            temporal_vars = self.descriptive_analyzer.identify_temporal_variables(df)\n",
    "            if temporal_vars:\n",
    "                time_col = temporal_vars[0]\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols if col != time_col][:10]\n",
    "\n",
    "                if value_cols:\n",
    "                    temp_agg = self.aggregation_engine.temporal_aggregation(\n",
    "                        df, time_col, value_cols\n",
    "                    )\n",
    "                    if not temp_agg.empty:\n",
    "                        dataset_aggs[\"temporal\"] = temp_agg\n",
    "                        temp_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_temporal_agg.csv\"\n",
    "                        )\n",
    "                        temp_agg.to_csv(temp_path, index=False)\n",
    "\n",
    "            spatial_vars = self.descriptive_analyzer.identify_spatial_variables(df)\n",
    "            if spatial_vars:\n",
    "                spatial_col = spatial_vars[0]\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols][:10]\n",
    "\n",
    "                if value_cols:\n",
    "                    spatial_agg = self.aggregation_engine.spatial_aggregation(\n",
    "                        df, spatial_col, value_cols\n",
    "                    )\n",
    "                    if not spatial_agg.empty:\n",
    "                        dataset_aggs[\"spatial\"] = spatial_agg\n",
    "                        spatial_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_spatial_agg.csv\"\n",
    "                        )\n",
    "                        spatial_agg.to_csv(spatial_path, index=False)\n",
    "\n",
    "            if \"total_population\" in df.columns or \"population\" in df.columns:\n",
    "                pop_col = (\n",
    "                    \"total_population\"\n",
    "                    if \"total_population\" in df.columns\n",
    "                    else \"population\"\n",
    "                )\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "                value_cols = [col for col in numeric_cols if col != pop_col][:5]\n",
    "\n",
    "                if value_cols:\n",
    "                    normalized = self.aggregation_engine.normalize_by_population(\n",
    "                        df, value_cols, pop_col\n",
    "                    )\n",
    "                    new_cols = [\n",
    "                        col for col in normalized.columns if \"_per_capita\" in col\n",
    "                    ]\n",
    "\n",
    "                    if new_cols:\n",
    "                        dataset_aggs[\"per_capita\"] = normalized[new_cols + [pop_col]]\n",
    "                        norm_path = (\n",
    "                            self.directories[\"processed\"] / f\"{name}_per_capita.csv\"\n",
    "                        )\n",
    "                        normalized.to_csv(norm_path, index=False)\n",
    "\n",
    "            if dataset_aggs:\n",
    "                aggregations[name] = dataset_aggs\n",
    "                print(f\"\\n‚úÖ {name}: {len(dataset_aggs)} types d'agr√©gation\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Agr√©gations termin√©es\")\n",
    "        print(f\"   Datasets agr√©g√©s: {len(aggregations)}\")\n",
    "\n",
    "        return aggregations\n",
    "\n",
    "    def generate_methodology_document(self) -> pd.DataFrame:\n",
    "        self.logger.info(\"üìù G√©n√©ration de la documentation m√©thodologique\")\n",
    "\n",
    "        methodology = []\n",
    "\n",
    "        anomaly_report = self.anomaly_detector.generate_anomaly_report()\n",
    "        if not anomaly_report.empty:\n",
    "            for _, row in anomaly_report.iterrows():\n",
    "                methodology.append(\n",
    "                    {\n",
    "                        \"category\": \"Anomalie\",\n",
    "                        \"dataset\": row[\"dataset\"],\n",
    "                        \"variable\": row[\"variable\"],\n",
    "                        \"type\": row[\"anomaly_type\"],\n",
    "                        \"action\": f\"{row['count']} valeurs d√©tect√©es ({row['percentage']}%)\",\n",
    "                        \"justification\": f\"Seuil: {self.config.ZSCORE_THRESHOLD if row['anomaly_type'] == 'zscore' else self.config.IQR_MULTIPLIER}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"population_growth_rate\",\n",
    "                \"type\": \"D√©riv√©\",\n",
    "                \"action\": \"Taux de croissance calcul√©\",\n",
    "                \"justification\": \"Variation annuelle en pourcentage\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"gdp_per_capita\",\n",
    "                \"type\": \"Ratio\",\n",
    "                \"action\": \"PIB / Population\",\n",
    "                \"justification\": \"Normalisation √©conomique\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Indicateur\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"regional_development_index\",\n",
    "                \"type\": \"Composite\",\n",
    "                \"action\": \"Indice multi-dimensionnel\",\n",
    "                \"justification\": \"Combinaison normalis√©e de plusieurs indicateurs\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Agr√©gation\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"temporal_aggregation\",\n",
    "                \"type\": \"Temporelle\",\n",
    "                \"action\": \"Agr√©gation par ann√©e\",\n",
    "                \"justification\": \"Analyse des tendances historiques\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology.append(\n",
    "            {\n",
    "                \"category\": \"Agr√©gation\",\n",
    "                \"dataset\": \"G√©n√©ral\",\n",
    "                \"variable\": \"spatial_aggregation\",\n",
    "                \"type\": \"Spatiale\",\n",
    "                \"action\": \"Agr√©gation par r√©gion\",\n",
    "                \"justification\": \"Comparaisons g√©ographiques\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        methodology_df = pd.DataFrame(methodology)\n",
    "\n",
    "        method_path = self.directories[\"analysis\"] / \"methodology_documentation.csv\"\n",
    "        methodology_df.to_csv(method_path, index=False)\n",
    "\n",
    "        self.logger.info(f\"‚úÖ Documentation: {method_path.name}\")\n",
    "\n",
    "        return methodology_df\n",
    "\n",
    "    def run_complete_analysis(self) -> Dict[str, Any]:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üöÄ D√âMARRAGE PIPELINE ANALYSE COMPL√àTE - T√ÇCHE 2\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        datasets = self.loader.load_all_datasets()\n",
    "\n",
    "        if not datasets:\n",
    "            self.logger.error(\n",
    "                \"‚ùå Aucune donn√©e charg√©e. V√©rifiez le r√©pertoire d'entr√©e.\"\n",
    "            )\n",
    "            return {}\n",
    "\n",
    "        descriptive_results = self.run_descriptive_analysis(datasets)\n",
    "\n",
    "        trend_results = self.run_trend_analysis(datasets)\n",
    "\n",
    "        spatial_results = self.run_spatial_analysis(datasets)\n",
    "\n",
    "        correlation_results = self.run_correlation_analysis(datasets)\n",
    "\n",
    "        anomaly_results = self.run_anomaly_detection(datasets)\n",
    "\n",
    "        enriched_datasets = self.run_indicator_creation(datasets)\n",
    "\n",
    "        aggregation_results = self.run_aggregations(\n",
    "            enriched_datasets if enriched_datasets else datasets\n",
    "        )\n",
    "\n",
    "        methodology_doc = self.generate_methodology_document()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nüìä R√âSULTATS:\")\n",
    "        print(f\"   - Datasets analys√©s: {len(datasets)}\")\n",
    "        print(f\"   - Datasets enrichis: {len(enriched_datasets)}\")\n",
    "        print(f\"   - Tendances identifi√©es: {len(trend_results)}\")\n",
    "        print(f\"   - Analyses spatiales: {len(spatial_results)}\")\n",
    "        print(f\"   - Matrices de corr√©lation: {len(correlation_results)}\")\n",
    "        print(\n",
    "            f\"   - Anomalies d√©tect√©es: {len(anomaly_results.get('anomaly_report', []))}\"\n",
    "        )\n",
    "        print(f\"\\nüìÇ LIVRABLES:\")\n",
    "        print(f\"   - Donn√©es enrichies: {self.directories['enriched']}\")\n",
    "        print(f\"   - Analyses: {self.directories['analysis']}\")\n",
    "        print(f\"   - Rapports d'anomalies: {self.directories['anomalies']}\")\n",
    "        print(f\"   - Visualisations: {self.directories['visualizations']}\")\n",
    "        print(f\"   - Donn√©es agr√©g√©es: {self.directories['processed']}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"datasets\": datasets,\n",
    "            \"descriptive_analysis\": descriptive_results,\n",
    "            \"trends\": trend_results,\n",
    "            \"spatial_analysis\": spatial_results,\n",
    "            \"correlations\": correlation_results,\n",
    "            \"anomalies\": anomaly_results,\n",
    "            \"enriched_datasets\": enriched_datasets,\n",
    "            \"aggregations\": aggregation_results,\n",
    "            \"methodology\": methodology_doc,\n",
    "        }\n",
    "\n"
   ],
   "id": "99a25c5038dd1f86",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:34.047380Z",
     "start_time": "2025-10-17T21:58:34.044656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to set up the environment, initialize the orchestrator, and execute\n",
    "    a complete analysis.\n",
    "\n",
    "    Return:\n",
    "        The results of the complete analysis.\n",
    "\n",
    "    \"\"\"\n",
    "    setup_analysis_environment(log_dir=Path(\"logs_task_2\"))\n",
    "\n",
    "    orchestrator = ExplorationOrchestrator()\n",
    "\n",
    "    results = orchestrator.run_complete_analysis()\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "id": "ac46019f024e2d22",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T21:58:41.459413Z",
     "start_time": "2025-10-17T21:58:34.091879Z"
    }
   },
   "cell_type": "code",
   "source": "_ = main()\n",
   "id": "a929ff10c7e56e5f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:58:34 | INFO     | DirectoryManager | ‚úÖ 8 r√©pertoires cr√©√©s\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | üìÇ Chargement de 5 fichiers...\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ‚úÖ geographic_cities: 3172 lignes, 7 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ‚úÖ geographic_admin_pays: 1 lignes, 8 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ‚úÖ web_scraping: 148 lignes, 7 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ‚úÖ economic_indicators: 59 lignes, 8 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | ‚úÖ geographic: 3173 lignes, 7 colonnes\n",
      "2025-10-17 22:58:34 | INFO     | DataLoader | üìä Total: 5 datasets charg√©s\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | üìä PHASE 1: ANALYSE DESCRIPTIVE\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ‚ñ∂Ô∏è Analyse: geographic_cities\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | üìä Analyse descriptive: geographic_cities\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ‚ñ∂Ô∏è Analyse: geographic_admin_pays\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | üìä Analyse descriptive: geographic_admin_pays\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ‚ñ∂Ô∏è Analyse: web_scraping\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | üìä Analyse descriptive: web_scraping\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ‚ñ∂Ô∏è Analyse: economic_indicators\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | üìä Analyse descriptive: economic_indicators\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | ‚ñ∂Ô∏è Analyse: geographic\n",
      "2025-10-17 22:58:34 | INFO     | DescriptiveAnalyzer | üìä Analyse descriptive: geographic\n",
      "2025-10-17 22:58:34 | INFO     | __main__ | üìà PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\n",
      "2025-10-17 22:58:34 | INFO     | TrendAnalyzer | üìà Analyse des tendances temporelles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ D√âMARRAGE PIPELINE ANALYSE COMPL√àTE - T√ÇCHE 2\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìä PHASE 1: ANALYSE DESCRIPTIVE\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Analyse descriptive termin√©e\n",
      "   Datasets analys√©s: 5\n",
      "   Rapport: descriptive_summary.csv\n",
      "\n",
      "================================================================================\n",
      "üìà PHASE 2: ANALYSE DES TENDANCES TEMPORELLES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:58:34 | INFO     | VisualizationEngine | üíæ Graphique sauvegard√©: temporal_trends.png\n",
      "2025-10-17 22:58:34 | INFO     | TrendAnalyzer | üìà Analyse des tendances temporelles\n",
      "2025-10-17 22:58:35 | INFO     | VisualizationEngine | üíæ Graphique sauvegard√©: temporal_trends.png\n",
      "2025-10-17 22:58:35 | INFO     | __main__ | üó∫Ô∏è PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\n",
      "2025-10-17 22:58:35 | INFO     | SpatialAnalyzer | üó∫Ô∏è Analyse spatiale sur latitude\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Analyse des tendances termin√©e\n",
      "   Datasets avec tendances: 2\n",
      "\n",
      "================================================================================\n",
      "üó∫Ô∏è PHASE 3: ANALYSE DES DYNAMIQUES SPATIALES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:58:35 | INFO     | VisualizationEngine | üíæ Distribution sauvegard√©e: spatial_distribution.png\n",
      "2025-10-17 22:58:35 | INFO     | SpatialAnalyzer | üó∫Ô∏è Analyse spatiale sur latitude\n",
      "2025-10-17 22:58:36 | INFO     | VisualizationEngine | üíæ Distribution sauvegard√©e: spatial_distribution.png\n",
      "2025-10-17 22:58:36 | INFO     | SpatialAnalyzer | üó∫Ô∏è Analyse spatiale sur latitude\n",
      "2025-10-17 22:58:37 | INFO     | VisualizationEngine | üíæ Distribution sauvegard√©e: spatial_distribution.png\n",
      "2025-10-17 22:58:37 | INFO     | __main__ | üîó PHASE 4: ANALYSE DES CORR√âLATIONS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Analyse spatiale termin√©e\n",
      "   Datasets analys√©s spatialement: 3\n",
      "\n",
      "================================================================================\n",
      "üîó PHASE 4: ANALYSE DES CORR√âLATIONS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 22:58:38 | INFO     | VisualizationEngine | üíæ Heatmap sauvegard√©e: correlation_heatmap.png\n",
      "2025-10-17 22:58:38 | INFO     | VisualizationEngine | üíæ Heatmap sauvegard√©e: correlation_heatmap.png\n",
      "2025-10-17 22:58:39 | INFO     | VisualizationEngine | üíæ Heatmap sauvegard√©e: correlation_heatmap.png\n",
      "2025-10-17 22:58:40 | INFO     | VisualizationEngine | üíæ Heatmap sauvegard√©e: correlation_heatmap.png\n",
      "2025-10-17 22:58:41 | INFO     | VisualizationEngine | üíæ Heatmap sauvegard√©e: correlation_heatmap.png\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | üîç PHASE 5: D√âTECTION DES ANOMALIES\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | üîß PHASE 6: CR√âATION D'INDICATEURS D√âRIV√âS\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d√©mographiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs √©conomiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d'√©ducation cr√©√©s\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ‚ö†Ô∏è Pas assez d'indicateurs pour l'indice de d√©veloppement\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d√©mographiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs √©conomiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d'√©ducation cr√©√©s\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ‚ö†Ô∏è Pas assez d'indicateurs pour l'indice de d√©veloppement\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d√©mographiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs √©conomiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d'√©ducation cr√©√©s\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ‚ö†Ô∏è Pas assez d'indicateurs pour l'indice de d√©veloppement\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d√©mographiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs √©conomiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d'√©ducation cr√©√©s\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ‚ö†Ô∏è Pas assez d'indicateurs pour l'indice de d√©veloppement\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d√©mographiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs √©conomiques cr√©√©s\n",
      "2025-10-17 22:58:41 | INFO     | IndicatorBuilder | ‚úÖ Indicateurs d'√©ducation cr√©√©s\n",
      "2025-10-17 22:58:41 | WARNING  | IndicatorBuilder | ‚ö†Ô∏è Pas assez d'indicateurs pour l'indice de d√©veloppement\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | üìä PHASE 7: AGR√âGATIONS TEMPORELLES ET SPATIALES\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ‚úÖ Agr√©gation spatiale: 3172 zones\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ‚úÖ Agr√©gation spatiale: 1 zones\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ‚úÖ Agr√©gation temporelle: 1 p√©riodes\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ‚úÖ Agr√©gation temporelle: 10 p√©riodes\n",
      "2025-10-17 22:58:41 | INFO     | AggregationEngine | ‚úÖ Agr√©gation spatiale: 3173 zones\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | üìù G√©n√©ration de la documentation m√©thodologique\n",
      "2025-10-17 22:58:41 | INFO     | __main__ | ‚úÖ Documentation: methodology_documentation.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Analyse des corr√©lations termin√©e\n",
      "   Datasets analys√©s: 5\n",
      "\n",
      "================================================================================\n",
      "üîç PHASE 5: D√âTECTION DES ANOMALIES\n",
      "================================================================================\n",
      "\n",
      "üìÑ Rapport d'anomalies: anomaly_report.csv\n",
      "   Total anomalies d√©tect√©es: 3\n",
      "\n",
      "‚úÖ D√©tection des anomalies termin√©e\n",
      "\n",
      "================================================================================\n",
      "üîß PHASE 6: CR√âATION D'INDICATEURS D√âRIV√âS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Cr√©ation d'indicateurs termin√©e\n",
      "   Datasets enrichis: 0\n",
      "\n",
      "================================================================================\n",
      "üìä PHASE 7: AGR√âGATIONS TEMPORELLES ET SPATIALES\n",
      "================================================================================\n",
      "\n",
      "‚úÖ geographic_cities: 1 types d'agr√©gation\n",
      "\n",
      "‚úÖ geographic_admin_pays: 1 types d'agr√©gation\n",
      "\n",
      "‚úÖ web_scraping: 1 types d'agr√©gation\n",
      "\n",
      "‚úÖ economic_indicators: 1 types d'agr√©gation\n",
      "\n",
      "‚úÖ geographic: 1 types d'agr√©gation\n",
      "\n",
      "‚úÖ Agr√©gations termin√©es\n",
      "   Datasets agr√©g√©s: 5\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS\n",
      "================================================================================\n",
      "\n",
      "üìä R√âSULTATS:\n",
      "   - Datasets analys√©s: 5\n",
      "   - Datasets enrichis: 0\n",
      "   - Tendances identifi√©es: 2\n",
      "   - Analyses spatiales: 3\n",
      "   - Matrices de corr√©lation: 5\n",
      "   - Anomalies d√©tect√©es: 3\n",
      "\n",
      "üìÇ LIVRABLES:\n",
      "   - Donn√©es enrichies: data_task_2/enriched\n",
      "   - Analyses: data_task_2/analysis\n",
      "   - Rapports d'anomalies: data_task_2/anomalies\n",
      "   - Visualisations: data_task_2/visualizations\n",
      "   - Donn√©es agr√©g√©es: data_task_2/processed\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
