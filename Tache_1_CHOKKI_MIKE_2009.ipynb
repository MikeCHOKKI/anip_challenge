{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# COLLECTE ET HARMONISATION DES DONNEES - REPUBLIQUE DU BENIN\n",
    "\n",
    "## Objectifs\n",
    "   - **Thématique**: Démographie - Economie - Santé - Education - Géographie\n",
    "   - **Sources**: INStad - Portails Open Data - Worl Bank API - Web scraping\n",
    "   - **Couverture**: Tous les départements du bénin\n",
    "   - **Période**: 2020 - 2025 (Selon la disponibilité des données au niveau des sources)\n",
    "### Python version: 3.13.7"
   ],
   "id": "29246f3e545590cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration des imports",
   "id": "b5604f4c399f266c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:41.937567Z",
     "start_time": "2025-09-20T13:07:41.905002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================\n",
    "# Imports\n",
    "# =============================\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import warnings\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from io import StringIO\n",
    "\n",
    "# Third-party\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =============================\n",
    "# Configurations globales\n",
    "# =============================\n",
    "\n",
    "# Warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "\n",
    "# Pandas\n",
    "pd.set_option(\"display.max_rows\", 100)  # nb max de lignes affichées\n",
    "pd.set_option(\"display.max_columns\", None)  # affiche toutes les colonnes\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)  # formatage des floats\n",
    "pd.set_option(\"display.expand_frame_repr\", False)  # évite retour ligne inutile\n",
    "\n",
    "# Matplotlib / Seaborn\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")  # style moderne et lisible\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# =============================\n",
    "# Check initialisation\n",
    "# =============================\n",
    "\n",
    "logging.info(\"Librairies et configurations chargées\")\n",
    "logging.info(\"Début de la collecte de données: %s\", datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n"
   ],
   "id": "f4878bcc29f9406b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 14:07:41,932 | INFO | Librairies et configurations chargées\n",
      "2025-09-20 14:07:41,934 | INFO | Début de la collecte de données: 20/09/2025 14:07:41\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration des dossiers",
   "id": "446520b4a707b964"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fonction de configurations de dossiers, réutilisable",
   "id": "d0bd16f57d830833"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:41.988968Z",
     "start_time": "2025-09-20T13:07:41.981066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_directory(base_dir: Optional[Path] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Initializes directory structure under the specified base directory.\n",
    "\n",
    "    Creates a directory structure for storing data, including raw data,\n",
    "    processed data, and final data. If directories already exist, their\n",
    "    existence is logged; otherwise, they are created.\n",
    "\n",
    "    :param base_dir: Optional base directory path. Defaults to the current\n",
    "        working directory if not provided.\n",
    "    :type base_dir: Optional[Path]\n",
    "    :return: A dictionary with the keys 'data', 'raw_data', 'processed_data',\n",
    "        and 'final_data', where each value is the Path object of the corresponding\n",
    "        directory.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    if base_dir is None:\n",
    "        base_dir = Path(\".\")\n",
    "\n",
    "    data_dir = base_dir / \"data\"  # Répertoire ou se trouvera les données\n",
    "    dirs = {\n",
    "        \"data\": data_dir,\n",
    "        \"raw_data\": data_dir / \"raw_data\",  # Répertoire des données brutes\n",
    "        \"processed_data\": data_dir / \"processed_data\",  # Répertoire des données traitées\n",
    "        \"final_data\": data_dir / \"final_data\",  # Répertoire des données finales\n",
    "    }\n",
    "\n",
    "    for name, path in dirs.items():\n",
    "        try:\n",
    "            if path.exists():\n",
    "                logging.info(f\"Dossier {name} existe déjà.\")\n",
    "            else:\n",
    "                path.mkdir(parents=True, exist_ok=True)\n",
    "                logging.info(f\"Répertoire {name} prêt à {path}\")\n",
    "        except PermissionError as pe:\n",
    "            logging.error(f\"Erreur de permission pour {name}: {pe}\")\n",
    "        except OSError as oe:\n",
    "            logging.error(f\"Erreur système pour {name}: {oe}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erreur inconnu pour {name}: {e}\")\n",
    "\n",
    "    return dirs"
   ],
   "id": "e4b827eb5f4f2c19",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Initialisation des dossiers",
   "id": "7f1368c76208bc00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:42.017300Z",
     "start_time": "2025-09-20T13:07:41.998266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR, RAW_DIR, PROCESSED_DIR, FINAL_DIR = init_directory().values()\n",
    "\n",
    "logging.info(\"Dossiers chargés avec succès\")"
   ],
   "id": "86164855e4155650",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 14:07:42,007 | INFO | Dossier data existe déjà.\n",
      "2025-09-20 14:07:42,008 | INFO | Dossier raw_data existe déjà.\n",
      "2025-09-20 14:07:42,009 | INFO | Dossier processed_data existe déjà.\n",
      "2025-09-20 14:07:42,010 | INFO | Dossier final_data existe déjà.\n",
      "2025-09-20 14:07:42,011 | INFO | Dossiers chargés avec succès\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Traitement des sources",
   "id": "c2c710eeab06e5bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### URL et sources API externes",
   "id": "8417157955813708"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:42.053310Z",
     "start_time": "2025-09-20T13:07:42.048282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Base URL pour l'API de la Banque Mondiale\n",
    "API_BASE_URL_WORLD_BANK = \"https://api.worldbank.org/v2\"\n",
    "\n",
    "# Base URL pour l'API d'Instad (service local ou national)\n",
    "API_BASE_URL_INSTAD = \"https://instad.bj\"\n",
    "\n",
    "# Base URL pour l'API Overpass (OpenStreetMap) pour requêtes sur les cartes\n",
    "API_BASE_URL_OPEN_STREET_MAP = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# Liste des URLs CSV provenant de sources externes potentielles\n",
    "# Ici un exemple : données UN Data via l'API UNSD (2015-2024)\n",
    "POTENTIAL_API_EXTERNAL_SOURCE_CSV_FILE = [\n",
    "    \"https://data.un.org/ws/rest/data/UNSD,DF_UNData_UNFCC,1.0/all/?startPeriod=2015&endPeriod=2024\"\n",
    "]"
   ],
   "id": "3529220150762d4c",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configuration supplémentaire",
   "id": "2f88d4b385b2ac2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:42.122188Z",
     "start_time": "2025-09-20T13:07:42.112786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "COUNTRY_CODE = \"BJ\"\n",
    "\n",
    "DEFAULT_INDICATOR_WORLD_BANK = [\n",
    "    'SP.POP.TOTL',  # Population totale\n",
    "    'NY.GDP.MKTP.CD',  # PIB\n",
    "    'SE.PRM.NENR',  # Scolarisation primaire\n",
    "    'SH.DYN.MORT',  # Taux de mortalité\n",
    "    'AG.LND.TOTL.K2',  # Surface totale\n",
    "    'NY.GDP.PCAP.CD',  # PIB par habitant\n",
    "    'SL.TLF.TOTL.IN',  # Force de travail\n",
    "    'SP.DYN.TFRT.IN'  # Taux de fertilité\n",
    "]"
   ],
   "id": "2adb2b9085eb537f",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Collecte WORLD BANK API",
   "id": "25dcb5c15ce90898"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### WorldBankAPI (mise à jour)\n",
    "\n",
    "Cette section documente la classe WorldBankAPI, utilisée pour interroger l’API de la Banque mondiale et collecter des séries d’indicateurs pour un pays donné sur une plage d’années. La méthode principale retourne un DataFrame normalisé, prêt pour des analyses ultérieures.\n",
    "\n",
    "- Aperçu:\n",
    "  - Interroge l’API World Bank pour des indicateurs économiques, démographiques, sociaux, etc.\n",
    "  - Standardise les colonnes de sortie: indicator_code, indicator_name, country_code, country_name, year, value, source, collection_date.\n",
    "  - Utilise une session HTTP persistante avec en-tête User-Agent défini.\n",
    "\n",
    "- Paramètres du constructeur (défauts actuels):\n",
    "  - url: base de l’API (par défaut: https://api.worldbank.org/v2).\n",
    "  - country_code: code pays ISO alpha-2 attendu par l’API (ex: BJ pour Bénin).\n",
    "  - start_year, end_year: bornes temporelles (par défaut: 2015–2024).\n",
    "  - default_per_page: taille de page demandée à l’API (par défaut: 100).\n",
    "\n",
    "- Méthode principale:\n",
    "  - get_indicators(indicators):\n",
    "    - Entrée: liste de codes d’indicateurs (ex: SP.POP.TOTL, NY.GDP.MKTP.CD).\n",
    "    - Traite chaque indicateur en appelant l’endpoint /country/{country_code}/indicator/{indicator}.\n",
    "    - Sortie: pandas.DataFrame enrichi avec les métadonnées, la source (“WORLD BANK API”) et la date de collecte du jour.\n",
    "\n",
    "- Comportements et bonnes pratiques intégrés:\n",
    "  - Temporisation de 0.5s entre requêtes pour limiter la charge.\n",
    "  - Conversion du champ year en numérique (coercition des valeurs invalides).\n",
    "  - Journalisation informative: début de collecte par indicateur, nombre d’enregistrements récupérés, erreurs HTTP/JSON.\n",
    "  - En-tête User-Agent explicite pour une identification claire côté API.\n",
    "\n",
    "- Limites connues (inchangées):\n",
    "  - La pagination n’est pas itérée au-delà de per_page (si > per_page, le surplus n’est pas récupéré).\n",
    "  - Valeurs manquantes possibles (null) dans value (retournées telles quelles).\n",
    "  - Gestion d’erreurs volontairement simple via logging (pas de retries/backoff).\n",
    "  - Délai réseau fixe (timeout=10s) non paramétrable via le constructeur.\n",
    "\n",
    "- Exemple rapide d’usage:\n",
    "  - Instanciation: world_bank = WorldBankAPI(country_code=\"BJ\", start_year=2015, end_year=2024)\n",
    "  - Collecte: df = world_bank.get_indicators([\"SP.POP.TOTL\", \"NY.GDP.MKTP.CD\"])\n",
    "  - Export: df.to_csv(\"data/raw_data/world_bank_data.csv\", index=False)\n"
   ],
   "id": "e0275c6bec67e7a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:42.146846Z",
     "start_time": "2025-09-20T13:07:42.130141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WorldBankAPI:\n",
    "    def __init__(self, url: str = API_BASE_URL_WORLD_BANK, country_code: str = COUNTRY_CODE, start_year: int = 2015,\n",
    "                 end_year: int = 2024, default_per_page: int = 100):\n",
    "        \"\"\"\n",
    "        Initializes the API request session with the given parameters for interacting\n",
    "        with the World Bank API. Allows setting a specific country, time range, and API\n",
    "        base URL. Configures default headers for the session to include a `User-Agent`\n",
    "        string customized for educational research purposes.\n",
    "\n",
    "        :param url: The base URL of the API to be used for requests.\n",
    "        :type url: str\n",
    "        :param country_code: The ISO 3166-1 alpha-3 country code for the target country.\n",
    "        :type country_code: str\n",
    "        :param start_year: The starting year for the data range.\n",
    "        :type start_year: int\n",
    "        :param end_year: The ending year for the data range.\n",
    "        :type end_year: int\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.country_code = country_code\n",
    "        self.start_year = start_year\n",
    "        self.end_year = end_year\n",
    "        self.per_page = default_per_page\n",
    "\n",
    "        self.session = requests.Session()\n",
    "\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Educational Research Purpose)'\n",
    "        })\n",
    "\n",
    "    def get_indicators(self, indicators: list = DEFAULT_INDICATOR_WORLD_BANK) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Récupère les données pour une liste d'indicateurs économiques ou financiers via l'API World Bank.\n",
    "        Les données sont accumulées dans un pandas DataFrame.\n",
    "\n",
    "        :param indicators: Liste des codes d'indicateurs à récupérer. Si non fournie, une liste par défaut est utilisée.\n",
    "        :type indicators: list\n",
    "        :return: DataFrame contenant les données récupérées, avec code et nom de l'indicateur, informations pays,\n",
    "                 année, valeur, source et date de collecte.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        donnees = []\n",
    "\n",
    "        for indicator in indicators:\n",
    "            logging.info(f\"Récupération des données pour l'indicateur {indicator}\")\n",
    "\n",
    "            url = f\"{self.url}/country/{self.country_code}/indicator/{indicator}\"\n",
    "            params = {\n",
    "                'date': f'{self.start_year}:{self.end_year}',\n",
    "                'format': 'json',\n",
    "                'per_page': self.per_page\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = self.session.get(url, params=params, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "\n",
    "                # Vérifie si des données sont disponibles\n",
    "                entries = data[1] if len(data) > 1 and data[1] else []\n",
    "                logging.info(f\"{len(entries)} enregistrements récupérés pour {indicator} \\n\")\n",
    "\n",
    "                for entry in entries:\n",
    "                    donnees.append({\n",
    "                        'indicator_code': entry['indicator']['id'],\n",
    "                        'indicator_name': entry['indicator']['value'],\n",
    "                        'country_code': entry['country']['id'],\n",
    "                        'country_name': entry['country']['value'],\n",
    "                        'year': entry['date'],\n",
    "                        'value': entry['value']\n",
    "                    })\n",
    "\n",
    "                # Pause pour éviter de saturer l'API\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(f\"Erreur HTTP pour l'indicateur {indicator}: {e}\")\n",
    "            except ValueError as e:\n",
    "                logging.error(f\"Erreur JSON pour l'indicateur {indicator}: {e}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erreur inattendue pour l'indicateur {indicator}: {e}\")\n",
    "\n",
    "        dataset = pd.DataFrame(donnees)\n",
    "\n",
    "        if not dataset.empty:\n",
    "            dataset['year'] = pd.to_numeric(dataset['year'], errors='coerce')\n",
    "            dataset['source'] = \"WORLD BANK API\"\n",
    "            dataset['collection_date'] = datetime.now().date()\n",
    "\n",
    "        return dataset\n"
   ],
   "id": "8666b7a94a010186",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Implémentation",
   "id": "4c189efbb7eda79f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:50.878640Z",
     "start_time": "2025-09-20T13:07:42.157017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "world_bank = WorldBankAPI()\n",
    "path_to_save = RAW_DIR / 'world_bank_data.csv'\n",
    "\n",
    "has_data = False\n",
    "world_bank_data = None\n",
    "\n",
    "if not Path(path_to_save).exists():\n",
    "    world_bank_data = world_bank.get_indicators()\n",
    "    has_data = len(world_bank_data) > 0\n",
    "    if has_data:\n",
    "        logging.info(f\"World Bank: {len(world_bank_data)} enrégistrement collectés\")\n",
    "    else:\n",
    "        logging.info(f\"World Bank: Aucun enrégistrement collectés\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")"
   ],
   "id": "722ceb689b37835f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 14:07:42,166 | INFO | Récupération des données pour l'indicateur SP.POP.TOTL\n",
      "2025-09-20 14:07:44,502 | INFO | 10 enregistrements récupérés pour SP.POP.TOTL \n",
      "\n",
      "2025-09-20 14:07:45,004 | INFO | Récupération des données pour l'indicateur NY.GDP.MKTP.CD\n",
      "2025-09-20 14:07:45,333 | INFO | 10 enregistrements récupérés pour NY.GDP.MKTP.CD \n",
      "\n",
      "2025-09-20 14:07:45,835 | INFO | Récupération des données pour l'indicateur SE.PRM.NENR\n",
      "2025-09-20 14:07:46,127 | INFO | 10 enregistrements récupérés pour SE.PRM.NENR \n",
      "\n",
      "2025-09-20 14:07:46,630 | INFO | Récupération des données pour l'indicateur SH.DYN.MORT\n",
      "2025-09-20 14:07:46,906 | INFO | 10 enregistrements récupérés pour SH.DYN.MORT \n",
      "\n",
      "2025-09-20 14:07:47,408 | INFO | Récupération des données pour l'indicateur AG.LND.TOTL.K2\n",
      "2025-09-20 14:07:47,969 | INFO | 10 enregistrements récupérés pour AG.LND.TOTL.K2 \n",
      "\n",
      "2025-09-20 14:07:48,474 | INFO | Récupération des données pour l'indicateur NY.GDP.PCAP.CD\n",
      "2025-09-20 14:07:48,771 | INFO | 10 enregistrements récupérés pour NY.GDP.PCAP.CD \n",
      "\n",
      "2025-09-20 14:07:49,277 | INFO | Récupération des données pour l'indicateur SL.TLF.TOTL.IN\n",
      "2025-09-20 14:07:49,580 | INFO | 10 enregistrements récupérés pour SL.TLF.TOTL.IN \n",
      "\n",
      "2025-09-20 14:07:50,084 | INFO | Récupération des données pour l'indicateur SP.DYN.TFRT.IN\n",
      "2025-09-20 14:07:50,360 | INFO | 10 enregistrements récupérés pour SP.DYN.TFRT.IN \n",
      "\n",
      "2025-09-20 14:07:50,870 | INFO | World Bank: 80 enrégistrement collectés\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Sauvegarde des données",
   "id": "f2dcf6414f128a09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:50.963368Z",
     "start_time": "2025-09-20T13:07:50.923315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if has_data and not world_bank_data.empty:\n",
    "    world_bank_data.to_csv(path_to_save, index=False)\n",
    "\n",
    "    # Taille du fichier en octets\n",
    "    size_bytes = path_to_save.stat().st_size\n",
    "\n",
    "    # Optionnel : convertir en Ko/Mo pour lecture facile\n",
    "    size_kb = size_bytes / 1024\n",
    "    size_mb = size_kb / 1024\n",
    "\n",
    "    logging.info(f\"Sauvegarde des données à {path_to_save} ({size_bytes} bytes / {size_kb:.2f} KB / {size_mb:.2f} MB)\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")"
   ],
   "id": "c0c2e7aeb4191ec2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 14:07:50,955 | INFO | Sauvegarde des données à data\\raw_data\\world_bank_data.csv (7597 bytes / 7.42 KB / 0.01 MB)\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Collecte INSTAD API - WEB SCRAPING",
   "id": "fb267cb1b32d8f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### INStadScraper\n",
    "\n",
    "Cette section documente la classe INStadScraper, utilisée pour extraire (web scraping) des tableaux HTML et des données JSON depuis le site de l’INStaD (https://instad.bj) et autres pages apparentées.\n",
    "\n",
    "Objectif\n",
    "- Automatiser la collecte de tableaux publiés sur des pages HTML (publications, indicateurs récents).\n",
    "- Normaliser les résultats sous forme de DataFrame pandas afin de faciliter l’analyse et l’export.\n",
    "- Enrichir les tables extraites avec des métadonnées: source_url, table_index, collection_date.\n",
    "\n",
    "Prérequis\n",
    "- Connexion Internet active.\n",
    "- Respect des conditions d’utilisation du site cible.\n",
    "- Bibliothèques: requests, pandas, BeautifulSoup (bs4).\n",
    "\n",
    "Paramètres\n",
    "- base_url (str): URL de base du site INStaD. Par défaut: https://instad.bj.\n",
    "\n",
    "Méthodes\n",
    "1) scrape_html_tables(urls: list, max_tables: int = 5) -> list[pd.DataFrame]\n",
    "   - Rôle: Parcourt une liste d’URLs, récupère jusqu’à max_tables tableaux HTML par page.\n",
    "   - Sortie: Liste de DataFrames (une par table trouvée). Colonnes ajoutées:\n",
    "     - source_url: URL d’origine.\n",
    "     - table_index: index du tableau sur la page (0..max_tables-1).\n",
    "     - collection_date: date du jour (YYYY-MM-DD).\n",
    "   - Journalisation: nombre de tableaux trouvés, taille de chaque extraction, erreurs éventuelles.\n",
    "\n",
    "2) scrape_json_data(json_urls: list) -> list[pd.DataFrame]\n",
    "   - Rôle: Appelle une liste d’URLs retournant du JSON et normalise la structure en DataFrame.\n",
    "   - Sortie: Liste de DataFrames (une par ressource JSON). Colonnes ajoutées:\n",
    "     - source_url, collection_date.\n",
    "   - Journalisation: taille de l’extraction, erreurs HTTP/JSON.\n",
    "\n",
    "3) scrape_demographic_data() -> list[pd.DataFrame]\n",
    "   - Rôle: Cible des pages démographiques prédéfinies (indicateurs récents, publications annuelles).\n",
    "   - Sortie: Liste de DataFrames issus des tableaux HTML.\n",
    "\n",
    "4) scrape_economic_data() -> pd.DataFrame\n",
    "   - Rôle: Cible des pages économiques (publications mensuelles/trimestrielles) et, le cas échéant, des endpoints JSON.\n",
    "   - Sortie: DataFrame unique concaténant toutes les extractions (HTML + JSON). DataFrame vide si aucune donnée.\n",
    "\n",
    "Bonnes pratiques intégrées\n",
    "- En-tête User-Agent explicite pour éviter les blocages courants.\n",
    "- Temporisation entre requêtes (1–2s) afin de limiter la charge sur le serveur.\n",
    "- Gestion d’erreurs via logging (HTTP, JSON, parsing HTML).\n",
    "- Métadonnées systématiques pour la traçabilité.\n",
    "\n",
    "Limites connues\n",
    "- Structure HTML susceptible de changer (sélecteurs “table” génériques).\n",
    "- Tables complexes (multi-index, cellules fusionnées) parfois mal interprétées par pandas.read_html.\n",
    "- Aucune stratégie de retry/backoff en cas d’échec réseau temporaire.\n",
    "\n",
    "Exemples d’utilisation rapide\n",
    "- Démographie:\n",
    "  - demographic_tables = scraper.scrape_demographic_data()\n",
    "  - demographic_df = pd.concat(demographic_tables, ignore_index=True) si la liste n’est pas vide.\n",
    "- Économie:\n",
    "  - economic_df = scraper.scrape_economic_data()\n",
    "- Export:\n",
    "  - df_final.to_csv(\"data/raw_data/instad_scraping.csv\", index=False)\n",
    "\n",
    "Conseils\n",
    "- Vérifier les colonnes extraites (titres, formats numériques, dates) avant analyse.\n",
    "- Conserver les champs source_url et collection_date pour l’audit et la reproductibilité.\n",
    "- Adapter max_tables selon la page; augmenter au besoin si plusieurs tableaux sont attendus.\n"
   ],
   "id": "f34cbe24d7c71c99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:07:51.153288Z",
     "start_time": "2025-09-20T13:07:51.102049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class INStadScraper:\n",
    "    def __init__(self, base_url: str = API_BASE_URL_INSTAD):\n",
    "        self.url = base_url\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "        })\n",
    "\n",
    "    def scrape_html_tables(self, urls: list, max_tables: int = 5):\n",
    "        html_data_tables = []\n",
    "\n",
    "        if urls:\n",
    "            for url in urls:\n",
    "                try:\n",
    "                    response = self.session.get(url, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    logging.info(f\"Scraping html de l'url {response.url}\")\n",
    "\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                    tables = soup.find_all('table')\n",
    "                    logging.info(f\"{len(tables)} tableaux trouvées sur {url}\")\n",
    "\n",
    "                    for i, table in enumerate(tables[:max_tables]):\n",
    "                        try:\n",
    "                            dataset = pd.read_html(StringIO(str(table)))[0]\n",
    "                            dataset['source_url'] = url\n",
    "                            dataset['table_index'] = i\n",
    "                            dataset['collection_date'] = datetime.now().date()\n",
    "                            html_data_tables.append(dataset)\n",
    "                            logging.info(f\"Tableau {i + 1} récupéré: {dataset.shape} \\n\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Impossible de récupérer le table à l'index {i} sur {url}: {e}\")\n",
    "\n",
    "                    time.sleep(2)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    logging.error(f\"Erreur requête pour {url}: {e}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Erreur inattendue pour l'url {url}: {e}\")\n",
    "\n",
    "        return html_data_tables\n",
    "\n",
    "    def scrape_json_data(self, json_urls: list):\n",
    "        json_data = []\n",
    "\n",
    "        if json_urls:\n",
    "            for url in json_urls:\n",
    "                try:\n",
    "                    response = self.session.get(url, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    logging.info(f\"Scraping json de l'url {response.url}\")\n",
    "\n",
    "                    data = response.json()\n",
    "\n",
    "                    dataset = pd.json_normalize(data)\n",
    "                    dataset['source_url'] = url\n",
    "                    dataset['collection_date'] = datetime.now().date()\n",
    "                    json_data.append(dataset)\n",
    "                    logging.info(f\"Json récupérer: {dataset.shape} \\n\")\n",
    "\n",
    "                    time.sleep(1)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    logging.error(f\"Erreur HTTP pour {url}: {e}\")\n",
    "                except ValueError as e:\n",
    "                    logging.error(f\"JSON invalide pour {url}: {e}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Erreur inattendue pour {url}: {e}\")\n",
    "\n",
    "        return json_data\n",
    "\n",
    "    def scrape_demographic_data(self):\n",
    "\n",
    "        urls = [\n",
    "            f\"{self.url}/statistiques/indicateurs-recents/43-population\",  # indicateurs récents\n",
    "            f\"{self.url}/publications/publications-annuelles\"  # publications annuelles\n",
    "        ]\n",
    "\n",
    "        return self.scrape_html_tables(urls)\n",
    "\n",
    "    def scrape_economic_data(self):\n",
    "        html_urls = [\n",
    "            f\"{self.url}/publications/publications-trimestrielles\",  # publications trimestrielles\n",
    "            f\"{self.url}/publications/publications-mensuelles\"  # publications mensuelles\n",
    "        ]\n",
    "\n",
    "        # A renseigner une url json a scraper\n",
    "        json_urls = []\n",
    "\n",
    "        html_data = self.scrape_html_tables(html_urls)\n",
    "        json_data = self.scrape_json_data(json_urls)\n",
    "\n",
    "        donnees = html_data + json_data\n",
    "        if donnees:\n",
    "            return pd.concat(donnees, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n"
   ],
   "id": "7bbf9faca71d018a",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Implémentations",
   "id": "669a1c0ce6052ccc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:08:11.995960Z",
     "start_time": "2025-09-20T13:07:51.169723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scraper = INStadScraper()\n",
    "\n",
    "path_instad_to_save = RAW_DIR / 'instad_scraping.csv'\n",
    "\n",
    "has_data = False\n",
    "instad_data = None\n",
    "\n",
    "if not Path(path_instad_to_save).exists():\n",
    "    demographic_tables = scraper.scrape_demographic_data()\n",
    "\n",
    "    if demographic_tables:\n",
    "        demographic_df = pd.concat(demographic_tables, ignore_index=True)\n",
    "    else:\n",
    "        demographic_df = pd.DataFrame()\n",
    "\n",
    "    logging.info(f\"Nombre de lignes démographiques : {len(demographic_df)}\")\n",
    "\n",
    "    economic_df = scraper.scrape_economic_data()\n",
    "    logging.info(f\"Nombre de lignes économiques : {len(economic_df)}\")\n",
    "\n",
    "    instad_data = pd.concat([demographic_df, economic_df], ignore_index=True)\n",
    "    has_data = len(instad_data) > 0\n",
    "    logging.info(f\"Total lignes récupérées : {len(instad_data)}\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")\n"
   ],
   "id": "8b0eb2c40ecf6b23",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 14:07:53,719 | INFO | Scraping html de l'url https://instad.bj/statistiques/indicateurs-recents/43-population\n",
      "2025-09-20 14:07:53,827 | INFO | 4 tableaux trouvées sur https://instad.bj/statistiques/indicateurs-recents/43-population\n",
      "2025-09-20 14:07:53,838 | INFO | Tableau 1 récupéré: (1, 8) \n",
      "\n",
      "2025-09-20 14:07:53,851 | INFO | Tableau 2 récupéré: (13, 5) \n",
      "\n",
      "2025-09-20 14:07:53,858 | INFO | Tableau 3 récupéré: (5, 10) \n",
      "\n",
      "2025-09-20 14:07:53,868 | INFO | Tableau 4 récupéré: (5, 10) \n",
      "\n",
      "2025-09-20 14:07:57,760 | INFO | Scraping html de l'url https://instad.bj/publications/publications-annuelles\n",
      "2025-09-20 14:07:57,969 | INFO | 7 tableaux trouvées sur https://instad.bj/publications/publications-annuelles\n",
      "2025-09-20 14:07:57,975 | INFO | Tableau 1 récupéré: (1, 8) \n",
      "\n",
      "2025-09-20 14:07:57,986 | INFO | Tableau 2 récupéré: (11, 6) \n",
      "\n",
      "2025-09-20 14:07:58,014 | INFO | Tableau 3 récupéré: (107, 6) \n",
      "\n",
      "2025-09-20 14:07:58,023 | INFO | Tableau 4 récupéré: (27, 6) \n",
      "\n",
      "2025-09-20 14:07:58,029 | INFO | Tableau 5 récupéré: (6, 6) \n",
      "\n",
      "2025-09-20 14:08:00,039 | INFO | Nombre de lignes démographiques : 176\n",
      "2025-09-20 14:08:02,002 | INFO | Scraping html de l'url https://instad.bj/publications/publications-trimestrielles\n",
      "2025-09-20 14:08:02,586 | INFO | 18 tableaux trouvées sur https://instad.bj/publications/publications-trimestrielles\n",
      "2025-09-20 14:08:02,598 | INFO | Tableau 1 récupéré: (1, 8) \n",
      "\n",
      "2025-09-20 14:08:02,614 | INFO | Tableau 2 récupéré: (2, 6) \n",
      "\n",
      "2025-09-20 14:08:02,634 | INFO | Tableau 3 récupéré: (20, 6) \n",
      "\n",
      "2025-09-20 14:08:02,651 | INFO | Tableau 4 récupéré: (2, 6) \n",
      "\n",
      "2025-09-20 14:08:02,682 | INFO | Tableau 5 récupéré: (33, 6) \n",
      "\n",
      "2025-09-20 14:08:06,997 | INFO | Scraping html de l'url https://instad.bj/publications/publications-mensuelles\n",
      "2025-09-20 14:08:07,133 | INFO | 3 tableaux trouvées sur https://instad.bj/publications/publications-mensuelles\n",
      "2025-09-20 14:08:07,151 | INFO | Tableau 1 récupéré: (1, 8) \n",
      "\n",
      "2025-09-20 14:08:07,170 | INFO | Tableau 2 récupéré: (5, 10) \n",
      "\n",
      "2025-09-20 14:08:07,190 | INFO | Tableau 3 récupéré: (5, 10) \n",
      "\n",
      "2025-09-20 14:08:11,951 | INFO | Scraping json de l'url https://api.worldbank.org/v2/country/BJ/indicator/SP.POP.TOTL?format=json\n",
      "2025-09-20 14:08:11,956 | ERROR | Erreur inattendue pour https://api.worldbank.org/v2/country/BJ/indicator/SP.POP.TOTL?format=json: 'list' object has no attribute 'keys'\n",
      "2025-09-20 14:08:11,970 | INFO | Nombre de lignes économiques : 69\n",
      "2025-09-20 14:08:11,983 | INFO | Total lignes récupérées : 245\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Sauvegarde des données",
   "id": "5a716c2cf326bf76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T13:08:12.139736Z",
     "start_time": "2025-09-20T13:08:12.102267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if has_data and not instad_data.empty:\n",
    "    instad_data.to_csv(path_instad_to_save, index=False)\n",
    "\n",
    "    # Taille du fichier en octets\n",
    "    size_instad_bytes = path_instad_to_save.stat().st_size\n",
    "\n",
    "    # Optionnel : convertir en Ko/Mo pour lecture facile\n",
    "    size_instad_kb = size_instad_bytes / 1024\n",
    "    size_instad_mb = size_instad_kb / 1024\n",
    "\n",
    "    logging.info(\n",
    "        f\"Sauvegarde des données à {path_instad_to_save} ({size_instad_bytes} bytes / {size_instad_kb:.2f} KB / {size_instad_mb:.2f} MB)\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")"
   ],
   "id": "15fbbf911cabd5aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 14:08:12,134 | INFO | Sauvegarde des données à data\\raw_data\\instad_scraping.csv (31837 bytes / 31.09 KB / 0.03 MB)\n"
     ]
    }
   ],
   "execution_count": 119
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
