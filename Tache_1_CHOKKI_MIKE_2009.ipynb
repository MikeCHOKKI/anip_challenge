{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29246f3e545590cc",
   "metadata": {},
   "source": [
    "# COLLECTE ET HARMONISATION DES DONNEES - REPUBLIQUE DU BENIN\n",
    "\n",
    "## Objectifs\n",
    "   - **Thématique**: Démographie - Economie - Santé - Education - Géographie\n",
    "   - **Sources**: INStad - Portails Open Data - Worl Bank API - Web scraping\n",
    "   - **Couverture**: Tous les départements du bénin\n",
    "   - **Période**: 2020 - 2025 (Selon la disponibilité des données au niveau des sources)\n",
    "### Python version: 3.13.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5604f4c399f266c",
   "metadata": {},
   "source": [
    "### Configuration des imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4878bcc29f9406b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.280508Z",
     "start_time": "2025-09-20T20:10:24.271252Z"
    }
   },
   "source": [
    "# =============================\n",
    "# Imports\n",
    "# =============================\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import warnings\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from io import StringIO\n",
    "\n",
    "# Third-party\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =============================\n",
    "# Configurations globales\n",
    "# =============================\n",
    "\n",
    "# Warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "\n",
    "# Pandas\n",
    "pd.set_option(\"display.max_rows\", 100)  # nb max de lignes affichées\n",
    "pd.set_option(\"display.max_columns\", None)  # affiche toutes les colonnes\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)  # formatage des floats\n",
    "pd.set_option(\"display.expand_frame_repr\", False)  # évite retour ligne inutile\n",
    "\n",
    "# Matplotlib / Seaborn\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")  # style moderne et lisible\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# =============================\n",
    "# Check initialisation\n",
    "# =============================\n",
    "\n",
    "logging.info(\"Librairies et configurations chargées\")\n",
    "logging.info(\"Début de la collecte de données: %s\", datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:24,276 | INFO | Librairies et configurations chargées\n",
      "2025-09-20 21:10:24,278 | INFO | Début de la collecte de données: 20/09/2025 21:10:24\n"
     ]
    }
   ],
   "execution_count": 190
  },
  {
   "cell_type": "markdown",
   "id": "446520b4a707b964",
   "metadata": {},
   "source": [
    "### Configuration des dossiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd16f57d830833",
   "metadata": {},
   "source": [
    "#### Fonction de configurations de dossiers, réutilisable"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4b827eb5f4f2c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.292089Z",
     "start_time": "2025-09-20T20:10:24.286138Z"
    }
   },
   "source": [
    "def init_directory(base_dir: Optional[Path] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Initializes directory structure under the specified base directory.\n",
    "\n",
    "    Creates a directory structure for storing data, including raw data,\n",
    "    processed data, and final data. If directories already exist, their\n",
    "    existence is logged; otherwise, they are created.\n",
    "\n",
    "    :param base_dir: Optional base directory path. Defaults to the current\n",
    "        working directory if not provided.\n",
    "    :type base_dir: Optional[Path]\n",
    "    :return: A dictionary with the keys 'data', 'raw_data', 'processed_data',\n",
    "        and 'final_data', where each value is the Path object of the corresponding\n",
    "        directory.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    if base_dir is None:\n",
    "        base_dir = Path(\".\")\n",
    "\n",
    "    data_dir = base_dir / \"data\"  # Répertoire ou se trouvera les données\n",
    "    dirs = {\n",
    "        \"data\": data_dir,\n",
    "        \"raw_data\": data_dir / \"raw_data\",  # Répertoire des données brutes\n",
    "        \"processed_data\": data_dir / \"processed_data\",  # Répertoire des données traitées\n",
    "        \"final_data\": data_dir / \"final_data\",  # Répertoire des données finales\n",
    "    }\n",
    "\n",
    "    for name, path in dirs.items():\n",
    "        try:\n",
    "            if path.exists():\n",
    "                logging.info(f\"Dossier {name} existe déjà.\")\n",
    "            else:\n",
    "                path.mkdir(parents=True, exist_ok=True)\n",
    "                logging.info(f\"Répertoire {name} prêt à {path}\")\n",
    "        except PermissionError as pe:\n",
    "            logging.error(f\"Erreur de permission pour {name}: {pe}\")\n",
    "        except OSError as oe:\n",
    "            logging.error(f\"Erreur système pour {name}: {oe}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erreur inconnu pour {name}: {e}\")\n",
    "\n",
    "    return dirs"
   ],
   "outputs": [],
   "execution_count": 191
  },
  {
   "cell_type": "markdown",
   "id": "7f1368c76208bc00",
   "metadata": {},
   "source": [
    "#### Initialisation des dossiers"
   ]
  },
  {
   "cell_type": "code",
   "id": "86164855e4155650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.306831Z",
     "start_time": "2025-09-20T20:10:24.297289Z"
    }
   },
   "source": [
    "DATA_DIR, RAW_DIR, PROCESSED_DIR, FINAL_DIR = init_directory().values()\n",
    "\n",
    "logging.info(\"Dossiers chargés avec succès\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:24,300 | INFO | Dossier data existe déjà.\n",
      "2025-09-20 21:10:24,301 | INFO | Dossier raw_data existe déjà.\n",
      "2025-09-20 21:10:24,302 | INFO | Dossier processed_data existe déjà.\n",
      "2025-09-20 21:10:24,303 | INFO | Dossier final_data existe déjà.\n",
      "2025-09-20 21:10:24,304 | INFO | Dossiers chargés avec succès\n"
     ]
    }
   ],
   "execution_count": 192
  },
  {
   "cell_type": "markdown",
   "id": "c2c710eeab06e5bb",
   "metadata": {},
   "source": [
    "### Traitement des sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417157955813708",
   "metadata": {},
   "source": [
    "#### URL et sources API externes"
   ]
  },
  {
   "cell_type": "code",
   "id": "3529220150762d4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.319589Z",
     "start_time": "2025-09-20T20:10:24.311906Z"
    }
   },
   "source": [
    "# Base URL pour l'API de la Banque Mondiale\n",
    "API_BASE_URL_WORLD_BANK = \"https://api.worldbank.org/v2\"\n",
    "\n",
    "# Base URL pour l'API d'Instad (service local ou national)\n",
    "API_BASE_URL_INSTAD = \"https://instad.bj\"\n",
    "\n",
    "# Base URL pour l'API Overpass (OpenStreetMap) pour requêtes sur les cartes\n",
    "API_BASE_URL_OVERPASS = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# Liste des URLs CSV provenant de sources externes potentielles\n",
    "# Ici un exemple : données UN Data via l'API UNSD (2015-2024)\n",
    "POTENTIAL_API_EXTERNAL_SOURCE_CSV_FILE = [\n",
    "    \"https://data.un.org/ws/rest/data/UNSD,DF_UNData_UNFCC,1.0/all/?startPeriod=2015&endPeriod=2024\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 193
  },
  {
   "cell_type": "markdown",
   "id": "2f88d4b385b2ac2a",
   "metadata": {},
   "source": [
    "#### Configuration supplémentaire"
   ]
  },
  {
   "cell_type": "code",
   "id": "2adb2b9085eb537f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.339961Z",
     "start_time": "2025-09-20T20:10:24.324262Z"
    }
   },
   "source": [
    "COUNTRY_CODE = \"BJ\"\n",
    "\n",
    "DEFAULT_INDICATOR_WORLD_BANK = [\n",
    "    'SP.POP.TOTL',  # Population totale\n",
    "    'NY.GDP.MKTP.CD',  # PIB\n",
    "    'SE.PRM.NENR',  # Scolarisation primaire\n",
    "    'SH.DYN.MORT',  # Taux de mortalité\n",
    "    'AG.LND.TOTL.K2',  # Surface totale\n",
    "    'NY.GDP.PCAP.CD',  # PIB par habitant\n",
    "    'SL.TLF.TOTL.IN',  # Force de travail\n",
    "    'SP.DYN.TFRT.IN'  # Taux de fertilité\n",
    "]\n",
    "\n",
    "OPEN_STREET_MAP_ADMIN_LEVEL_PAYS = \"2\"\n",
    "OPEN_STREET_MAP_ADMIN_LEVEL_DEPARTEMENT = \"4\"\n",
    "OPEN_STREET_MAP_ADMIN_LEVEL_COMMUNES = \"6\""
   ],
   "outputs": [],
   "execution_count": 194
  },
  {
   "cell_type": "markdown",
   "id": "25dcb5c15ce90898",
   "metadata": {},
   "source": [
    "#### Collecte WORLD BANK API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0275c6bec67e7a4",
   "metadata": {},
   "source": "##### WorldBankAPI"
  },
  {
   "cell_type": "code",
   "id": "8666b7a94a010186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.360118Z",
     "start_time": "2025-09-20T20:10:24.345868Z"
    }
   },
   "source": [
    "class WorldBankAPI:\n",
    "    def __init__(self, url: str = API_BASE_URL_WORLD_BANK, country_code: str = COUNTRY_CODE, start_year: int = 2015,\n",
    "                 end_year: int = 2024, default_per_page: int = 100):\n",
    "        \"\"\"\n",
    "        Initializes the API request session with the given parameters for interacting\n",
    "        with the World Bank API. Allows setting a specific country, time range, and API\n",
    "        base URL. Configures default headers for the session to include a `User-Agent`\n",
    "        string customized for educational research purposes.\n",
    "\n",
    "        :param url: The base URL of the API to be used for requests.\n",
    "        :type url: str\n",
    "        :param country_code: The ISO 3166-1 alpha-3 country code for the target country.\n",
    "        :type country_code: str\n",
    "        :param start_year: The starting year for the data range.\n",
    "        :type start_year: int\n",
    "        :param end_year: The ending year for the data range.\n",
    "        :type end_year: int\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.country_code = country_code\n",
    "        self.start_year = start_year\n",
    "        self.end_year = end_year\n",
    "        self.per_page = default_per_page\n",
    "\n",
    "        self.session = requests.Session()\n",
    "\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Educational Research Purpose)'\n",
    "        })\n",
    "\n",
    "    def get_indicators(self, indicators: list = DEFAULT_INDICATOR_WORLD_BANK) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Récupère les données pour une liste d'indicateurs économiques ou financiers via l'API World Bank.\n",
    "        Les données sont accumulées dans un pandas DataFrame.\n",
    "\n",
    "        :param indicators: Liste des codes d'indicateurs à récupérer. Si non fournie, une liste par défaut est utilisée.\n",
    "        :type indicators: list\n",
    "        :return: DataFrame contenant les données récupérées, avec code et nom de l'indicateur, informations pays,\n",
    "                 année, valeur, source et date de collecte.\n",
    "        :rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        donnees = []\n",
    "\n",
    "        for indicator in indicators:\n",
    "            logging.info(f\"Récupération des données pour l'indicateur {indicator}\")\n",
    "\n",
    "            url = f\"{self.url}/country/{self.country_code}/indicator/{indicator}\"\n",
    "            params = {\n",
    "                'date': f'{self.start_year}:{self.end_year}',\n",
    "                'format': 'json',\n",
    "                'per_page': self.per_page\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = self.session.get(url, params=params, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "\n",
    "                # Vérifie si des données sont disponibles\n",
    "                entries = data[1] if len(data) > 1 and data[1] else []\n",
    "                logging.info(f\"{len(entries)} enregistrements récupérés pour {indicator} \\n\")\n",
    "\n",
    "                for entry in entries:\n",
    "                    donnees.append({\n",
    "                        'indicator_code': entry['indicator']['id'],\n",
    "                        'indicator_name': entry['indicator']['value'],\n",
    "                        'country_code': entry['country']['id'],\n",
    "                        'country_name': entry['country']['value'],\n",
    "                        'year': entry['date'],\n",
    "                        'value': entry['value']\n",
    "                    })\n",
    "\n",
    "                # Pause pour éviter de saturer l'API\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(f\"Erreur HTTP pour l'indicateur {indicator}: {e}\")\n",
    "            except ValueError as e:\n",
    "                logging.error(f\"Erreur JSON pour l'indicateur {indicator}: {e}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Erreur inattendue pour l'indicateur {indicator}: {e}\")\n",
    "\n",
    "        dataset = pd.DataFrame(donnees)\n",
    "\n",
    "        if not dataset.empty:\n",
    "            dataset['year'] = pd.to_numeric(dataset['year'], errors='coerce')\n",
    "            dataset['source'] = \"WORLD BANK API\"\n",
    "            dataset['collection_date'] = datetime.now().date()\n",
    "\n",
    "        return dataset\n"
   ],
   "outputs": [],
   "execution_count": 195
  },
  {
   "cell_type": "markdown",
   "id": "4c189efbb7eda79f",
   "metadata": {},
   "source": [
    "##### Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "id": "722ceb689b37835f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.378169Z",
     "start_time": "2025-09-20T20:10:24.366829Z"
    }
   },
   "source": [
    "world_bank = WorldBankAPI()\n",
    "path_to_save = RAW_DIR / 'world_bank_data.csv'\n",
    "\n",
    "has_data = False\n",
    "world_bank_data = None\n",
    "\n",
    "if not Path(path_to_save).exists():\n",
    "    world_bank_data = world_bank.get_indicators()\n",
    "    has_data = len(world_bank_data) > 0\n",
    "    if has_data:\n",
    "        logging.info(f\"World Bank: {len(world_bank_data)} enrégistrement collectés\")\n",
    "    else:\n",
    "        logging.info(f\"World Bank: Aucun enrégistrement collectés\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:24,375 | WARNING | Déjà éffectué\n"
     ]
    }
   ],
   "execution_count": 196
  },
  {
   "cell_type": "markdown",
   "id": "f2dcf6414f128a09",
   "metadata": {},
   "source": [
    "##### Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0c2e7aeb4191ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.397283Z",
     "start_time": "2025-09-20T20:10:24.387108Z"
    }
   },
   "source": [
    "if has_data and not world_bank_data.empty:\n",
    "    world_bank_data.to_csv(path_to_save, index=False)\n",
    "\n",
    "    # Taille du fichier en octets\n",
    "    size_bytes = path_to_save.stat().st_size\n",
    "\n",
    "    # Optionnel : convertir en Ko/Mo pour lecture facile\n",
    "    size_kb = size_bytes / 1024\n",
    "    size_mb = size_kb / 1024\n",
    "\n",
    "    logging.info(f\"Sauvegarde des données à {path_to_save} ({size_bytes} bytes / {size_kb:.2f} KB / {size_mb:.2f} MB)\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:24,394 | WARNING | Déjà éffectué\n"
     ]
    }
   ],
   "execution_count": 197
  },
  {
   "cell_type": "markdown",
   "id": "fb267cb1b32d8f1",
   "metadata": {},
   "source": [
    "#### Collecte INSTAD API - WEB SCRAPING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34cbe24d7c71c99",
   "metadata": {},
   "source": "###### INStadScraper"
  },
  {
   "cell_type": "code",
   "id": "7bbf9faca71d018a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.419594Z",
     "start_time": "2025-09-20T20:10:24.405501Z"
    }
   },
   "source": [
    "class INStadScraper:\n",
    "    def __init__(self, base_url: str = API_BASE_URL_INSTAD):\n",
    "        self.url = base_url\n",
    "\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "        })\n",
    "\n",
    "    def scrape_html_tables(self, urls: list, max_tables: int = 5):\n",
    "        html_data_tables = []\n",
    "\n",
    "        if urls:\n",
    "            for url in urls:\n",
    "                try:\n",
    "                    response = self.session.get(url, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    logging.info(f\"Scraping html de l'url {response.url}\")\n",
    "\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                    tables = soup.find_all('table')\n",
    "                    logging.info(f\"{len(tables)} tableaux trouvées sur {url}\")\n",
    "\n",
    "                    for i, table in enumerate(tables[:max_tables]):\n",
    "                        try:\n",
    "                            dataset = pd.read_html(StringIO(str(table)))[0]\n",
    "                            dataset['source_url'] = url\n",
    "                            dataset['table_index'] = i\n",
    "                            dataset['collection_date'] = datetime.now().date()\n",
    "                            html_data_tables.append(dataset)\n",
    "                            logging.info(f\"Tableau {i + 1} récupéré: {dataset.shape} \\n\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Impossible de récupérer le table à l'index {i} sur {url}: {e}\")\n",
    "\n",
    "                    time.sleep(2)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    logging.error(f\"Erreur requête pour {url}: {e}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Erreur inattendue pour l'url {url}: {e}\")\n",
    "\n",
    "        return html_data_tables\n",
    "\n",
    "    def scrape_json_data(self, json_urls: list):\n",
    "        json_data = []\n",
    "\n",
    "        if json_urls:\n",
    "            for url in json_urls:\n",
    "                try:\n",
    "                    response = self.session.get(url, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    logging.info(f\"Scraping json de l'url {response.url}\")\n",
    "\n",
    "                    data = response.json()\n",
    "\n",
    "                    dataset = pd.json_normalize(data)\n",
    "                    dataset['source_url'] = url\n",
    "                    dataset['collection_date'] = datetime.now().date()\n",
    "                    json_data.append(dataset)\n",
    "                    logging.info(f\"Json récupérer: {dataset.shape} \\n\")\n",
    "\n",
    "                    time.sleep(1)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    logging.error(f\"Erreur HTTP pour {url}: {e}\")\n",
    "                except ValueError as e:\n",
    "                    logging.error(f\"JSON invalide pour {url}: {e}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Erreur inattendue pour {url}: {e}\")\n",
    "\n",
    "        return json_data\n",
    "\n",
    "    def scrape_demographic_data(self):\n",
    "\n",
    "        urls = [\n",
    "            f\"{self.url}/statistiques/indicateurs-recents/43-population\",  # indicateurs récents\n",
    "            f\"{self.url}/publications/publications-annuelles\"  # publications annuelles\n",
    "        ]\n",
    "\n",
    "        return self.scrape_html_tables(urls)\n",
    "\n",
    "    def scrape_economic_data(self):\n",
    "        html_urls = [\n",
    "            f\"{self.url}/publications/publications-trimestrielles\",  # publications trimestrielles\n",
    "            f\"{self.url}/publications/publications-mensuelles\"  # publications mensuelles\n",
    "        ]\n",
    "\n",
    "        # A renseigner une url json a scraper\n",
    "        json_urls = []\n",
    "\n",
    "        html_data = self.scrape_html_tables(html_urls)\n",
    "        json_data = self.scrape_json_data(json_urls)\n",
    "\n",
    "        donnees = html_data + json_data\n",
    "        if donnees:\n",
    "            return pd.concat(donnees, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n"
   ],
   "outputs": [],
   "execution_count": 198
  },
  {
   "cell_type": "markdown",
   "id": "669a1c0ce6052ccc",
   "metadata": {},
   "source": [
    "##### Implémentations"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b0eb2c40ecf6b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.439405Z",
     "start_time": "2025-09-20T20:10:24.426730Z"
    }
   },
   "source": [
    "scraper = INStadScraper()\n",
    "\n",
    "path_instad_to_save = RAW_DIR / 'instad_scraping.csv'\n",
    "\n",
    "has_data = False\n",
    "instad_data = None\n",
    "\n",
    "if not Path(path_instad_to_save).exists():\n",
    "    demographic_tables = scraper.scrape_demographic_data()\n",
    "\n",
    "    if demographic_tables:\n",
    "        demographic_df = pd.concat(demographic_tables, ignore_index=True)\n",
    "    else:\n",
    "        demographic_df = pd.DataFrame()\n",
    "\n",
    "    logging.info(f\"Nombre de lignes démographiques : {len(demographic_df)}\")\n",
    "\n",
    "    economic_df = scraper.scrape_economic_data()\n",
    "    logging.info(f\"Nombre de lignes économiques : {len(economic_df)}\")\n",
    "\n",
    "    instad_data = pd.concat([demographic_df, economic_df], ignore_index=True)\n",
    "    has_data = len(instad_data) > 0\n",
    "    logging.info(f\"Total lignes récupérées : {len(instad_data)}\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:24,437 | WARNING | Déjà éffectué\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "cell_type": "markdown",
   "id": "5a716c2cf326bf76",
   "metadata": {},
   "source": [
    "##### Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "id": "15fbbf911cabd5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.453101Z",
     "start_time": "2025-09-20T20:10:24.446337Z"
    }
   },
   "source": [
    "if has_data and not instad_data.empty:\n",
    "    instad_data.to_csv(path_instad_to_save, index=False)\n",
    "\n",
    "    # Taille du fichier en octets\n",
    "    size_instad_bytes = path_instad_to_save.stat().st_size\n",
    "\n",
    "    # Optionnel : convertir en Ko/Mo pour lecture facile\n",
    "    size_instad_kb = size_instad_bytes / 1024\n",
    "    size_instad_mb = size_instad_kb / 1024\n",
    "\n",
    "    logging.info(\n",
    "        f\"Sauvegarde des données à {path_instad_to_save} ({size_instad_bytes} bytes / {size_instad_kb:.2f} KB / {size_instad_mb:.2f} MB)\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:24,450 | WARNING | Déjà éffectué\n"
     ]
    }
   ],
   "execution_count": 200
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Collecte Données Géographique",
   "id": "97951b24ab7bcf59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### GeographicDataCollector",
   "id": "16b4ef34e82ec0a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:24.475749Z",
     "start_time": "2025-09-20T20:10:24.465278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GeographicDataCollector:\n",
    "    def __init__(self, base_url: str = API_BASE_URL_OVERPASS, country_code: str = COUNTRY_CODE):\n",
    "        self.url = base_url\n",
    "        self.country_code = country_code\n",
    "\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def get_openstreetmap_cities(self, admin_level: str = OPEN_STREET_MAP_ADMIN_LEVEL_PAYS):\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:60];\n",
    "        area[\"ISO3166-1\"={self.country_code}][admin_level={admin_level}];\n",
    "        (\n",
    "          node(area)[\"place\"~\"city|town\"];\n",
    "          way(area)[\"place\"~\"city|town\"];\n",
    "          relation(area)[\"place\"~\"city|town\"];\n",
    "        );\n",
    "        out center tags;\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.session.post(self.url, data={'data': query}, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            logging.info(f\"Données villes récupérées à {response.url}\")\n",
    "\n",
    "            cities = []\n",
    "\n",
    "            for element in data.get('elements', []):\n",
    "                if 'tags' in element:\n",
    "                    cities.append({\n",
    "                        'name': element.get('tags').get('name'),\n",
    "                        'place_type': element.get('tags').get('place'),\n",
    "                        'population': element.get('tags').get('population'),\n",
    "                        'latitude': element.get('lat') or (element.get('center', {}) or {}).get('lat'),\n",
    "                        'longitude': element.get('lon') or (element.get('center', {}) or {}).get('lon'),\n",
    "                        'source': \"OpenStreetMap\",\n",
    "                        'collection_date': datetime.now().date(),\n",
    "                    })\n",
    "\n",
    "            logging.info(f\"{len(cities)} enregistrements récupérés \\n\")\n",
    "\n",
    "            return pd.DataFrame(cities)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Erreur HTTP pour OpenStreetMap: {e}\")\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"JSON invalide pour OpenStreetMap: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erreur inattendue pour OpenStreetMap: {e}\")\n",
    "\n",
    "    def get_administrative_boundaries(self, admin_level: str = OPEN_STREET_MAP_ADMIN_LEVEL_PAYS):\n",
    "        query = f\"\"\"\n",
    "            [out:json][timeout:60];\n",
    "            relation[\"boundary\"=\"administrative\"][\"admin_level\"={admin_level}][\"name\"=\"Bénin\"];\n",
    "            out center tags;\n",
    "            \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.session.post(self.url, data={'data': query}, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            logging.info(f\"Données administrative récupérées à {response.url}\\n{query}\\n{data}\")\n",
    "\n",
    "            boundaries = []\n",
    "\n",
    "            for element in data.get('elements', []):\n",
    "                if 'tags' in element:\n",
    "                    boundaries.append({\n",
    "                        'name': element.get('tags').get('name'),\n",
    "                        'admin_level': admin_level,\n",
    "                        'wikidata': element.get('tags').get('wikidata'),\n",
    "                        'latitude': element.get('lat') or (element.get('center', {}) or {}).get('lat'),\n",
    "                        'longitude': element.get('lon') or (element.get('center', {}) or {}).get('lon'),\n",
    "                        'osm_id': element.get('id'),\n",
    "                        'source': 'OpenStreetMap',\n",
    "                        'collection_date': datetime.now().date()\n",
    "                    })\n",
    "\n",
    "            logging.info(f\"{len(boundaries)} enregistrements récupérés \\n\")\n",
    "\n",
    "            return pd.DataFrame(boundaries)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Erreur HTTP pour OpenStreetMap: {e}\")\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"JSON invalide pour OpenStreetMap: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erreur inattendue pour OpenStreetMap: {e}\")"
   ],
   "id": "8091967e22df23bd",
   "outputs": [],
   "execution_count": 201
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Implémentation",
   "id": "a7161b7bd42519b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:27.204041Z",
     "start_time": "2025-09-20T20:10:24.485279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "geo_collector = GeographicDataCollector()\n",
    "\n",
    "path_osm_ville_to_save = RAW_DIR / \"osm_ville.csv\"\n",
    "path_osm_administrative_boundaries_to_save = RAW_DIR / \"osm_administrative_boundaries.csv\"\n",
    "\n",
    "has_data_osm_ville = False\n",
    "osm_ville_data = pd.DataFrame()\n",
    "\n",
    "has_data_osm_administrative_boundaries = False\n",
    "osm_administrative_boundaries_data = pd.DataFrame()\n",
    "\n",
    "if not Path(path_osm_ville_to_save).exists():\n",
    "    osm_ville_data = geo_collector.get_openstreetmap_cities()\n",
    "\n",
    "    has_data_osm_ville = len(osm_ville_data) > 0\n",
    "    logging.info(f\"OpenStreetMap: {len(osm_ville_data)} villes récupérées\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")\n",
    "\n",
    "if not Path(path_osm_administrative_boundaries_to_save).exists():\n",
    "    osm_administrative_boundaries_data = geo_collector.get_administrative_boundaries()\n",
    "\n",
    "    has_data_osm_administrative_boundaries = len(osm_administrative_boundaries_data) > 0\n",
    "    logging.info(f\"OpenStreetMap: {len(osm_administrative_boundaries_data)} données administrative récupérées\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")"
   ],
   "id": "19c122fafb9e5928",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:24,491 | WARNING | Déjà éffectué\n",
      "2025-09-20 21:10:27,195 | INFO | Données administrative récupérées à https://overpass-api.de/api/interpreter\n",
      "\n",
      "            [out:json][timeout:60];\n",
      "            relation[\"boundary\"=\"administrative\"][\"admin_level\"=2][\"name\"=\"Bénin\"];\n",
      "            out center tags;\n",
      "            \n",
      "{'version': 0.6, 'generator': 'Overpass API 0.7.62.8 e802775f', 'osm3s': {'timestamp_osm_base': '2025-09-20T20:08:57Z', 'copyright': 'The data included in this document is from www.openstreetmap.org. The data is made available under ODbL.'}, 'elements': []}\n",
      "2025-09-20 21:10:27,198 | INFO | 0 enregistrements récupérés \n",
      "\n",
      "2025-09-20 21:10:27,200 | INFO | OpenStreetMap: 0 données administrative récupérées\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Sauvegarde des données",
   "id": "539ac1a73c21fc0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T20:10:27.226494Z",
     "start_time": "2025-09-20T20:10:27.218960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if has_data_osm_ville and not osm_ville_data.empty:\n",
    "    osm_ville_data.to_csv(path_osm_ville_to_save, index=False)\n",
    "\n",
    "    # Taille du fichier en octets\n",
    "    size_osm_ville_bytes = path_osm_ville_to_save.stat().st_size\n",
    "\n",
    "    # Optionnel : convertir en Ko/Mo pour lecture facile\n",
    "    size_osm_ville_kb = size_osm_ville_bytes / 1024\n",
    "    size_osm_ville_mb = size_osm_ville_kb / 1024\n",
    "\n",
    "    logging.info(\n",
    "        f\"Sauvegarde des données à {path_osm_ville_to_save} ({size_osm_ville_bytes} bytes / {size_osm_ville_kb:.2f} KB / {size_osm_ville_mb:.2f} MB)\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")\n",
    "\n",
    "if has_data_osm_administrative_boundaries and not osm_administrative_boundaries_data.empty:\n",
    "    osm_administrative_boundaries_data.to_csv(path_osm_administrative_boundaries_to_save, index=False)\n",
    "\n",
    "    # Taille du fichier en octets\n",
    "    size_osm_administrative_bytes = path_osm_administrative_boundaries_to_save.stat().st_size\n",
    "\n",
    "    # Optionnel : convertir en Ko/Mo pour lecture facile\n",
    "    size_osm_administrative_kb = size_osm_administrative_bytes / 1024\n",
    "    size_osm_administrative_mb = size_osm_administrative_kb / 1024\n",
    "\n",
    "    logging.info(\n",
    "        f\"Sauvegarde des données à {path_osm_administrative_boundaries_to_save} ({size_osm_administrative_bytes} bytes / {size_osm_administrative_kb:.2f} KB / {size_osm_administrative_mb:.2f} MB)\")\n",
    "else:\n",
    "    logging.warning(\"Déjà éffectué\")"
   ],
   "id": "2227524044450b76",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:27,223 | WARNING | Déjà éffectué\n",
      "2025-09-20 21:10:27,224 | WARNING | Déjà éffectué\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nettoyage, Harmonisation, Fusion, Contrôle qualités des données",
   "id": "c03e8411174ac79d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
